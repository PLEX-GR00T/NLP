{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PLEX-GR00T/NLP/blob/main/HW3_TextExtraction_web_ppt_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MWpjyQ8pIqa",
        "outputId": "4b24a7dd-1c36-4851-9a0f-68bec5fc7092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.5.0-py3-none-any.whl (995 kB)\n",
            "\u001b[K     |████████████████████████████████| 995 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.7/dist-packages (from selenium) (2022.9.24)\n",
            "Collecting urllib3[socks]~=1.26\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[K     |████████████████████████████████| 384 kB 40.0 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc9\n",
            "  Downloading exceptiongroup-1.0.0rc9-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, urllib3, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.12 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 exceptiongroup-1.0.0rc9 h11-0.14.0 outcome-1.2.0 selenium-4.5.0 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.12 wsproto-1.2.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,992 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [950 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,546 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,424 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,324 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,163 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,108 kB]\n",
            "Fetched 14.8 MB in 4s (3,597 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 91.7 MB of archives.\n",
            "After this operation, 309 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 105.0.5195.102-0ubuntu0.18.04.1 [1,156 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 105.0.5195.102-0ubuntu0.18.04.1 [80.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 105.0.5195.102-0ubuntu0.18.04.1 [5,097 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 105.0.5195.102-0ubuntu0.18.04.1 [5,320 kB]\n",
            "Fetched 91.7 MB in 2s (43.6 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 123934 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_105.0.5195.102-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ],
      "source": [
        "# installing the libraries\n",
        "!pip install requests\n",
        "!pip install bs4\n",
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDflO561pPfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36445fce-abab-4f76-fb75-825f1bcd6744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ],
      "source": [
        "# importing the libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.colab import drive\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up-YBRWqnieX"
      },
      "source": [
        "# Question 1:\n",
        "\n",
        "As discussed in the demo above, using the example of geeksforgeeks, extract the information from marketwatch articles apart from the title and date that is already demonstrated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8dJLuMLnXUG"
      },
      "outputs": [],
      "source": [
        "# getting the second url\n",
        "# again open the url in parallel to track the extracted information\n",
        "url = \"https://www.marketwatch.com/story/the-next-financial-crisis-may-already-be-brewing-but-not-where-investors-might-expect-11663170963?mod=home-page\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQkfFr8yqNS1",
        "outputId": "8bf2450e-cae5-4a4c-abae-9cb5582067e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ],
      "source": [
        "# hitting the url and getting the response\n",
        "res = requests.get(url)\n",
        "print(res.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut9rcl3XqPE-"
      },
      "outputs": [],
      "source": [
        "sp = soup(res.text, \"html.parser\")\n",
        "# sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0-5WPTdqSa9",
        "outputId": "3015940f-e18f-4038-e3fa-92f3c6b951fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Joseph Adinolfi\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# getting the Author\n",
        "print(sp.find(\"div\", {\"class\" : \"author\"}).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJCxJGgFq_GX",
        "outputId": "dce054d4-917d-4d89-98b9-9087792ac905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Last Updated: Sept. 17, 2022 at 10:33 a.m. ET\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# getting the Last updated\n",
        "print(sp.find(\"time\", {\"class\" : \"timestamp--update\"}).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aNre8WrrwE1",
        "outputId": "2f537190-3062-4fad-8d22-822fa7a3c900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  As the Fed prepares to kick its balance-sheet runoff into high gear, some on Wall Street are worried that thinning Treasury-market liquidity could create a perfect storm.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# getting the sub heading\n",
        "print(sp.find(\"h2\", {\"class\" : \"article__subhead\"}).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSjAWpcbr5Kr",
        "outputId": "0f0c5bca-9092-475d-8395-eaea5637f932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://images.mktw.net/im-622981?width=700&height=466\n"
          ]
        }
      ],
      "source": [
        "# getting the image\n",
        "print(sp.find(\"div\", {\"class\" : \"article__inset__image__image\"}).img[\"src\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLIjHJ0ptY2C",
        "outputId": "ef5afd74-3877-4420-baaa-200241c17a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A growing number of traders, academics, and bond-market gurus are worried that the $24 trillion market for U.S. Treasury debt could be headed for a crisis as the Federal Reserve kicks its “quantitative tightening” into high gear this month.\n",
            "With the Fed doubling the pace at which its bond holdings will “roll off” its balance sheet in September, some bankers and institutional traders are worried that already-thinning liquidity in the Treasury market could set the stage for an economic catastrophe — or, falling short of that, involve a host of other drawbacks. \n",
            "In corners of Wall Street, some have been pointing out these risks. One particularly stark warning landed earlier this month, when Bank of America \n",
            "        BAC,\n",
            "        -2.26%\n",
            "       interest-rate strategist Ralph Axel warned the bank’s clients that “declining liquidity and resiliency of the Treasury market arguably poses one of the greatest threats to global financial stability today, potentially worse than the housing bubble of 2004-2007.”\n",
            "How could the normally staid Treasury market become ground zero for another financial crisis? Well, Treasurys play a critical role in the international financial system, with their yields forming a benchmark for trillions of dollars of loans, including most mortgages. \n",
            "Around the world, the 10-year Treasury yield \n",
            "        TMUBMUSD10Y,\n",
            "        3.889%\n",
            "      is considered the “risk-free rate” that sets the baseline by which many other assets — including stocks — are valued against. \n",
            "But outsize and erratic moves in Treasury yields aren’t the only issue: since the bonds themselves are used as collateral for banks seeking short-term financing in the “repo market” (often described as the “beating heart” of the U.S. financial system) it’s possible that if the Treasury market seizes up again — as it has nearly done in the recent past — various credit channels including corporate, household and government borrowing “would cease,” Axle wrote. \n",
            "See: Stock-market wild card: What investors need to know as Fed shrinks balance sheet at faster pace\n",
            "Short of an all-out blowup, thinning liquidity comes with a host of other drawbacks for investors, market participants, and the federal government, including higher borrowing costs, increased cross-asset volatility and — in one particularly extreme example — the possibility that the Federal government could default on its debt if auctions of newly issued Treasury bonds cease to function properly.\n",
            "Waning liquidity has been an issue since before the Fed started allowing its massive nearly $9 trillion balance sheet to shrink in June. But this month, the pace of this unwind will accelerate to $95 billion a month — an unprecedented pace, according to a pair of Kansas City Fed economists who published a paper about these risks earlier this year. \n",
            "According to Kansas City Fed economists Rajdeep Sengupta and Lee Smith, other market participants who otherwise might help to compensate for a less-active Fed are already at, or near, capacity in terms of their Treasury holdings. \n",
            "This could further exacerbate thinning liquidity, unless another class of buyers arrives — making the present period of Fed tightening potentially far more chaotic than the previous episode, which took place between 2017 and 2019. \n",
            "“This QT [quantitative tightening] episode could play out quite differently, and maybe it won’t be as tranquil and calm as that previous episode started out,” Smith said during a phone interview with MarketWatch. \n",
            "“Since banks’ balance sheet space is lower than it was in 2017, it’s more likely that other market participants will have to step in,” Sengupta said during the call. \n",
            "At some point, higher yields should attract new buyers, Sengupta and Smith said. But it’s difficult to say how high yields will need to go before that happens — although as the Fed pulls back, it seems the market is about to find out. \n",
            "To be sure, Treasury market liquidity has been thinning for some time now, with a host of factors playing a role, even while the Fed was still scooping up billions of dollars of government debt per month, something it only stopped doing in March. \n",
            "Since then, bond traders have noticed unusually wild swings in what is typically a more staid market. \n",
            "In July, a team of interest-rate strategists at Barclays \n",
            "        BARC,\n",
            "        -0.91%\n",
            "       discussed symptoms of thinning Treasury market in a report prepared for the bank’s clients. \n",
            "These include wider bid-ask spreads. The spread is the amount that brokers and dealers charge for facilitating a trade. According to economists and academics, smaller spreads are typically associated with more-liquid markets, and vice-versa. \n",
            "But wider spreads aren’t the only symptom: Trading volume has declined substantially since the middle of last year, the Barclays team said, as speculators and traders increasingly turn to the Treasury futures markets to take short-term positions.  According to Barclays’ data, average aggregate nominal Treasury trading volume has declined from nearly $3.5 trillion every four weeks at the beginning of 2022 to just above $2 trillion. \n",
            "At the same time, market depth — that is, the dollar amount of bonds on offer via dealers and brokers — has deteriorated substantially since the middle of last year. The Barclays team illustrated this trend with a chart, which is included below. \n",
            "Other measures of bond-market liquidity confirm the trend. For example, the ICE Bank of America Merrill Lynch MOVE Index, a popular gauge of implied bond-market volatility, was above 120 on Wednesday, a level signifying that options traders are bracing for more ructions ahead in the Treasury market. The gauge is similar to the CBOE Volatility Index, or “VIX”, the Wall Street “fear gauge” that measures expected volatility in equity markets. \n",
            "The MOVE index nearly reached 160 back in June, which is not far from 160.3 peak from 2020 seen on March 9 of that year, which was the highest level since the financial crisis. \n",
            "Bloomberg also maintains an index of liquidity in U.S. government securities with a maturity greater than one year. The index is higher when Treasurys are trading further away from “fair value”, which typically happens when liquidity conditions deteriorate. \n",
            "It stood at roughly 2.7 on Wednesday, right around its highest level in more than a decade, if one excludes the spring of 2020. \n",
            "Thinning liquidity has had the biggest impact along the short end of the Treasury curve — since short-dated Treasurys are typically more susceptible to Fed interest-rate hikes, as well as changes in the outlook for inflation. \n",
            "Also,  “off the run” Treasurys, a term used to describe all but the most recent issues of Treasury bonds for each tenor, have been affected more than their “on the run” counterparts. \n",
            "Because of this thinning liquidity, traders and portfolio managers told MarketWatch that they need to be more careful about the sizing and timing of their trades as market conditions grow increasingly volatile. \n",
            "“Liquidity is pretty bad right now,” said John Luke Tyner, a portfolio manager at Aptus Capital Advisors. \n",
            "“We have had four or five days in recent months where the two-year Treasury has moved more than 20 [basis points] in a day. It’s certainly eye opening.”\n",
            "Tyner previously worked on the institutional fixed income desk at Duncan-Williams Inc. and has been analyzing and trading fixed-income products since shortly after graduating from the University of Memphis. \n",
            "Treasury debt is considered a global reserve asset — just like the U.S. dollar is considered a reserve currency. This means it’s widely held by foreign central banks that need access to dollars to help facilitate international trade. \n",
            "To ensure that Treasurys retain this status, market participants must be able to trade them quickly, easily and cheaply, wrote Fed economist Michael Fleming in a 2001 paper entitled “Measuring Treasury Market Liquidity”. \n",
            " Fleming, who still works at the Fed, didn’t respond to a request for comment. But interest-rate strategists at JP Morgan Chase & Co.  \n",
            "        JPM,\n",
            "        -2.00%,\n",
            "       Credit Suisse \n",
            "        CS,\n",
            "        +13.05%\n",
            "        and TD Securities told MarketWatch that maintaining ample liquidity is just as important today — if not more so.\n",
            "The reserve status of Treasurys confers myriad benefits to the U.S. government, including the ability to finance large deficits relatively cheaply. \n",
            "When chaos upended global markets in the spring of 2020, the Treasury market wasn’t spared from the fallout. \n",
            "As the Group of 30’s Working Group on Treasury Market Liquidity recounted in a report recommending tactics for improving the functioning of the Treasury market, the fallout came surprisingly close to causing global credit markets to seize up. \n",
            "As brokers pulled liquidity for fear of being saddled with losses, the Treasury market saw outsize moves that made seemingly little sense. Yields on Treasury bonds with similar maturities became entirely unhinged. \n",
            "Between March 9 and March 18, bid-ask spreads exploded and the number of trade “failures” — which occur when a booked trade fails to settle because one of the two counterparties doesn’t have the money, or the  assets — soared to roughly three times the normal rate. \n",
            "The Federal Reserve eventually rode to the rescue, but market participants had been put on notice, and the Group of 30 decided to explore how a repeat of these market ructions could be avoided. \n",
            "The panel, which was led by former Treasury Secretary and New York Fed President Timothy Geithner, published its report last year, which included  a host of recommendations for making the Treasury market more resilient during times of stress.  A Group of 30 representative was unable to make any of the authors available for comment when contacted by MarketWatch. \n",
            "Recommendations included the establishment of universal clearing of all Treasury trades and repos, establishing regulatory carve-outs to regulatory leverage ratios to allow dealers to warehouse more bonds on their books,  and the establishment of standing repo operations at the Federal Reserve. \n",
            "While most of the recommendations from the report have yet to be implemented, the Fed did establish standing repo facilities for domestic and foreign dealers in July 2021. And the Securities and Exchange Commission is taking steps toward mandating more centralized clearing. \n",
            "However, in a status update released earlier this year, the working group said the Fed facilities didn’t go far enough.\n",
            "On Wednesday, the Securities and Exchange Commission is preparing to announce that it would propose rules to help reform how Treasurys are traded and cleared, including making sure more Treasury trades are centrally cleared, as the Group of 30 recommended, as MarketWatch reported. \n",
            "See: SEC set to advance reforms to head off next crisis in $24 trillion market for U.S. government debt\n",
            "As the Group of 30 noted, SEC Chairman Gary Gensler has expressed support for expanding centralized clearing of Treasurys, which would help improve liquidity during times of stress by helping to ensure that all trades settle on time without any hiccups.\n",
            "Still, if regulators seem complacent when it comes to addressing these risks, it’s probably because they expect that if something does go wrong, the Fed can simply ride to the rescue, as it has in the past. \n",
            "But Bank of America’s Axel believes this assumption is misguided. \n",
            "“It is not structurally sound for the U.S. public debt to become increasingly reliant on Fed QE. The Fed is a lender of last resort to the banking system, not to the federal government,” Axel wrote. \n",
            "—Vivien Lou Chen contributed reporting\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n"
          ]
        }
      ],
      "source": [
        "# getting the sub heading\n",
        "body = sp.find(id =\"js-article__body\")\n",
        "for p in body.find_all('p'):\n",
        "    # print('\\n')\n",
        "    print(p.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Ylu2CuvLoP"
      },
      "source": [
        "# Qeustion 2:\n",
        " \n",
        "As discussed in the demo above, extract the salaries of each of the players from the hoopshype website using the example of how to extract the names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80qCTOTwvHpR"
      },
      "outputs": [],
      "source": [
        "# download the selenium chromedriver executable file and paste the link in the following code\n",
        "# this code should open a new chrome window in your machine\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vficjz3av3To"
      },
      "outputs": [],
      "source": [
        "# this code should open hoopshype website in your newly opened chrome window\n",
        "driver.get('https://hoopshype.com/salaries/players/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnnOitYOwEem"
      },
      "outputs": [],
      "source": [
        "# getting players name list\n",
        "players = driver.find_elements(\"xpath\", '//td[@class=\"name\"]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaBVgLubwGdN",
        "outputId": "3ca2663c-40d7-4e66-b836-b14748302920"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'PLAYER',\n",
              " 'Stephen Curry',\n",
              " 'John Wall',\n",
              " 'Russell Westbrook',\n",
              " 'LeBron James',\n",
              " 'Kevin Durant',\n",
              " 'Bradley Beal',\n",
              " 'Paul George',\n",
              " 'Kawhi Leonard',\n",
              " 'Giannis Antetokounmpo',\n",
              " 'Damian Lillard',\n",
              " 'Klay Thompson',\n",
              " 'Rudy Gobert',\n",
              " 'Khris Middleton',\n",
              " 'Anthony Davis',\n",
              " 'Jimmy Butler',\n",
              " 'Tobias Harris',\n",
              " 'Trae Young',\n",
              " 'Zach LaVine',\n",
              " 'Luka Doncic',\n",
              " 'Kyrie Irving',\n",
              " 'Kemba Walker',\n",
              " 'Ben Simmons',\n",
              " 'Pascal Siakam',\n",
              " 'Karl-Anthony Towns',\n",
              " 'Devin Booker',\n",
              " 'Kristaps Porzingis',\n",
              " 'Jrue Holiday',\n",
              " 'Joel Embiid',\n",
              " 'Andrew Wiggins',\n",
              " 'CJ McCollum',\n",
              " 'Nikola Jokic',\n",
              " 'James Harden',\n",
              " 'Jamal Murray',\n",
              " 'Brandon Ingram',\n",
              " \"D'Angelo Russell\",\n",
              " 'Michael Porter',\n",
              " 'Shai Gilgeous-Alexander',\n",
              " 'Donovan Mitchell',\n",
              " 'Deandre Ayton',\n",
              " 'Jayson Tatum',\n",
              " 'Bam Adebayo',\n",
              " \"De'Aaron Fox\",\n",
              " 'Gordon Hayward',\n",
              " 'Jaren Jackson Jr',\n",
              " 'Kevin Love',\n",
              " 'Jaylen Brown',\n",
              " 'Chris Paul',\n",
              " 'Kyle Lowry',\n",
              " 'Jalen Brunson',\n",
              " 'DeMar DeRozan',\n",
              " 'Al Horford',\n",
              " 'Draymond Green',\n",
              " 'Julius Randle',\n",
              " 'John Collins',\n",
              " 'Mike Conley',\n",
              " 'Malcolm Brogdon',\n",
              " 'Anfernee Simons',\n",
              " 'Nikola Vucevic',\n",
              " 'Terry Rozier',\n",
              " 'Fred VanVleet',\n",
              " 'Buddy Hield',\n",
              " 'Jerami Grant',\n",
              " 'Spencer Dinwiddie',\n",
              " 'Mikal Bridges',\n",
              " 'Jarrett Allen',\n",
              " 'Nicolas Batum',\n",
              " 'Aaron Gordon',\n",
              " 'Tim Hardaway Jr',\n",
              " 'Eric Gordon',\n",
              " 'Bojan Bogdanovic',\n",
              " 'Lonzo Ball',\n",
              " 'Danilo Gallinari',\n",
              " 'Caris LeVert',\n",
              " 'Clint Capela',\n",
              " 'Joe Harris',\n",
              " 'Domantas Sabonis',\n",
              " 'Harrison Barnes',\n",
              " 'Evan Fournier',\n",
              " 'Bogdan Bogdanovic',\n",
              " 'Myles Turner',\n",
              " 'Steven Adams',\n",
              " 'Gary Trent Jr',\n",
              " 'Jonathan Isaac',\n",
              " 'OG Anunoby',\n",
              " 'Marcus Smart',\n",
              " 'Mitchell Robinson',\n",
              " 'Duncan Robinson',\n",
              " 'Derrick White',\n",
              " 'Norman Powell',\n",
              " 'Dejounte Murray',\n",
              " 'Collin Sexton',\n",
              " 'Markelle Fultz',\n",
              " 'Lauri Markkanen',\n",
              " 'Marcus Morris',\n",
              " 'Davis Bertans',\n",
              " 'Jusuf Nurkic',\n",
              " 'Malik Beasley',\n",
              " 'Luguentz Dort',\n",
              " 'Tyus Jones',\n",
              " 'Jonas Valanciunas',\n",
              " 'Derrick Rose',\n",
              " 'Kevin Huerter',\n",
              " 'Will Barton',\n",
              " 'Christian Wood',\n",
              " 'Wendell Carter',\n",
              " 'Kentavious Caldwell-Pope',\n",
              " 'Brook Lopez',\n",
              " 'Doug McDermott',\n",
              " 'Luke Kennard',\n",
              " 'Zion Williamson',\n",
              " 'Jordan Clarkson',\n",
              " 'Gary Harris',\n",
              " 'Kyle Kuzma',\n",
              " 'Patrick Beverley',\n",
              " 'Josh Hart',\n",
              " 'Kelly Olynyk',\n",
              " 'Chris Boucher',\n",
              " 'Kelly Oubre',\n",
              " 'Marvin Bagley',\n",
              " 'Dorian Finney-Smith',\n",
              " 'Robert Covington',\n",
              " 'Josh Richardson',\n",
              " 'Ja Morant',\n",
              " 'Devonte Graham',\n",
              " 'Terrence Ross',\n",
              " 'Dillon Brooks',\n",
              " 'Richaun Holmes',\n",
              " 'Reggie Jackson',\n",
              " 'Dwight Powell',\n",
              " 'Paolo Banchero',\n",
              " 'Robert Williams',\n",
              " 'RJ Barrett',\n",
              " 'Bobby Portis',\n",
              " 'DeAndre Jordan',\n",
              " 'Anthony Edwards',\n",
              " 'Cade Cunningham',\n",
              " 'PJ Tucker',\n",
              " 'Mo Bamba',\n",
              " 'Talen Horton-Tucker',\n",
              " 'Derrick Favors',\n",
              " 'Jae Crowder',\n",
              " 'Ivica Zubac',\n",
              " 'Reggie Bullock',\n",
              " 'Alec Burks',\n",
              " 'Danny Green',\n",
              " 'Chet Holmgren',\n",
              " \"De'Andre Hunter\",\n",
              " 'Larry Nance Jr',\n",
              " 'James Wiseman',\n",
              " 'Landry Shamet',\n",
              " 'Malik Monk',\n",
              " 'Jalen Green',\n",
              " 'Jakob Poeltl',\n",
              " 'Nerlens Noel',\n",
              " 'Dario Saric',\n",
              " \"Royce O'Neale\",\n",
              " 'Monte Morris',\n",
              " 'Mason Plumlee',\n",
              " 'Alex Caruso',\n",
              " 'Maxi Kleber',\n",
              " 'Darius Garland',\n",
              " 'Jabari Smith',\n",
              " 'Kyle Anderson',\n",
              " 'Victor Oladipo',\n",
              " 'Daniel Theis',\n",
              " 'LaMelo Ball',\n",
              " 'Nicolas Claxton',\n",
              " 'Grayson Allen',\n",
              " 'Seth Curry',\n",
              " 'Evan Mobley',\n",
              " 'Gary Payton II',\n",
              " \"De'Anthony Melton\",\n",
              " 'JaMychal Green',\n",
              " 'TJ McConnell',\n",
              " 'Keegan Murray',\n",
              " 'Kevon Looney',\n",
              " 'Thaddeus Young',\n",
              " 'Isaiah Hartenstein',\n",
              " 'Delon Wright',\n",
              " 'Patrick Williams',\n",
              " 'Scottie Barnes',\n",
              " 'Dewayne Dedmon',\n",
              " 'Cedi Osman',\n",
              " 'Coby White',\n",
              " 'Zach Collins',\n",
              " 'Taurean Prince',\n",
              " 'Jaden Ivey',\n",
              " \"Jae'Sean Tate\",\n",
              " 'Isaac Okoro',\n",
              " 'Cody Martin',\n",
              " 'Jalen Suggs',\n",
              " 'Jaxson Hayes',\n",
              " 'Khem Birch',\n",
              " 'Bennedict Mathurin',\n",
              " 'Patrick Mills',\n",
              " 'Bruce Brown',\n",
              " 'Caleb Martin',\n",
              " 'Lonnie Walker',\n",
              " 'Joe Ingles',\n",
              " 'Onyeka Okongwu',\n",
              " 'Justin Holiday',\n",
              " 'Josh Giddey',\n",
              " 'Rui Hachimura',\n",
              " 'Rudy Gay',\n",
              " 'Ty Jerome',\n",
              " 'Jevon Carter',\n",
              " 'Shaedon Sharpe',\n",
              " 'Cameron Payne',\n",
              " 'Otto Porter',\n",
              " 'Cam Reddish',\n",
              " 'Cameron Johnson',\n",
              " 'Ricky Rubio',\n",
              " 'Killian Hayes',\n",
              " 'PJ Washington',\n",
              " 'Jonathan Kuminga',\n",
              " 'Pat Connaughton',\n",
              " 'Tyler Herro',\n",
              " 'Romeo Langford',\n",
              " 'Dyson Daniels',\n",
              " 'JaVale McGee',\n",
              " 'Obi Toppin',\n",
              " 'Franz Wagner',\n",
              " 'Kendrick Nunn',\n",
              " 'Hamidou Diallo',\n",
              " 'Cory Joseph',\n",
              " 'Garrett Temple',\n",
              " 'Torrey Craig',\n",
              " 'Jeremy Sochan',\n",
              " 'David Nwaba',\n",
              " 'Nickeil Alexander-Walker',\n",
              " 'Furkan Korkmaz',\n",
              " 'Deni Avdija',\n",
              " 'Davion Mitchell',\n",
              " 'Johnny Davis',\n",
              " 'Goga Bitadze',\n",
              " 'Ishmael Smith',\n",
              " 'Jalen Smith',\n",
              " 'Ziaire Williams',\n",
              " 'Ousmane Dieng',\n",
              " 'Moe Harkless',\n",
              " 'Donte DiVincenzo',\n",
              " 'Jeff Green',\n",
              " 'Devin Vassell',\n",
              " 'Matisse Thybulle',\n",
              " 'Jarred Vanderbilt',\n",
              " 'James Bouknight',\n",
              " 'Brandon Clarke',\n",
              " 'Jalen Williams',\n",
              " 'Grant Williams',\n",
              " 'Darius Bazley',\n",
              " 'Tyrese Haliburton',\n",
              " 'Nassir Little',\n",
              " 'Josh Primo',\n",
              " 'Jalen Duren',\n",
              " 'Danuel House',\n",
              " 'Justise Winslow',\n",
              " 'Dylan Windler',\n",
              " 'Kira Lewis',\n",
              " 'George Hill',\n",
              " 'Terence Davis',\n",
              " 'Chris Duarte',\n",
              " 'Alex Len',\n",
              " 'Ochai Agbaji',\n",
              " 'Jordan Poole',\n",
              " 'Keldon Johnson',\n",
              " 'Sviatoslav Mykhailiuk',\n",
              " 'Aaron Nesmith',\n",
              " 'Moses Moody',\n",
              " 'Mark Williams',\n",
              " 'Cole Anthony',\n",
              " 'Corey Kispert',\n",
              " 'AJ Griffin',\n",
              " 'Boban Marjanovic',\n",
              " 'Mike Muscala',\n",
              " 'Georges Niang',\n",
              " 'Isaiah Stewart',\n",
              " 'Chuma Okeke',\n",
              " 'Trey Burke',\n",
              " 'Amir Coffey',\n",
              " 'Alperen Sengun',\n",
              " 'Tari Eason',\n",
              " 'Aleksej Pokusevski',\n",
              " 'Kevin Porter',\n",
              " 'Trey Murphy',\n",
              " 'Simone Fontecchio',\n",
              " 'Andre Drummond',\n",
              " 'Derrick Jones',\n",
              " 'Dalen Terry',\n",
              " 'Josh Green',\n",
              " 'Jake LaRavia',\n",
              " 'Tre Mann',\n",
              " 'Jordan Nwora',\n",
              " 'Kevin Knox',\n",
              " 'Sterling Brown',\n",
              " 'Saddiq Bey',\n",
              " 'Malaki Branham',\n",
              " 'Kai Jones',\n",
              " 'Blake Griffin',\n",
              " 'Markieff Morris',\n",
              " 'Goran Dragic',\n",
              " 'Robin Lopez',\n",
              " 'Andre Iguodala',\n",
              " 'James Johnson',\n",
              " 'Udonis Haslem',\n",
              " 'Serge Ibaka',\n",
              " 'Wesley Matthews',\n",
              " 'Austin Rivers',\n",
              " 'Bismack Biyombo',\n",
              " 'Kent Bazemore',\n",
              " 'Gorgui Dieng',\n",
              " 'Taj Gibson',\n",
              " 'Andrew Nicholson',\n",
              " 'Precious Achiuwa',\n",
              " 'Christian Braun',\n",
              " 'Jalen Johnson',\n",
              " 'Tyrese Maxey',\n",
              " 'Bruno Fernando',\n",
              " 'Walker Kessler',\n",
              " 'Keon Johnson',\n",
              " 'Dennis Schroeder',\n",
              " 'Cody Zeller',\n",
              " 'TJ Warren',\n",
              " 'Langston Galloway',\n",
              " 'Matthew Dellavedova',\n",
              " 'Trey Lyles',\n",
              " 'Zeke Nnaji',\n",
              " 'David Roddy',\n",
              " 'Isaiah Jackson',\n",
              " 'MarJon Beauchamp',\n",
              " 'Usman Garuba',\n",
              " 'Leandro Bolmaro',\n",
              " 'Montrezl Harrell',\n",
              " 'Frank Kaminsky',\n",
              " 'Noah Vonleh',\n",
              " 'Raul Neto',\n",
              " 'Willy Hernangomez',\n",
              " 'RJ Hampton',\n",
              " 'Malik Fitts',\n",
              " 'Blake Wesley',\n",
              " 'Josh Christopher',\n",
              " 'Stanley Johnson',\n",
              " 'Immanuel Quickley',\n",
              " 'Wendell Moore',\n",
              " 'John Konchar',\n",
              " 'Damian Jones',\n",
              " 'Jake Layman',\n",
              " 'Rodney McGruder',\n",
              " 'Bryn Forbes',\n",
              " 'Ryan Arcidiacono',\n",
              " 'Timothe Luwawu',\n",
              " 'Juan Hernangomez',\n",
              " 'Quentin Grimes',\n",
              " 'Andrew Nembhard',\n",
              " 'Nikola Jovic',\n",
              " 'Payton Pritchard',\n",
              " 'Vlatko Cancar',\n",
              " 'Patrick Baldwin',\n",
              " 'TyTy Washington',\n",
              " 'Bones Hyland',\n",
              " 'Bol Bol',\n",
              " 'Peyton Watson',\n",
              " 'Marquese Chriss',\n",
              " 'Joe Wieskamp',\n",
              " 'Udoka Azubuike',\n",
              " 'Jaden McDaniels',\n",
              " 'Jordan McLaughlin',\n",
              " 'Malachi Flynn',\n",
              " 'Cam Thomas',\n",
              " 'Luke Kornet',\n",
              " 'DJ Wilson',\n",
              " 'Justin Jackson',\n",
              " 'Dennis Smith',\n",
              " 'Shaquille Harrison',\n",
              " 'Juan Toscano-Anderson',\n",
              " 'PJ Dozier',\n",
              " 'Frank Jackson',\n",
              " 'Damion Lee',\n",
              " 'Quinn Cook',\n",
              " 'Josh Jackson',\n",
              " 'Desmond Bane',\n",
              " 'Jaden Springer',\n",
              " \"Day'Ron Sharpe\",\n",
              " 'Santiago Aldama',\n",
              " 'Tony Bradley',\n",
              " 'Frank Ntilikina',\n",
              " 'Kenrich Williams',\n",
              " 'Garrison Mathews',\n",
              " 'Jaylin Williams',\n",
              " 'Caleb Houstan',\n",
              " 'Jeremiah Robinson-Earl',\n",
              " 'Shake Milton',\n",
              " 'Edmond Sumner',\n",
              " 'Aaron Holiday',\n",
              " 'Yuta Watanabe',\n",
              " 'Theo Pinson',\n",
              " 'Troy Brown',\n",
              " 'Thomas Bryant',\n",
              " 'Josh Okogie',\n",
              " 'Drew Eubanks',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "players_list = []\n",
        "for p in range(len(players)): players_list.append(players[p].text)\n",
        "players_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn-HG5DFwcg7"
      },
      "outputs": [],
      "source": [
        "# getting players name list\n",
        "salaries = driver.find_elements(\"xpath\", '//td[@class=\"hh-salaries-sorted\"]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrKXmPDEwpFp",
        "outputId": "994d2f35-aee7-4138-ccdc-115ca1e6ea28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2022/23',\n",
              " '$48,070,014',\n",
              " '$47,345,760',\n",
              " '$47,063,478',\n",
              " '$44,474,988',\n",
              " '$44,119,845',\n",
              " '$43,279,250',\n",
              " '$42,492,568',\n",
              " '$42,492,492',\n",
              " '$42,492,492',\n",
              " '$42,492,492',\n",
              " '$40,600,080',\n",
              " '$38,172,414',\n",
              " '$37,984,276',\n",
              " '$37,980,720',\n",
              " '$37,653,300',\n",
              " '$37,633,050',\n",
              " '$37,096,500',\n",
              " '$37,096,500',\n",
              " '$37,096,500',\n",
              " '$36,934,550',\n",
              " '$36,596,549',\n",
              " '$35,448,672',\n",
              " '$35,448,672',\n",
              " '$33,833,400',\n",
              " '$33,833,400',\n",
              " '$33,833,400',\n",
              " '$33,665,040',\n",
              " '$33,616,770',\n",
              " '$33,616,770',\n",
              " '$33,333,333',\n",
              " '$33,047,803',\n",
              " '$33,000,000',\n",
              " '$31,650,600',\n",
              " '$31,650,600',\n",
              " '$31,377,750',\n",
              " '$30,913,750',\n",
              " '$30,913,750',\n",
              " '$30,913,750',\n",
              " '$30,913,750',\n",
              " '$30,351,780',\n",
              " '$30,351,780',\n",
              " '$30,351,780',\n",
              " '$30,075,000',\n",
              " '$28,946,605',\n",
              " '$28,942,830',\n",
              " '$28,741,071',\n",
              " '$28,400,000',\n",
              " '$28,333,334',\n",
              " '$27,733,332',\n",
              " '$27,300,000',\n",
              " '$26,500,000',\n",
              " '$25,806,468',\n",
              " '$23,760,000',\n",
              " '$23,500,000',\n",
              " '$22,680,000',\n",
              " '$22,600,000',\n",
              " '$22,321,429',\n",
              " '$22,000,000',\n",
              " '$21,486,316',\n",
              " '$21,250,000',\n",
              " '$21,177,750',\n",
              " '$20,955,000',\n",
              " '$20,171,427',\n",
              " '$20,100,000',\n",
              " '$20,000,000',\n",
              " '$19,700,319',\n",
              " '$19,690,909',\n",
              " '$19,602,273',\n",
              " '$19,568,360',\n",
              " '$19,550,000',\n",
              " '$19,534,884',\n",
              " '$19,479,000',\n",
              " '$18,796,296',\n",
              " '$18,706,896',\n",
              " '$18,642,857',\n",
              " '$18,500,000',\n",
              " '$18,352,273',\n",
              " '$18,000,000',\n",
              " '$18,000,000',\n",
              " '$18,000,000',\n",
              " '$17,926,829',\n",
              " '$17,505,000',\n",
              " '$17,400,000',\n",
              " '$17,357,143',\n",
              " '$17,207,142',\n",
              " '$17,045,454',\n",
              " '$16,902,000',\n",
              " '$16,892,857',\n",
              " '$16,758,621',\n",
              " '$16,571,120',\n",
              " '$16,500,000',\n",
              " '$16,500,000',\n",
              " '$16,475,454',\n",
              " '$16,372,093',\n",
              " '$16,000,000',\n",
              " '$15,625,000',\n",
              " '$15,558,035',\n",
              " '$15,277,778',\n",
              " '$15,000,000',\n",
              " '$14,700,000',\n",
              " '$14,520,730',\n",
              " '$14,508,929',\n",
              " '$14,375,000',\n",
              " '$14,317,459',\n",
              " '$14,150,000',\n",
              " '$14,004,703',\n",
              " '$13,906,976',\n",
              " '$13,750,000',\n",
              " '$13,745,455',\n",
              " '$13,534,817',\n",
              " '$13,340,000',\n",
              " '$13,000,000',\n",
              " '$13,000,000',\n",
              " '$13,000,000',\n",
              " '$12,960,000',\n",
              " '$12,804,878',\n",
              " '$12,690,000',\n",
              " '$12,600,000',\n",
              " '$12,500,000',\n",
              " '$12,402,000',\n",
              " '$12,307,692',\n",
              " '$12,196,094',\n",
              " '$12,119,440',\n",
              " '$11,550,000',\n",
              " '$11,500,000',\n",
              " '$11,400,000',\n",
              " '$11,215,260',\n",
              " '$11,215,260',\n",
              " '$11,080,125',\n",
              " '$11,055,120',\n",
              " '$10,937,502',\n",
              " '$10,900,634',\n",
              " '$10,843,350',\n",
              " '$10,733,758',\n",
              " '$10,733,400',\n",
              " '$10,552,800',\n",
              " '$10,490,000',\n",
              " '$10,300,000',\n",
              " '$10,260,000',\n",
              " '$10,183,800',\n",
              " '$10,183,800',\n",
              " '$10,123,457',\n",
              " '$10,012,800',\n",
              " '$10,012,800',\n",
              " '$10,000,000',\n",
              " '$9,891,240',\n",
              " '$9,835,881',\n",
              " '$9,672,727',\n",
              " '$9,603,360',\n",
              " '$9,500,000',\n",
              " '$9,472,219',\n",
              " '$9,441,840',\n",
              " '$9,398,148',\n",
              " '$9,240,000',\n",
              " '$9,240,000',\n",
              " '$9,200,000',\n",
              " '$9,125,000',\n",
              " '$9,080,417',\n",
              " '$9,030,000',\n",
              " '$9,000,000',\n",
              " '$8,920,794',\n",
              " '$8,882,640',\n",
              " '$8,780,488',\n",
              " '$8,750,000',\n",
              " '$8,694,369',\n",
              " '$8,623,920',\n",
              " '$8,500,000',\n",
              " '$8,500,000',\n",
              " '$8,496,653',\n",
              " '$8,478,720',\n",
              " '$8,300,000',\n",
              " '$8,250,000',\n",
              " '$8,200,000',\n",
              " '$8,100,000',\n",
              " '$8,008,440',\n",
              " '$8,000,000',\n",
              " '$8,000,000',\n",
              " '$7,804,879',\n",
              " '$7,804,878',\n",
              " '$7,775,400',\n",
              " '$7,644,600',\n",
              " '$7,448,674',\n",
              " '$7,426,088',\n",
              " '$7,413,955',\n",
              " '$7,350,000',\n",
              " '$7,295,000',\n",
              " '$7,252,200',\n",
              " '$7,065,217',\n",
              " '$7,040,880',\n",
              " '$7,000,000',\n",
              " '$6,922,320',\n",
              " '$6,803,012',\n",
              " '$6,667,500',\n",
              " '$6,586,800',\n",
              " '$6,479,000',\n",
              " '$6,479,000',\n",
              " '$6,479,000',\n",
              " '$6,479,000',\n",
              " '$6,479,000',\n",
              " '$6,395,160',\n",
              " '$6,292,440',\n",
              " '$6,287,400',\n",
              " '$6,263,188',\n",
              " '$6,184,500',\n",
              " '$6,122,190',\n",
              " '$6,025,000',\n",
              " '$6,012,960',\n",
              " '$6,000,000',\n",
              " '$6,000,000',\n",
              " '$5,954,454',\n",
              " '$5,887,899',\n",
              " '$5,853,659',\n",
              " '$5,837,760',\n",
              " '$5,808,435',\n",
              " '$5,739,840',\n",
              " '$5,728,393',\n",
              " '$5,722,116',\n",
              " '$5,634,257',\n",
              " '$5,508,600',\n",
              " '$5,461,219',\n",
              " '$5,348,280',\n",
              " '$5,258,280',\n",
              " '$5,250,000',\n",
              " '$5,200,000',\n",
              " '$5,155,500',\n",
              " '$5,155,500',\n",
              " '$5,121,951',\n",
              " '$5,063,520',\n",
              " '$5,022,000',\n",
              " '$5,009,633',\n",
              " '$5,000,000',\n",
              " '$4,916,160',\n",
              " '$4,833,600',\n",
              " '$4,810,320',\n",
              " '$4,765,339',\n",
              " '$4,725,000',\n",
              " '$4,670,160',\n",
              " '$4,591,680',\n",
              " '$4,569,840',\n",
              " '$4,564,980',\n",
              " '$4,500,000',\n",
              " '$4,500,000',\n",
              " '$4,437,000',\n",
              " '$4,379,526',\n",
              " '$4,374,000',\n",
              " '$4,362,240',\n",
              " '$4,343,919',\n",
              " '$4,341,480',\n",
              " '$4,306,281',\n",
              " '$4,264,628',\n",
              " '$4,215,120',\n",
              " '$4,171,548',\n",
              " '$4,144,320',\n",
              " '$4,124,280',\n",
              " '$4,105,000',\n",
              " '$4,097,561',\n",
              " '$4,037,277',\n",
              " '$4,004,280',\n",
              " '$4,000,000',\n",
              " '$4,000,000',\n",
              " '$3,936,960',\n",
              " '$3,918,600',\n",
              " '$3,918,360',\n",
              " '$3,901,399',\n",
              " '$3,873,024',\n",
              " '$3,846,895',\n",
              " '$3,804,360',\n",
              " '$3,740,160',\n",
              " '$3,722,040',\n",
              " '$3,613,680',\n",
              " '$3,552,840',\n",
              " '$3,536,160',\n",
              " '$3,500,000',\n",
              " '$3,500,000',\n",
              " '$3,465,000',\n",
              " '$3,433,320',\n",
              " '$3,433,320',\n",
              " '$3,423,750',\n",
              " '$3,395,062',\n",
              " '$3,375,360',\n",
              " '$3,359,160',\n",
              " '$3,261,480',\n",
              " '$3,217,631',\n",
              " '$3,206,520',\n",
              " '$3,205,128',\n",
              " '$3,200,000',\n",
              " '$3,200,000',\n",
              " '$3,191,400',\n",
              " '$3,098,400',\n",
              " '$3,047,640',\n",
              " '$3,046,200',\n",
              " '$3,000,000',\n",
              " '$3,000,000',\n",
              " '$3,000,000',\n",
              " '$2,959,080',\n",
              " '$2,925,600',\n",
              " '$2,909,040',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,905,851',\n",
              " '$2,844,429',\n",
              " '$2,840,160',\n",
              " '$2,808,600',\n",
              " '$2,792,640',\n",
              " '$2,726,880',\n",
              " '$2,717,391',\n",
              " '$2,696,400',\n",
              " '$2,681,040',\n",
              " '$2,641,682',\n",
              " '$2,641,682',\n",
              " '$2,628,597',\n",
              " '$2,628,597',\n",
              " '$2,628,597',\n",
              " '$2,625,000',\n",
              " '$2,617,800',\n",
              " '$2,588,640',\n",
              " '$2,573,760',\n",
              " '$2,485,200',\n",
              " '$2,471,160',\n",
              " '$2,471,160',\n",
              " '$2,463,490',\n",
              " '$2,463,490',\n",
              " '$2,463,490',\n",
              " '$2,463,490',\n",
              " '$2,443,581',\n",
              " '$2,412,840',\n",
              " '$2,391,307',\n",
              " '$2,385,480',\n",
              " '$2,372,160',\n",
              " '$2,351,521',\n",
              " '$2,316,240',\n",
              " '$2,306,520',\n",
              " '$2,300,000',\n",
              " '$2,298,385',\n",
              " '$2,298,385',\n",
              " '$2,298,385',\n",
              " '$2,298,385',\n",
              " '$2,298,385',\n",
              " '$2,298,385',\n",
              " '$2,298,385',\n",
              " '$2,277,000',\n",
              " '$2,244,111',\n",
              " '$2,239,920',\n",
              " '$2,239,200',\n",
              " '$2,234,359',\n",
              " '$2,226,000',\n",
              " '$2,210,040',\n",
              " '$2,201,520',\n",
              " '$2,200,000',\n",
              " '$2,193,960',\n",
              " '$2,193,920',\n",
              " '$2,175,000',\n",
              " '$2,174,880',\n",
              " '$2,161,440',\n",
              " '$2,160,000',\n",
              " '$2,145,720',\n",
              " '$2,138,160',\n",
              " '$2,133,278',\n",
              " '$2,133,278',\n",
              " '$2,133,278',\n",
              " '$2,133,278',\n",
              " '$2,133,278',\n",
              " '$2,133,278',\n",
              " '$2,133,278',\n",
              " '$2,133,278',\n",
              " '$2,133,278',\n",
              " '$2,133,278',\n",
              " '$2,133,278',\n",
              " '$2,130,240',\n",
              " '$2,125,200',\n",
              " '$2,109,480',\n",
              " '$2,094,120',\n",
              " '$2,036,318',\n",
              " '$2,036,318',\n",
              " '$2,000,000',\n",
              " '$2,000,000',\n",
              " '$2,000,000',\n",
              " '$2,000,000',\n",
              " '$2,000,000',\n",
              " '$1,997,718',\n",
              " '$1,968,175',\n",
              " '$1,968,175',\n",
              " '$1,968,175',\n",
              " '$1,968,175',\n",
              " '$1,968,175',\n",
              " '$1,968,175',\n",
              " '$1,968,175',\n",
              " '$1,968,175',\n",
              " '$1,968,175',\n",
              " '$1,930,681',\n",
              " '$1,930,681',\n",
              " '$1,930,681',\n",
              " '$1,930,681',\n",
              " '$1,930,681',\n",
              " '$1,930,681',\n",
              " '$1,930,681',\n",
              " '$1,910,860',\n",
              " '$1,902,133',\n",
              " '$1,902,133',\n",
              " '$1,902,133',\n",
              " '$1,902,133',\n",
              " '$1,902,133',\n",
              " '$1,902,133',\n",
              " '$1,902,133',\n",
              " '$1,902,133',\n",
              " '$1,902,133',\n",
              " '$1,902,133',\n",
              " '$1,901,625',\n",
              " '$1,900,000',\n",
              " '$1,886,090',\n",
              " '$1,878,720',\n",
              " '$1,878,720',\n",
              " '$1,878,720',\n",
              " '$1,878,720',\n",
              " '$1,846,738',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,836,090',\n",
              " '$1,815,677',\n",
              " '$1,815,677',\n",
              " '$1,815,677',\n",
              " '$1,785,000',\n",
              " '$1,782,621',\n",
              " '$1,782,621',\n",
              " '$1,782,621',\n",
              " '$1,782,621',\n",
              " '$1,782,621',\n",
              " '$1,782,621',\n",
              " '$1,782,621',\n",
              " '$1,782,621',\n",
              " '$1,782,621',\n",
              " '$1,782,621',\n",
              " '$1,752,638',\n",
              " '$1,752,638',\n",
              " '$1,752,638',\n",
              " '$1,639,842',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,637,966',\n",
              " '$1,575,000',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,563,518',\n",
              " '$1,500,000',\n",
              " '$1,300,000',\n",
              " '$1,200,000',\n",
              " '$1,068,200',\n",
              " '$1,067,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$1,017,781',\n",
              " '$999,200',\n",
              " '$702,311',\n",
              " '$633,891',\n",
              " '$576,230',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$508,891',\n",
              " '$502,080',\n",
              " '$502,080',\n",
              " '$502,080',\n",
              " '$502,080',\n",
              " '$502,080',\n",
              " '$502,080',\n",
              " '$333,333',\n",
              " '$300,000',\n",
              " '$268,032',\n",
              " '$122,741']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "salaries_list = []\n",
        "for p in range(len(salaries)): \n",
        "    if salaries[p].text:\n",
        "        salaries_list.append(salaries[p].text)\n",
        "salaries_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXkb29z1yvtz",
        "outputId": "e0224de8-7176-42f8-b942-e69414810e04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "599"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "salaries_list.pop(0)\n",
        "len(salaries_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS_Uk14izkPb"
      },
      "source": [
        "# Question 3:\n",
        "\n",
        "Apart from that choose any 2 websites of your choice and extract meaningful and structured information from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVHk1Sd-0BWp"
      },
      "source": [
        "## 1. Website: flipkart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjNk0Wu34IdU"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.flipkart.com/sg-super-cover-english-willow-cricket-bat/p/itmfegrtwqhqwcbw\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9puhHW74ZoK",
        "outputId": "46363dd4-f677-4a5a-a3ae-045fbf69da7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ],
      "source": [
        "# hitting the url and getting the response\n",
        "res = requests.get(url)\n",
        "print(res.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsgR9E944q4y"
      },
      "outputs": [],
      "source": [
        "# creating a soup object from the returned html page\n",
        "sp = soup(res.text, \"html.parser\")\n",
        "# sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX23pHr94uiM",
        "outputId": "1a265bab-c278-4e21-ab0e-71b45631cff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SG SUPER COVER English Willow Cricket  Bat  (1.180-1.250 kg)\n"
          ]
        }
      ],
      "source": [
        "# getting the title\n",
        "print(sp.find(\"span\", {\"class\" : \"B_NuCI\"}).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NPHd3Fk43Lz",
        "outputId": "494d5e37-e4b4-4bba-b58b-dfd161c55a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "₹3,676\n"
          ]
        }
      ],
      "source": [
        "# getting the title\n",
        "print(sp.find(\"div\", {\"class\" : \"_30jeq3 _16Jk6d\"}).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72y5fiv44_9Q",
        "outputId": "98dce029-b776-4cc6-e96e-5c29157d9b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SG Super Cover English willow Cricket Bat is made of premium quality material which provides you long lasting durability and optimum performancePerfect for intermediate and professional level players. It is considered as Top grade Premium English Willow Cricket bat and player will get around 3 to 6 straight grains in it. SG used best technique to process willow which provides optimized in the bat which will offer best durability and optimum performance.\n"
          ]
        }
      ],
      "source": [
        "print(sp.find(\"div\", {\"class\" : \"_1mXcCf RmoJUa\"}).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXjnxvL45yGh",
        "outputId": "a67fa18d-7b8f-43d9-b63b-34f6c903f1b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://rukminim1.flixcart.com/image/416/416/jvfk58w0/bat/a/s/j/1-180-1-250-super-cover-short-handle-super-cover-sg-original-imaffeu2epzy7uyf.jpeg?q=70\n"
          ]
        }
      ],
      "source": [
        "# getting the image\n",
        "print(sp.find(\"div\", {\"class\" : \"CXW8mj _3nMexc\"}).img[\"src\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqmiqvh26G8G",
        "outputId": "e914c037-067e-41ac-c00b-4851143c6e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nice super magnificent I won the World Cup it's perfect for training flipcart is wonder full super Iam very luckyREAD MORE\n",
            "This is excellent bat but it is take time to settle downFor me I took 3 months for perfect it all real bat is like this worth for moneyREAD MORE\n",
            "It is good for practice nice for hitting painted ni grains but nice ping good for hittingREAD MORE\n",
            "very very  goodREAD MORE\n",
            "THE BAT WAS VERY LIGHT, IT WAS ORIGINAL BAT AS I SAW IN SG OFFICIAL WEBSITE. ITS ACTUAL PRICE WAS NEARLY 2500 JUNIOR BAT. ORIGINAL ENGLISH WILLOW BAT WHICH WAS PAINTED, THE MID STROKE WAS AWESOME. THE BALL WAS SAILING AWAY, EVEN I AM A FAN OF SG BATS,THIS WAS MY SECOND BAT.IT IS SUITABLE FOR 13-14 YEARS AS WELL. IT WILL BE BIT HEAVY FOR 11-12 YEARS. EVEN IT HAS A REAL BARCODE OF SG.. IT IS GRADE 2 BAT WHICH IS USEFUL FOR BEGINNER,INTERMIDIATE LEVEL..TRUST ME AND BUY THIS BAT. ITS SO CHEAPREAD MORE\n",
            "Based on price this is the best english willowREAD MORE\n",
            "best bat for me.hit 3 sixes in my 1st match.good stroke and light weight..luv u Flipkart for fast delivery...READ MORE\n",
            "Best batREAD MORE\n",
            "nice willow at this price worth for money just go for it!!!!READ MORE\n",
            "Good productREAD MORE\n"
          ]
        }
      ],
      "source": [
        "# getting the sub review comments.\n",
        "body = sp.find_all(\"div\", {\"class\" : \"t-ZTKy\"})\n",
        "for p in body:\n",
        "    # print('\\n')\n",
        "    print(p.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE9NKOe39Csr"
      },
      "source": [
        "## 2. Visual Odometry "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWnB_rzl9F9m"
      },
      "outputs": [],
      "source": [
        "url = \"https://guvencetinkaya.medium.com/visual-odometry-vs-visual-slam-cdda75df592\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTyQndvn90h_",
        "outputId": "0d21866d-7677-4021-9863-d04c246ed332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ],
      "source": [
        "res = requests.get(url)\n",
        "print(res.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS0Tjdvu95DF"
      },
      "outputs": [],
      "source": [
        "sp = soup(res.text, \"html.parser\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffy5BheC_fRk",
        "outputId": "77d616e0-52f7-4970-b88b-04cfa5ad1fa3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<!DOCTYPE doctype html>\n",
              "<html lang=\"en\"><head><title data-rh=\"true\">Visual Odometry vs. Visual SLAM vs. Structure-from-Motion | by GUVEN CETINKAYA | Medium</title><meta charset=\"utf-8\" data-rh=\"true\"/><meta content=\"width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1\" data-rh=\"true\" name=\"viewport\"/><meta content=\"#000000\" data-rh=\"true\" name=\"theme-color\"/><meta content=\"Medium\" data-rh=\"true\" name=\"twitter:app:name:iphone\"/><meta content=\"828256236\" data-rh=\"true\" name=\"twitter:app:id:iphone\"/><meta content=\"Medium\" data-rh=\"true\" property=\"al:ios:app_name\"/><meta content=\"828256236\" data-rh=\"true\" property=\"al:ios:app_store_id\"/><meta content=\"com.medium.reader\" data-rh=\"true\" property=\"al:android:package\"/><meta content=\"542599432471018\" data-rh=\"true\" property=\"fb:app_id\"/><meta content=\"Medium\" data-rh=\"true\" property=\"og:site_name\"/><meta content=\"article\" data-rh=\"true\" property=\"og:type\"/><meta content=\"2022-06-03T07:30:54.240Z\" data-rh=\"true\" property=\"article:published_time\"/><meta content=\"Visual Odometry vs. Visual SLAM vs. Structure-from-Motion | by GUVEN CETINKAYA | Medium\" data-rh=\"true\" name=\"title\"/><meta content=\"Visual Odometry vs. Visual SLAM vs. Structure-from-Motion\" data-rh=\"true\" property=\"og:title\"/><meta content=\"medium://p/cdda75df592\" data-rh=\"true\" property=\"al:android:url\"/><meta content=\"medium://p/cdda75df592\" data-rh=\"true\" property=\"al:ios:url\"/><meta content=\"Medium\" data-rh=\"true\" property=\"al:android:app_name\"/><meta content=\"The main goal of SLAM (Simultanous Localization and Mapping) is to obtain a global, consistent estimate of the robot path. The map of the environment is usually kept just for helping localization…\" data-rh=\"true\" name=\"description\"/><meta content=\"The main goal of SLAM (Simultanous Localization and Mapping) is to obtain a global, consistent estimate of the robot path. The map of the…\" data-rh=\"true\" property=\"og:description\"/><meta content=\"https://guvencetinkaya.medium.com/visual-odometry-vs-visual-slam-cdda75df592\" data-rh=\"true\" property=\"og:url\"/><meta content=\"https://guvencetinkaya.medium.com/visual-odometry-vs-visual-slam-cdda75df592\" data-rh=\"true\" property=\"al:web:url\"/><meta content=\"https://miro.medium.com/max/464/1*O9_muE5oiSyDsOVfgcuGwQ.jpeg\" data-rh=\"true\" property=\"og:image\"/><meta content=\"https://guvencetinkaya.medium.com\" data-rh=\"true\" property=\"article:author\"/><meta content=\"GUVEN CETINKAYA\" data-rh=\"true\" name=\"author\"/><meta content=\"index,follow,max-image-preview:large\" data-rh=\"true\" name=\"robots\"/><meta content=\"unsafe-url\" data-rh=\"true\" name=\"referrer\"/><meta content=\"Visual Odometry vs. Visual SLAM vs. Structure-from-Motion\" data-rh=\"true\" property=\"twitter:title\"/><meta content=\"@Medium\" data-rh=\"true\" name=\"twitter:site\"/><meta content=\"medium://p/cdda75df592\" data-rh=\"true\" name=\"twitter:app:url:iphone\"/><meta content=\"The main goal of SLAM (Simultanous Localization and Mapping) is to obtain a global, consistent estimate of the robot path. The map of the…\" data-rh=\"true\" property=\"twitter:description\"/><meta content=\"https://miro.medium.com/max/464/1*O9_muE5oiSyDsOVfgcuGwQ.jpeg\" data-rh=\"true\" name=\"twitter:image:src\"/><meta content=\"summary_large_image\" data-rh=\"true\" name=\"twitter:card\"/><meta content=\"@g_cetin\" data-rh=\"true\" name=\"twitter:creator\"/><meta content=\"Reading time\" data-rh=\"true\" name=\"twitter:label1\"/><meta content=\"3 min read\" data-rh=\"true\" name=\"twitter:data1\"/><meta content=\"2\" data-rh=\"true\" name=\"twitter:tile:template:testing\"/><meta content=\"https://miro.medium.com/max/464/1*O9_muE5oiSyDsOVfgcuGwQ.jpeg\" data-rh=\"true\" name=\"twitter:tile:image\"/><meta content=\"GUVEN CETINKAYA\" data-rh=\"true\" name=\"twitter:tile:info1:text\"/><meta content=\"User\" data-rh=\"true\" name=\"twitter:tile:info1:icon\"/><meta content=\"2022-06-03T07:30:54.240Z\" data-rh=\"true\" name=\"twitter:tile:info2:text\"/><meta content=\"Calendar\" data-rh=\"true\" name=\"twitter:tile:info2:icon\"/><meta content=\"Read on Medium\" data-rh=\"true\" name=\"twitter:cta\"/><link data-rh=\"true\" href=\"https://miro.medium.com/1*m-R_BkNf1Qjr1YbyOIJY2w.png\" rel=\"icon\"/><link data-rh=\"true\" href=\"/osd.xml\" rel=\"search\" title=\"Medium\" type=\"application/opensearchdescription+xml\"/><link data-rh=\"true\" href=\"https://miro.medium.com/fit/c/152/152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"152x152\"/><link data-rh=\"true\" href=\"https://miro.medium.com/fit/c/120/120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"120x120\"/><link data-rh=\"true\" href=\"https://miro.medium.com/fit/c/76/76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"76x76\"/><link data-rh=\"true\" href=\"https://miro.medium.com/fit/c/60/60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"60x60\"/><link color=\"#171717\" data-rh=\"true\" href=\"https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg\" rel=\"mask-icon\"/><link crossorigin=\"\" data-rh=\"true\" href=\"https://glyph.medium.com\" rel=\"preconnect\"/><link as=\"style\" data-rh=\"true\" href=\"https://glyph.medium.com/css/unbound.css\" id=\"glyph_preload_link\" rel=\"preload\" type=\"text/css\"/><link data-rh=\"true\" href=\"https://glyph.medium.com/css/unbound.css\" id=\"glyph_link\" rel=\"stylesheet\" type=\"text/css\"/><link data-rh=\"true\" href=\"https://guvencetinkaya.medium.com\" rel=\"author\"/><link data-rh=\"true\" href=\"https://guvencetinkaya.medium.com/visual-odometry-vs-visual-slam-cdda75df592\" rel=\"canonical\"/><link data-rh=\"true\" href=\"android-app://com.medium.reader/https/medium.com/p/cdda75df592\" rel=\"alternate\"/><script data-rh=\"true\" type=\"application/ld+json\">{\"@context\":\"http:\\u002F\\u002Fschema.org\",\"@type\":\"NewsArticle\",\"image\":[\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F1200\\u002F1*O9_muE5oiSyDsOVfgcuGwQ.jpeg\"],\"url\":\"https:\\u002F\\u002Fguvencetinkaya.medium.com\\u002Fvisual-odometry-vs-visual-slam-cdda75df592\",\"dateCreated\":\"2021-10-22T13:15:35.721Z\",\"datePublished\":\"2021-10-22T13:15:35.721Z\",\"dateModified\":\"2022-06-03T07:31:06.337Z\",\"headline\":\"Visual Odometry vs. Visual SLAM vs. Structure-from-Motion\",\"name\":\"Visual Odometry vs. Visual SLAM vs. Structure-from-Motion\",\"description\":\"The main goal of SLAM (Simultanous Localization and Mapping) is to obtain a global, consistent estimate of the robot path. The map of the environment is usually kept just for helping localization…\",\"identifier\":\"cdda75df592\",\"author\":{\"@type\":\"Person\",\"name\":\"GUVEN CETINKAYA\",\"url\":\"https:\\u002F\\u002Fguvencetinkaya.medium.com\"},\"creator\":[\"GUVEN CETINKAYA\"],\"publisher\":{\"@type\":\"Organization\",\"name\":\"Medium\",\"url\":\"https:\\u002F\\u002Fguvencetinkaya.medium.com\\u002F\",\"logo\":{\"@type\":\"ImageObject\",\"width\":308,\"height\":60,\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F616\\u002F1*OMF3fSqH8t4xBJ9-6oZDZw.png\"}},\"mainEntityOfPage\":\"https:\\u002F\\u002Fguvencetinkaya.medium.com\\u002Fvisual-odometry-vs-visual-slam-cdda75df592\"}</script><style data-fela-rehydration=\"522\" data-fela-type=\"STATIC\" type=\"text/css\">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden=\"true\"]{visibility:hidden;pointer-events:none}</style><style data-fela-rehydration=\"522\" data-fela-type=\"KEYFRAME\" type=\"text/css\">@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" type=\"text/css\">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, Cantarell, \"Open Sans\", \"Helvetica Neue\", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{margin:auto}.n{max-width:1504px}.o{display:flex}.u{justify-content:space-between}.ag{height:100%}.al{padding:0 24px}.am{box-shadow:0px -2px 10px rgba(0, 0, 0, 0.15)}.an{height:56px}.ao{align-items:center}.ap{position:fixed}.aq{top:0}.ar{right:0}.as{left:0}.at{z-index:500}.au{color:inherit}.av{fill:inherit}.aw{font-size:inherit}.ax{border:inherit}.ay{font-family:inherit}.az{letter-spacing:inherit}.ba{font-weight:inherit}.bb{padding:0}.bc{margin:0}.bg:disabled{cursor:default}.bh:disabled{color:rgba(117, 117, 117, 1)}.bi:disabled{fill:rgba(117, 117, 117, 1)}.bj{height:25px}.bk{fill:rgba(41, 41, 41, 1)}.bl{text-align:center}.bm{font-family:sohne, \"Helvetica Neue\", Helvetica, Arial, sans-serif}.bn{font-size:14px}.bo{line-height:20px}.bp{color:rgba(242, 242, 242, 1)}.bq{padding:7px 16px 9px}.br{fill:rgba(242, 242, 242, 1)}.bs{background:rgba(242, 242, 242, 1)}.bt{border-color:rgba(242, 242, 242, 1)}.bz:disabled{cursor:inherit !important}.ca:disabled{opacity:0.1}.cb:disabled:hover{background:rgba(25, 25, 25, 1)}.cc:disabled:hover{border-color:rgba(25, 25, 25, 1)}.cd{border-radius:99em}.ce{width:100%}.cf{border-width:1px}.cg{border-style:solid}.ch{box-sizing:border-box}.ci{display:inline-block}.cj{text-decoration:none}.ck{margin-left:16px}.cl{display:none}.cn{color:rgba(117, 117, 117, 1)}.co{color:rgba(26, 137, 23, 1)}.cp{fill:rgba(26, 137, 23, 1)}.cs:disabled{color:rgba(163, 208, 162, 0.5)}.ct:disabled{fill:rgba(163, 208, 162, 0.5)}.cz{height:100vh}.da{flex-direction:column}.db{position:sticky}.dc{height:23px}.dd{padding-bottom:35px}.de{fill:rgba(117, 117, 117, 1)}.df{padding-left:28px}.dg{transition:all 0.2s ease-in-out}.dk{margin-right:28px}.dm{font-size:16px}.dn{line-height:24px}.do{position:relative}.dp{margin:0px 0px 35px 28px }.dq{width:24px}.dr{border:0}.ds{height:1px}.dt{background-color:rgba(230, 230, 230, 1)}.du{padding:0 24px 24px}.dv{height:64px}.dw path{fill:rgba(168, 168, 168, 1)}.dx{justify-content:center}.dy{flex:1}.dz{border:none}.ea{background:transparent}.eb{box-shadow:0px 2px 10px rgba(0, 0, 0, 0.15)}.ec{z-index:600}.ed{bottom:0}.ee{justify-content:space-around}.ef{height:16px}.eg{background-color:rgba(237, 237, 237, 1)}.em{min-width:0}.en{flex:1 1 auto}.eo{padding:0 32px}.ep{border-left:1px solid rgba(230, 230, 230, 1)}.eq{min-height:100vh}.er{width:394px}.fm{margin-left:auto}.fn{margin-right:auto}.fo{max-width:728px}.fz{align-items:flex-start}.ga{margin-right:16px}.gb{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.gc{border-radius:50%}.gd{height:48px}.ge{width:48px}.gf{position:absolute}.gg{background-color:rgba(242, 242, 242, 1)}.gh{color:rgba(41, 41, 41, 1)}.gi{margin-bottom:4px}.gj{flex-direction:row}.gk{padding-left:12px}.gp{font-size:13px}.gq{color:rgba(255, 255, 255, 1)}.gr{padding:0px 8px 1px}.gs{fill:rgba(255, 255, 255, 1)}.gt{background:rgba(26, 137, 23, 1)}.gu{border-color:rgba(26, 137, 23, 1)}.gx:disabled{opacity:0.3}.gy:disabled:hover{background:rgba(26, 137, 23, 1)}.gz:disabled:hover{border-color:rgba(26, 137, 23, 1)}.ha{flex-wrap:wrap}.hb{padding:0 8px}.hf{padding-right:4px}.hg{flex:0 0 auto}.hh{padding:8px 2px}.hj{margin:0 4px 0 28px}.hk{display:inline-flex}.hl{padding-top:24px}.ho{padding-right:12px}.hp{background:rgba(255, 255, 255, 1)}.hq{border:1px solid rgba(230, 230, 230, 1)}.hr{border-radius:4px}.hs{box-shadow:0 1px 4px rgba(230, 230, 230, 1)}.ht{max-height:100vh}.hu{overflow-y:auto}.hv{top:calc(100vh + 100px)}.hw{bottom:calc(100vh + 100px)}.hx{width:10px}.hy{pointer-events:none}.hz{word-break:break-word}.ia{word-wrap:break-word}.ib:after{display:block}.ic:after{content:\"\"}.id:after{clear:both}.ie{line-height:1.23}.if{letter-spacing:0}.ig{font-style:normal}.ih{font-weight:700}.jc{margin-bottom:-0.27em}.jd{line-height:1.58}.je{letter-spacing:-0.004em}.jf{font-family:source-serif-pro, Georgia, Cambria, \"Times New Roman\", Times, serif}.ka{margin-bottom:-0.46em}.kb{font-style:italic}.kc{max-width:464px}.kh{clear:both}.ki{max-width:100%}.kj{height:auto}.kk{margin-top:10px}.kn{max-width:1458px}.kp{cursor:zoom-in}.kq{z-index:auto}.ks{max-width:1190px}.kt{line-height:1.31}.ku{letter-spacing:-0.022em}.kv{font-weight:600}.lq{margin-bottom:-0.37em}.lw{padding:16px 0 0}.lx{border-top:none}.ly{height:52px}.lz{max-height:52px}.ma{box-sizing:content-box}.mb{position:static}.mc{z-index:1}.md{flex:1 0 auto}.mf{max-width:155px}.mi{margin-right:5px}.ml{-webkit-user-select:none}.mm{cursor:progress}.mp{opacity:0.25}.mq{outline:0}.mr{user-select:none}.ms> svg{pointer-events:none}.nd{margin-left:24px}.ne{margin-top:0px}.nf{cursor:pointer}.ng{padding:4px 0}.nj path{fill:rgba(117, 117, 117, 1)}.nl{margin:0 20px}.nm{background-color:rgba(250, 250, 250, 1)}.nn{padding-bottom:4px}.no{padding-top:32px}.np{font-weight:500}.oc{overflow:hidden}.od{text-overflow:ellipsis}.oe{display:-webkit-box}.of{-webkit-line-clamp:1}.og{-webkit-box-orient:vertical}.oh{word-break:break-all}.oj{margin-left:8px}.ok{stroke:rgba(242, 242, 242, 1)}.ol{height:36px}.om{width:36px}.on{padding-top:5px}.oo{padding-top:25px}.op{padding-bottom:96px}.oq{padding-left:16px}.or{padding-top:40px}.os{padding-right:16px}.pi{background:rgba(25, 25, 25, 1)}.pj{border-color:rgba(25, 25, 25, 1)}.pm{padding-bottom:80px}.pn{padding-bottom:26px}.qx{flex-grow:0}.qy{padding-bottom:8px}.qz{margin-bottom:24px}.ra{margin-right:24px}.rb{flex:1 0 0%}.rc{margin-bottom:8px}.rd{margin-right:8px}.re{height:20px}.rf{width:20px}.rg{max-height:20px}.rh{max-height:60px}.ri{-webkit-line-clamp:3}.rj{width:56px}.rk{padding-bottom:100%}.rl{height:0}.rm{border-radius:2px}.rn{padding:30px 0}.ro{margin-bottom:0}.rp{min-width:100vw}.rq{background-color:rgba(0, 0, 0, 1)}.rv{max-width:1192px}.ry:disabled{color:rgba(255, 255, 255, 0.6)}.rz:disabled{fill:rgba(255, 255, 255, 0.45)}.sa{height:22px}.sb{margin-top:20px}.sc{color:rgba(255, 255, 255, 0.95)}.se{margin-right:20px}.sf{background-color:rgba(255, 255, 255, 0.4)}.sg{margin:28px 0 20px}.sh{padding-bottom:0px}.si{border-bottom:none}.sj{margin-top:40px}.sk{border-radius:20px}.sl{width:inherit}.sm{outline:none}.sn{padding:10px 20px 10px 0}.so{background-color:transparent}.sp::placeholder{color:rgba(117, 117, 117, 1)}.sq{padding:7px 7px 6px 8px}.sr{height:88px}.ss{width:88px}.st{margin-top:16px}.su{margin-top:4px}.sv{margin-top:12px}.sw{margin-top:24px}.sx{margin-bottom:40px}.sy{width:auto}.sz{padding:24px 0}.ta{margin-right:6px}.tb{font-size:11px}.tc{line-height:16px}.bd:hover{cursor:pointer}.be:hover{color:rgba(25, 25, 25, 1)}.bf:hover{fill:rgba(25, 25, 25, 1)}.bu:hover{background:rgba(242, 242, 242, 1)}.bv:hover{border-color:rgba(242, 242, 242, 1)}.bw:hover{cursor:wait}.bx:hover{color:rgba(242, 242, 242, 1)}.by:hover{fill:rgba(242, 242, 242, 1)}.cq:hover{color:rgba(15, 115, 12, 1)}.cr:hover{fill:rgba(15, 115, 12, 1)}.dh:hover{color:rgba(41, 41, 41, 1)}.di:hover{fill:rgba(41, 41, 41, 1)}.gv:hover{background:rgba(15, 115, 12, 1)}.gw:hover{border-color:rgba(15, 115, 12, 1)}.hi:hover path{fill:rgba(8, 8, 8, 1)}.mo:hover{fill:rgba(117, 117, 117, 1)}.nh:hover{fill:rgba(8, 8, 8, 1)}.ni:hover p{color:rgba(8, 8, 8, 1)}.pk:hover{background:rgba(8, 8, 8, 1)}.pl:hover{border-color:rgba(41, 41, 41, 1)}.rw:hover{color:rgba(255, 255, 255, 1)}.rx:hover{fill:rgba(255, 255, 255, 0.9)}.sd:hover{text-decoration:underline}.kr:focus{transform:scale(1.01)}.mn:focus{fill:rgba(117, 117, 117, 1)}.nk:focus path{fill:rgba(8, 8, 8, 1)}.mt:active{border-style:none}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"all and (min-width: 1080px)\" type=\"text/css\">.d{display:none}.t{flex-direction:row}.z{width:80px}.ab{min-height:100vh}.ac{flex-shrink:1}.ae{border-right:1px solid rgba(230, 230, 230, 1)}.cu{display:block}.cv{text-align:center}.cw{padding:40px 0}.el{margin-bottom:0}.ex{margin-bottom:40px}.fe{margin:0 32px}.ff{max-width:692px}.fl{padding:0 16px}.fx{margin-bottom:32px}.fy{margin-top:56px}.he{display:inline-flex}.iy{font-size:32px}.iz{margin-top:0.6em}.ja{line-height:40px}.jb{letter-spacing:-0.016em}.jw{font-size:20px}.jx{margin-top:2em}.jy{line-height:32px}.jz{letter-spacing:-0.003em}.lm{font-size:22px}.ln{margin-top:3.14em}.lo{line-height:28px}.lp{letter-spacing:0}.lv{margin-top:0.86em}.na{margin-top:0px}.nc{display:inline-block}.oa{line-height:24px}.ob{max-height:24px}.ox{margin:0}.pc{white-space:normal}.ph{margin:0 0 0 12px}.qa{width:calc(100% + 64px)}.qb{margin-left:-32px}.qc{margin-right:-32px}.qt{padding-left:32px}.qu{padding-right:32px}.qv{flex-basis:50%}.qw{max-width:50%}.ru{margin:0 64px}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"all and (max-width: 1079.98px)\" type=\"text/css\">.e{display:none}.kl{margin-left:auto}.km{text-align:center}.mz{margin-top:0px}.nb{display:inline-block}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"all and (max-width: 903.98px)\" type=\"text/css\">.f{display:none}.mh{display:inline-block}.my{margin-top:0px}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"all and (max-width: 727.98px)\" type=\"text/css\">.g{display:none}.cm{display:block}.mg{display:inline-block}.mw{margin-top:0px}.mx{margin-right:0px}.rr{padding:24px 0}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"all and (max-width: 551.98px)\" type=\"text/css\">.h{display:none}.p{flex-direction:column}.v{width:auto}.ah{display:block}.eh{margin-bottom:56px}.et{margin-bottom:80px}.ey{margin:0 24px}.fh{padding:0 8px}.fp{margin-bottom:24px}.fq{margin-top:32px}.gl{display:inline-block}.hm{display:flex}.ii{font-size:32px}.ij{margin-top:0.64em}.ik{line-height:40px}.il{letter-spacing:-0.016em}.jg{font-size:18px}.jh{margin-top:1.56em}.ji{line-height:28px}.jj{letter-spacing:-0.003em}.kd{margin-top:40px}.kw{font-size:20px}.kx{margin-top:1.9em}.ky{line-height:24px}.kz{letter-spacing:0}.lr{margin-top:0.67em}.mj{margin-left:0px}.mu{margin-top:0px}.mv{margin-right:0px}.nq{font-size:16px}.nr{line-height:20px}.ns{max-height:20px}.ot{margin:0}.oy{white-space:pre-line}.pd{margin:10px 0 0 0}.po{width:calc(100% + 24px)}.pp{margin-left:-12px}.pq{margin-right:-12px}.qd{padding-left:12px}.qe{padding-right:12px}.qf{flex-basis:100%}.qg{max-width:100%}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"all and (min-width: 904px) and (max-width: 1079.98px)\" type=\"text/css\">.i{display:none}.s{flex-direction:column}.y{width:auto}.ak{display:block}.ek{margin-bottom:56px}.ew{margin-bottom:40px}.fc{margin:0 32px}.fd{max-width:692px}.fk{padding:0 16px}.fv{margin-bottom:24px}.fw{margin-top:32px}.go{display:inline-block}.hd{display:inline-flex}.iu{font-size:32px}.iv{margin-top:0.6em}.iw{line-height:40px}.ix{letter-spacing:-0.016em}.js{font-size:20px}.jt{margin-top:2em}.ju{line-height:32px}.jv{letter-spacing:-0.003em}.kg{margin-top:56px}.li{font-size:22px}.lj{margin-top:3.14em}.lk{line-height:28px}.ll{letter-spacing:0}.lu{margin-top:0.86em}.ny{line-height:24px}.nz{max-height:24px}.ow{margin:0}.pb{white-space:normal}.pg{margin:0 0 0 12px}.px{width:calc(100% + 64px)}.py{margin-left:-32px}.pz{margin-right:-32px}.qp{padding-left:32px}.qq{padding-right:32px}.qr{flex-basis:50%}.qs{max-width:50%}.rt{margin:0 64px}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"all and (min-width: 728px) and (max-width: 903.98px)\" type=\"text/css\">.j{display:none}.r{flex-direction:column}.x{width:auto}.aj{display:block}.ej{margin-bottom:56px}.ev{margin-bottom:40px}.fa{margin:0 32px}.fb{max-width:692px}.fj{padding:0 16px}.ft{margin-bottom:24px}.fu{margin-top:32px}.gn{display:inline-block}.hc{display:inline-flex}.iq{font-size:32px}.ir{margin-top:0.6em}.is{line-height:40px}.it{letter-spacing:-0.016em}.jo{font-size:20px}.jp{margin-top:2em}.jq{line-height:32px}.jr{letter-spacing:-0.003em}.kf{margin-top:56px}.le{font-size:22px}.lf{margin-top:3.14em}.lg{line-height:28px}.lh{letter-spacing:0}.lt{margin-top:0.86em}.nw{line-height:24px}.nx{max-height:24px}.ov{margin:0}.pa{white-space:normal}.pf{margin:0 0 0 12px}.pu{width:calc(100% + 64px)}.pv{margin-left:-32px}.pw{margin-right:-32px}.ql{padding-left:32px}.qm{padding-right:32px}.qn{flex-basis:50%}.qo{max-width:50%}.rs{margin:0 48px}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"all and (min-width: 552px) and (max-width: 727.98px)\" type=\"text/css\">.k{display:none}.q{flex-direction:column}.w{width:auto}.ai{display:block}.ei{margin-bottom:56px}.eu{margin-bottom:80px}.ez{margin:0 24px}.fi{padding:0 8px}.fr{margin-bottom:24px}.fs{margin-top:32px}.gm{display:inline-block}.hn{display:flex}.im{font-size:32px}.in{margin-top:0.64em}.io{line-height:40px}.ip{letter-spacing:-0.016em}.jk{font-size:18px}.jl{margin-top:1.56em}.jm{line-height:28px}.jn{letter-spacing:-0.003em}.ke{margin-top:40px}.la{font-size:20px}.lb{margin-top:1.9em}.lc{line-height:24px}.ld{letter-spacing:0}.ls{margin-top:0.67em}.mk{margin-left:0px}.nt{font-size:16px}.nu{line-height:20px}.nv{max-height:20px}.ou{margin:0}.oz{white-space:pre-line}.pe{margin:10px 0 0 0}.pr{width:calc(100% + 64px)}.ps{margin-left:-32px}.pt{margin-right:-32px}.qh{padding-left:32px}.qi{padding-right:32px}.qj{flex-basis:50%}.qk{max-width:50%}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"print\" type=\"text/css\">.me{display:none}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"all and (min-width: 7000px)\" type=\"text/css\">.af{width:224px}.cx{text-align:left}.cy{padding:40px 24px}.dj{display:none}.dl{display:block}.fg{margin:0 auto}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"all and (max-width: 1239.98px)\" type=\"text/css\">.es{width:280px}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"(prefers-reduced-motion: no-preference)\" type=\"text/css\">.ko{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style><style data-fela-rehydration=\"522\" data-fela-type=\"RULE\" media=\"(orientation: landscape) and (max-width: 903.98px)\" type=\"text/css\">.oi{max-height:none}</style></head><body><div id=\"root\"><div class=\"a b c\"><div class=\"d e f g h i j k\"></div><script>document.domain = document.domain;</script><div class=\"l c\"><div class=\"m n l\"><div class=\"o p q r s t u\"><div class=\"v w x y z ab ac ae af\"><nav class=\"ag\"><div class=\"ah ai aj ak d\"><div class=\"al am an o ao u ap aq ar as at c\"><a aria-label=\"Homepage\" class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/?source=---three_column_layout_nav----------------------------------\" rel=\"noopener follow\"><svg class=\"bj bk\" viewbox=\"0 0 1043.63 592.71\"><g data-name=\"Layer 2\"><g data-name=\"Layer 1\"><path d=\"M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94\"></path></g></g></svg></a><div class=\"o ao\"><div class=\"l bl\"><div><a class=\"bm b bn bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd ce cf cg ch ci cj\" href=\"https://medium.com/plans?source=upgrade_membership---three_column_layout_nav----------------------------------\" rel=\"noopener follow\">Get unlimited access</a></div></div><div class=\"ck cl cm\"><span class=\"bm b bn bo cn\"><a class=\"co cp aw ax ay az ba bb bc bd cq cr bg cs ct\" href=\"https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcdda75df592&amp;%7Efeature=LoOpenInAppButton&amp;%7Echannel=ShowPostUnderUser&amp;%7Estage=mobileNavBar&amp;source=---three_column_layout_nav----------------------------------\" rel=\"noopener follow\">Open in app</a></span></div></div></div><div class=\"an l\"></div></div><div class=\"ag h k j i cu\"><div class=\"cz o da u db aq at c\"><div class=\"cv cw cx cy\"><a aria-label=\"Homepage\" href=\"https://medium.com/?source=---three_column_layout_nav----------------------------------\" rel=\"noopener follow\"><svg class=\"dc bk\" viewbox=\"0 0 1043.63 592.71\"><g data-name=\"Layer 2\"><g data-name=\"Layer 1\"><path d=\"M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94\"></path></g></g></svg></a></div><div class=\"l\"><div class=\"dd l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/?source=---three_column_layout_nav----------------------------------\" rel=\"noopener follow\"><div class=\"o do\"><div class=\"cn de o df dg dh di\"><div class=\"l dj dk\"><div><div aria-hidden=\"false\" class=\"ci\"><svg aria-label=\"Home\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M4.5 10.75v10.5c0 .14.11.25.25.25h5c.14 0 .25-.11.25-.25v-5.5c0-.14.11-.25.25-.25h3.5c.14 0 .25.11.25.25v5.5c0 .14.11.25.25.25h5c.14 0 .25-.11.25-.25v-10.5M22 9l-9.1-6.83a1.5 1.5 0 0 0-1.8 0L2 9\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path></svg></div></div></div><div aria-hidden=\"true\" class=\"cl dl dk\"><svg aria-label=\"Home\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M4.5 10.75v10.5c0 .14.11.25.25.25h5c.14 0 .25-.11.25-.25v-5.5c0-.14.11-.25.25-.25h3.5c.14 0 .25.11.25.25v5.5c0 .14.11.25.25.25h5c.14 0 .25-.11.25-.25v-10.5M22 9l-9.1-6.83a1.5 1.5 0 0 0-1.8 0L2 9\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path></svg></div><div class=\"cl dl bm b dm dn\">Home</div></div></div></a></div><span><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fme%2Fnotifications&amp;source=---three_column_layout_nav-----------------------notifications_sidenav-----------\" rel=\"noopener follow\"><div class=\"dd l\"><div class=\"o do\"><div class=\"cn de o df dg dh di\"><div class=\"l dj dk\"><div><div aria-hidden=\"false\" class=\"ci\"><svg aria-label=\"Notifications\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M15 18.5a3 3 0 1 1-6 0\" stroke=\"currentColor\" stroke-linecap=\"round\"></path><path d=\"M5.5 10.53V9a6.5 6.5 0 0 1 13 0v1.53c0 1.42.56 2.78 1.57 3.79l.03.03c.26.26.4.6.4.97v2.93c0 .14-.11.25-.25.25H3.75a.25.25 0 0 1-.25-.25v-2.93c0-.37.14-.71.4-.97l.03-.03c1-1 1.57-2.37 1.57-3.79z\" stroke=\"currentColor\" stroke-linejoin=\"round\"></path></svg></div></div></div><div aria-hidden=\"true\" class=\"cl dl dk\"><svg aria-label=\"Notifications\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M15 18.5a3 3 0 1 1-6 0\" stroke=\"currentColor\" stroke-linecap=\"round\"></path><path d=\"M5.5 10.53V9a6.5 6.5 0 0 1 13 0v1.53c0 1.42.56 2.78 1.57 3.79l.03.03c.26.26.4.6.4.97v2.93c0 .14-.11.25-.25.25H3.75a.25.25 0 0 1-.25-.25v-2.93c0-.37.14-.71.4-.97l.03-.03c1-1 1.57-2.37 1.57-3.79z\" stroke=\"currentColor\" stroke-linejoin=\"round\"></path></svg></div><div class=\"cl dl bm b dm dn\">Notifications</div></div></div></div></a></span><span><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fme%2Flists&amp;source=---three_column_layout_nav-----------------------lists_sidenav-----------\" rel=\"noopener follow\"><div class=\"dd l\"><div class=\"o do\"><div class=\"cn de o df dg dh di\"><div class=\"l dj dk\"><div><div aria-hidden=\"false\" class=\"ci\"><svg aria-label=\"Lists\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M4.5 6.25V21c0 .2.24.32.4.2l5.45-4.09a.25.25 0 0 1 .3 0l5.45 4.09c.16.12.4 0 .4-.2V6.25a.25.25 0 0 0-.25-.25H4.75a.25.25 0 0 0-.25.25z\" stroke=\"currentColor\" stroke-linecap=\"round\"></path><path d=\"M8 6V3.25c0-.14.11-.25.25-.25h11.5c.14 0 .25.11.25.25V16.5\" stroke=\"currentColor\" stroke-linecap=\"round\"></path></svg></div></div></div><div aria-hidden=\"true\" class=\"cl dl dk\"><svg aria-label=\"Lists\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M4.5 6.25V21c0 .2.24.32.4.2l5.45-4.09a.25.25 0 0 1 .3 0l5.45 4.09c.16.12.4 0 .4-.2V6.25a.25.25 0 0 0-.25-.25H4.75a.25.25 0 0 0-.25.25z\" stroke=\"currentColor\" stroke-linecap=\"round\"></path><path d=\"M8 6V3.25c0-.14.11-.25.25-.25h11.5c.14 0 .25.11.25.25V16.5\" stroke=\"currentColor\" stroke-linecap=\"round\"></path></svg></div><div class=\"cl dl bm b dm dn\">Lists</div></div></div></div></a></span><span><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fme%2Fstories%2Fdrafts&amp;source=---three_column_layout_nav-----------------------stories_sidenav-----------\" rel=\"noopener follow\"><div class=\"dd l\"><div class=\"o do\"><div class=\"cn de o df dg dh di\"><div class=\"l dj dk\"><div><div aria-hidden=\"false\" class=\"ci\"><svg aria-label=\"Stories\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M4.75 21.5h14.5c.14 0 .25-.11.25-.25V2.75a.25.25 0 0 0-.25-.25H4.75a.25.25 0 0 0-.25.25v18.5c0 .14.11.25.25.25z\" stroke=\"currentColor\"></path><path d=\"M8 8.5h8M8 15.5h5M8 12h8\" stroke=\"currentColor\" stroke-linecap=\"round\"></path></svg></div></div></div><div aria-hidden=\"true\" class=\"cl dl dk\"><svg aria-label=\"Stories\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M4.75 21.5h14.5c.14 0 .25-.11.25-.25V2.75a.25.25 0 0 0-.25-.25H4.75a.25.25 0 0 0-.25.25v18.5c0 .14.11.25.25.25z\" stroke=\"currentColor\"></path><path d=\"M8 8.5h8M8 15.5h5M8 12h8\" stroke=\"currentColor\" stroke-linecap=\"round\"></path></svg></div><div class=\"cl dl bm b dm dn\">Stories</div></div></div></div></a></span><div class=\"dp dq l\"><hr aria-hidden=\"true\" class=\"dr ds dt bc\"/></div><span><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fnew-story&amp;source=---three_column_layout_nav-----------------------new_post_sidenav-----------\" rel=\"noopener follow\"><div class=\"dd l\"><div class=\"o do\"><div class=\"cn de o df dg dh di\"><div class=\"l dj dk\"><div><div aria-hidden=\"false\" class=\"ci\"><svg aria-label=\"Write\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M14 4a.5.5 0 0 0 0-1v1zm7 6a.5.5 0 0 0-1 0h1zm-7-7H4v1h10V3zM3 4v16h1V4H3zm1 17h16v-1H4v1zm17-1V10h-1v10h1zm-1 1a1 1 0 0 0 1-1h-1v1zM3 20a1 1 0 0 0 1 1v-1H3zM4 3a1 1 0 0 0-1 1h1V3z\" fill=\"currentColor\"></path><path d=\"M17.5 4.5l-8.46 8.46a.25.25 0 0 0-.06.1l-.82 2.47c-.07.2.12.38.31.31l2.47-.82a.25.25 0 0 0 .1-.06L19.5 6.5m-2-2l2.32-2.32c.1-.1.26-.1.36 0l1.64 1.64c.1.1.1.26 0 .36L19.5 6.5m-2-2l2 2\" stroke=\"currentColor\"></path></svg></div></div></div><div aria-hidden=\"true\" class=\"cl dl dk\"><svg aria-label=\"Write\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M14 4a.5.5 0 0 0 0-1v1zm7 6a.5.5 0 0 0-1 0h1zm-7-7H4v1h10V3zM3 4v16h1V4H3zm1 17h16v-1H4v1zm17-1V10h-1v10h1zm-1 1a1 1 0 0 0 1-1h-1v1zM3 20a1 1 0 0 0 1 1v-1H3zM4 3a1 1 0 0 0-1 1h1V3z\" fill=\"currentColor\"></path><path d=\"M17.5 4.5l-8.46 8.46a.25.25 0 0 0-.06.1l-.82 2.47c-.07.2.12.38.31.31l2.47-.82a.25.25 0 0 0 .1-.06L19.5 6.5m-2-2l2.32-2.32c.1-.1.26-.1.36 0l1.64 1.64c.1.1.1.26 0 .36L19.5 6.5m-2-2l2 2\" stroke=\"currentColor\"></path></svg></div><div class=\"cl dl bm b dm dn\">Write</div></div></div></div></a></span></div><div class=\"du dv o ao\"></div></div></div><div class=\"ah ai aj ak d\"><div class=\"l ap ar ed as at c\"><div class=\"eb an do ec\"><div class=\"ag o ao ee\"><div class=\"ef dq l eg\"></div><div class=\"ef dq l eg\"></div><div class=\"ef dq l eg\"></div></div></div></div></div></nav></div><main class=\"eh ei ej ek el em l en\"><div class=\"l\"><div class=\"et eu ev ew ex l\"><div class=\"o dx\"><div class=\"em ce ey ez fa fb fc fd fe ff fg\"><article><div class=\"l\"><div class=\"fh fi fj fk fl fm fn ce fo ch l\"></div><div class=\"l\"><header class=\"pw-post-byline-header fp fq fr fs ft fu fv fw fx fy l\"><div class=\"o fz u\"><div class=\"o\"><div class=\"ga l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"/?source=post_page-----cdda75df592--------------------------------\" rel=\"noopener follow\"><div class=\"l do\"><img alt=\"GUVEN CETINKAYA\" class=\"l ch gc gd ge gg\" height=\"48\" loading=\"lazy\" src=\"https://miro.medium.com/fit/c/96/96/1*xPqPM3D2mX-B8q8EvI9g8g@2x.jpeg\" width=\"48\"/><div class=\"gb gc l gd ge gf aq\"></div></div></a></div><div class=\"l\"><div class=\"pw-author bm b dm dn gh\"><div class=\"gi o gj\"><div><div aria-hidden=\"false\" class=\"ci\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"/?source=post_page-----cdda75df592--------------------------------\" rel=\"noopener follow\">GUVEN CETINKAYA</a></div></div><div class=\"gk gl gm gn go d\"><span><button class=\"bm b gp bo gq gr gs gt gu gv gw bd bz gx gy gz cd cf cg ch ci cj\">Follow</button></span></div></div></div><div class=\"o ao ha\"><p class=\"pw-published-date bm b bn bo cn\"><span>Oct 22, 2021</span></p><div aria-hidden=\"true\" class=\"hb ci\"><span aria-hidden=\"true\" class=\"l\"><span class=\"bm b bn bo cn\">·</span></span></div><div class=\"pw-reading-time bm b bn bo cn\">3 min read</div></div></div></div><div class=\"o ao\"><div class=\"h k hc hd he\"><div class=\"hf l hg\"><div><div aria-hidden=\"false\" class=\"ci\"><button aria-label=\"Share on twitter\" class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\"><span class=\"ci hh dw hi\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33\" fill=\"#A8A8A8\"></path></svg></span></button></div></div></div><div class=\"hf l hg\"><div><div aria-hidden=\"false\" class=\"ci\"><button aria-label=\"Share on facebook\" class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\"><span class=\"ci hh dw hi\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71\" fill=\"#A8A8A8\"></path></svg></span></button></div></div></div><div class=\"hf l hg\"><div><div aria-hidden=\"false\" class=\"ci\"><button aria-label=\"Share on linkedin\" class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\"><span class=\"ci hh dw hi\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z\" fill=\"#A8A8A8\"></path></svg></span></button></div></div></div><div class=\"l hg\"><div><div aria-hidden=\"false\" class=\"ci\"><button class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\"><span class=\"ci hh dw hi\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z\" fill=\"#A8A8A8\" fill-rule=\"evenodd\"></path></svg></span></button></div></div></div><div class=\"hj o ao\"></div></div><div class=\"ck hk\"><div><div aria-hidden=\"false\" class=\"ci\"></div></div></div></div></div><div class=\"hl hm hn j i d\"><div class=\"ga l\"></div><div class=\"ho l hg\"><div><div aria-hidden=\"false\" class=\"ci\"><button aria-label=\"Share on twitter\" class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\"><span class=\"ci hh dw hi\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33\" fill=\"#A8A8A8\"></path></svg></span></button></div></div></div><div class=\"ho l hg\"><div><div aria-hidden=\"false\" class=\"ci\"><button aria-label=\"Share on facebook\" class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\"><span class=\"ci hh dw hi\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71\" fill=\"#A8A8A8\"></path></svg></span></button></div></div></div><div class=\"ho l hg\"><div><div aria-hidden=\"false\" class=\"ci\"><button aria-label=\"Share on linkedin\" class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\"><span class=\"ci hh dw hi\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z\" fill=\"#A8A8A8\"></path></svg></span></button></div></div></div><div class=\"l hg\"><div><div aria-hidden=\"false\" class=\"ci\"><button class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\"><span class=\"ci hh dw hi\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z\" fill=\"#A8A8A8\" fill-rule=\"evenodd\"></path></svg></span></button></div></div></div></div></header><span class=\"l\"></span><section><div><div class=\"gf as hv hw hx hy\"></div><div class=\"hz ia ib ic id\"><div class=\"\"><h1 class=\"pw-post-title ie if ig bm ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc gh\" id=\"b86b\">Visual Odometry vs. Visual SLAM vs. Structure-from-Motion</h1></div><p class=\"pw-post-body-paragraph jd je ig jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hz gh\" id=\"660e\">The main goal of SLAM (<strong class=\"jf ih\">S</strong>imultanous <strong class=\"jf ih\">L</strong>ocalization <strong class=\"jf ih\">a</strong>nd <strong class=\"jf ih\">M</strong>apping) is to obtain a global, consistent estimate of the robot path. The map of the environment is usually kept just for helping localization. Map information is utilized in Visual Odometry and Loop Clouse blocks. When a loop closure is detected , this information is used to reduce the drift in both the map and pose. Loop detection and loop closure are two main issues in SLAM besides localization[1].</p><p class=\"pw-post-body-paragraph jd je ig jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hz gh\" id=\"6a16\">Visual Odometry aims at recovering the path incrementally, pose after pose, and potentially optimizing only over the last <em class=\"kb\">n</em> poses of the path (windowed bundle adjustment). In VO, local consistency of the trajectory is the main concern and local map is used to obtain a more accurate estimate of the local trajectory. But SLAM is concerned with the global map consistency [1].</p><p class=\"pw-post-body-paragraph jd je ig jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hz gh\" id=\"9b47\">When we look at Figure-1, the map built by VO resides in leftside and map built by SLAM resides in right side. In SLAM, at point “<em class=\"kb\">B</em>”, by the help of loop detection and loop closure, the robot understands that it has passed over that place before and it really understands the real topology of the environment. In VO, the robot feels like moving in an infinite corrider and keeps exploring new areas indefinitely [2].</p><figure class=\"kd ke kf kg fy kh fm fn paragraph-image\"><div class=\"fm fn kc\"><picture><source data-testid=\"og\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 464px\" srcset=\"https://miro.medium.com/max/640/1*O9_muE5oiSyDsOVfgcuGwQ.jpeg 640w, https://miro.medium.com/max/720/1*O9_muE5oiSyDsOVfgcuGwQ.jpeg 720w, https://miro.medium.com/max/750/1*O9_muE5oiSyDsOVfgcuGwQ.jpeg 750w, https://miro.medium.com/max/786/1*O9_muE5oiSyDsOVfgcuGwQ.jpeg 786w, https://miro.medium.com/max/828/1*O9_muE5oiSyDsOVfgcuGwQ.jpeg 828w, https://miro.medium.com/max/1100/1*O9_muE5oiSyDsOVfgcuGwQ.jpeg 1100w, https://miro.medium.com/max/928/1*O9_muE5oiSyDsOVfgcuGwQ.jpeg 928w\"/><img alt=\"\" class=\"ce ki kj c\" height=\"226\" loading=\"eager\" role=\"presentation\" width=\"464\"/></picture></div><figcaption class=\"kk bl fo fm fn kl km bm b bn bo cn\">Figure-1: Odometry vs. SLAM [2]</figcaption></figure><p class=\"pw-post-body-paragraph jd je ig jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hz gh\" id=\"c19a\">Visual Odometry is one of the main building blocks of Visual SLAM. The other main blocks are Loop-Closure, Backend Optimization and Reconstruction (see Figure-2). Reconstruction block may seem unnecessary in classical SLAM approaches, but some applications need a dense representation of the environment and this is provided by the Reconstruction block. The dense map can be utilized for navigation, obstacle avoidance and interaction purposes in robotic applications [3].</p><figure class=\"kd ke kf kg fy kh fm fn paragraph-image\"><div class=\"ko kp do kq ce kr\" role=\"button\" tabindex=\"0\"><div class=\"fm fn kn\"><picture><source data-testid=\"og\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/max/640/1*I0mPuSVuQaWa66l5viHB2A.png 640w, https://miro.medium.com/max/720/1*I0mPuSVuQaWa66l5viHB2A.png 720w, https://miro.medium.com/max/750/1*I0mPuSVuQaWa66l5viHB2A.png 750w, https://miro.medium.com/max/786/1*I0mPuSVuQaWa66l5viHB2A.png 786w, https://miro.medium.com/max/828/1*I0mPuSVuQaWa66l5viHB2A.png 828w, https://miro.medium.com/max/1100/1*I0mPuSVuQaWa66l5viHB2A.png 1100w, https://miro.medium.com/max/1400/1*I0mPuSVuQaWa66l5viHB2A.png 1400w\"/><img alt=\"\" class=\"ce ki kj c\" height=\"311\" loading=\"lazy\" role=\"presentation\" width=\"700\"/></picture></div></div><figcaption class=\"kk bl fo fm fn kl km bm b bn bo cn\">Figure-2: Building blocks of Visual SLAM</figcaption></figure><p class=\"pw-post-body-paragraph jd je ig jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hz gh\" id=\"68fb\">Structure from Motion (SfM) is a more general concept compared to Visual SLAM but there are many commonalities as well. SfM is usually performed offline using unordered sequences of images. SfM is mostly concerned with creating a map of the environment using several images taken from different perspectives. In SfM, images can even be taken from different cameras. Visual SLAM is about solving the localization problem while constructing the map of the environment. The image sequences must be ordered in Visual SLAM and usually they are taken from the same camera. The relation between SfM, Visual SLAM and Visual Odometry is summarized in Figure-3.</p><figure class=\"kd ke kf kg fy kh fm fn paragraph-image\"><div class=\"ko kp do kq ce kr\" role=\"button\" tabindex=\"0\"><div class=\"fm fn ks\"><picture><source data-testid=\"og\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/max/640/1*wzDT0gqiuoaKqmDloO-cNA.png 640w, https://miro.medium.com/max/720/1*wzDT0gqiuoaKqmDloO-cNA.png 720w, https://miro.medium.com/max/750/1*wzDT0gqiuoaKqmDloO-cNA.png 750w, https://miro.medium.com/max/786/1*wzDT0gqiuoaKqmDloO-cNA.png 786w, https://miro.medium.com/max/828/1*wzDT0gqiuoaKqmDloO-cNA.png 828w, https://miro.medium.com/max/1100/1*wzDT0gqiuoaKqmDloO-cNA.png 1100w, https://miro.medium.com/max/1400/1*wzDT0gqiuoaKqmDloO-cNA.png 1400w\"/><img alt=\"\" class=\"ce ki kj c\" height=\"252\" loading=\"lazy\" role=\"presentation\" width=\"700\"/></picture></div></div><figcaption class=\"kk bl fo fm fn kl km bm b bn bo cn\">Figure-3: SfM vs. V-SLAM vs. VO</figcaption></figure><h1 class=\"kt ku ig bm kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq gh\" id=\"34bb\">References</h1><p class=\"pw-post-body-paragraph jd je ig jf b jg lr ji jj jk ls jm jn jo lt jq jr js lu ju jv jw lv jy jz ka hz gh\" id=\"8329\">[1] D. Scaramuzza and F. Fraundorfer, “Visual Odometry Part1: The First 30 Years and Fundamentals”</p><p class=\"pw-post-body-paragraph jd je ig jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hz gh\" id=\"ab1b\">[2] C. Cadena and L. Carlone and H. Carrillo and Y. Latif and D. Scaramuzza and J. Neira and I. Reid and J.J. Leonard, “Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age”, in IEEE Transactions on Robotics 32 (6) pp 1309–1332, 2016</p><p class=\"pw-post-body-paragraph jd je ig jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hz gh\" id=\"48ea\">[3] Xiang Gao, Tao Zhang, Yi Liu, and Qinrui Yan.14 Lectureson Visual SLAM: From Theory to Practice. Publishing Houseof Electronics Industry, 2017.</p></div></div></section></div></div></article><div class=\"lw o\"></div></div></div><div class=\"l\"></div><footer class=\"lx ly lz ma o ao mb mc c\"><div class=\"l md\"><div class=\"o dx\"><div class=\"em ce ey ez fa fb fc fd fe ff fg\"><div class=\"o u me\"><div class=\"o ao gj\"><div class=\"mf l\"><span class=\"l gl mg mh e d\"><div class=\"o ao gj\"><div class=\"pw-multi-vote-icon do mi mj mk ml\"><div class=\"\"><div class=\"dr mm de mn mo mp mq bb mr ms mt ml\"><svg aria-label=\"clap\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z\" fill-rule=\"evenodd\"></path><path clip-rule=\"evenodd\" d=\"M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z\" fill-rule=\"evenodd\"></path></svg></div></div></div><div class=\"pw-multi-vote-count l mu mv mw mx my mz na\"><p class=\"bm b gp bo cn\"><span class=\"mm\">--</span></p></div></div></span><span class=\"l h g f nb nc\"><div class=\"o ao gj\"><div class=\"pw-multi-vote-icon do mi mj mk ml\"><div class=\"\"><div class=\"dr mm de mn mo mp mq bb mr ms mt ml\"><svg aria-label=\"clap\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z\" fill-rule=\"evenodd\"></path><path clip-rule=\"evenodd\" d=\"M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z\" fill-rule=\"evenodd\"></path></svg></div></div></div><div class=\"pw-multi-vote-count l mu mv mw mx my mz na\"><p class=\"bm b gp bo cn\"><span class=\"mm\">--</span></p></div></div></span></div><div class=\"nd o\"><div><div aria-hidden=\"false\" class=\"ci\"><button aria-label=\"responses\" class=\"nf dr ng o ao de nh ni\"><svg aria-label=\"responses\" class=\"ne\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg></button></div></div></div></div><div class=\"o ao\"><div aria-describedby=\"postFooterSocialMenu\" aria-hidden=\"false\" aria-labelledby=\"postFooterSocialMenu\" class=\"ci\"><div><div aria-hidden=\"false\" class=\"ci\"><button aria-controls=\"postFooterSocialMenu\" aria-expanded=\"false\" aria-label=\"Share Post\" class=\"au av aw ax ay az ba hh bc bd be bf bg bh bi nj hi nk\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z\" fill=\"#000\" fill-rule=\"evenodd\"></path></svg></button></div></div></div><div class=\"nl l hg\"></div></div></div></div></div></div></footer></div><div class=\"o dx\"><div class=\"em ce ey ez fa fb fc fd fe ff fg\"></div></div><div class=\"l\"><div class=\"l nm me\"><div class=\"l me\"><div class=\"nn no l nm\"><div class=\"o dx\"><div class=\"em ce ey ez fa fb fc fd fe ff fg\"><div class=\"o ao u\"><h2 class=\"bm np nq nr ns kz nt nu nv ld jo nw nx lh js ny nz ll jw oa ob lp oc od oe of og oh oi gh\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"/?source=post_page-----cdda75df592--------------------------------\" rel=\"noopener follow\">More from GUVEN CETINKAYA</a></h2><div class=\"oj o\"><span><button class=\"bm b bn bo gq bq gs gt gu gv gw bd bz gx gy gz cd cf cg ch ci cj\">Follow</button></span><div class=\"oj l\"><div><div><div aria-hidden=\"false\" class=\"ci\"><div class=\"l\"><span><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F653f9727ece4&amp;operation=register&amp;redirect=https%3A%2F%2Fguvencetinkaya.medium.com%2Fvisual-odometry-vs-visual-slam-cdda75df592&amp;newsletterV3=9d17c73e6cfe&amp;newsletterV3Id=653f9727ece4&amp;user=GUVEN+CETINKAYA&amp;userId=9d17c73e6cfe&amp;source=-----cdda75df592---------------------subscribe_user-----------\" rel=\"noopener follow\"><button aria-label=\"Subscribe\" class=\"bm b bn bo bp bb br bs bt bu bv bw bx by bz gx gy gz cd cf cg ch ci cj\"><svg class=\"ok ol om\" fill=\"none\" height=\"38\" viewbox=\"0 0 38 38\" width=\"38\"><rect height=\"6.5\" rx=\"0.25\" width=\"0.5\" x=\"26.25\" y=\"9.25\"></rect><rect height=\"6.5\" rx=\"0.25\" transform=\"rotate(90 29.75 12.25)\" width=\"0.5\" x=\"29.75\" y=\"12.25\"></rect><path d=\"M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5\"></path><path d=\"M11.5 14.5L19 20l4-3\"></path></svg></button></a></span></div></div></div></div></div></div></div><div class=\"on l\"><p class=\"bm b bn bo cn\">Software Design Engineer</p></div></div></div></div></div><div class=\"oo l\"><div class=\"op oq or os l\"><div class=\"hr l bl\"><div class=\"o dx\"><div class=\"ot ou ov ow ox ki em ce\"><div class=\"ah oy ai oz gn pa go pb nc pc\"><p class=\"bm b dm dn gh\">Love podcasts or audiobooks? Learn on the go with our new app.</p></div><div class=\"pd ah pe ai pf gn pg go ph nc\"><a class=\"bm b bn bo gq bq gs pi pj pk pl bd bz ca cb cc cd cf cg ch ci cj\" href=\"https://knowable.fyi/?utm_source=medium&amp;utm_medium=referral&amp;utm_campaign=medium-post-footer&amp;source=post_page-----cdda75df592--------------------------------\" rel=\"noopener follow\">Try Knowable</a></div></div></div></div></div></div><div class=\"o dx\"><div class=\"em ce ey ez fa fb fc fd fe ff fg\"><div class=\"pm or l\"><section class=\"pw-more-medium-articles l\"><div class=\"pn l\"><h2 class=\"bm np nq nr ns kz nt nu nv ld jo nw nx lh js ny nz ll jw oa ob lp oc od oe of og oh oi gh\">Recommended from Medium</h2></div><div class=\"fz o gj ha po pp pq pr ps pt pu pv pw px py pz qa qb qc\"><div class=\"qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs qt qu qv qw qx\"><div class=\"ce ag\"><div class=\"qy l\"><div class=\"qz o da dx\"><div class=\"o gj u\"><div class=\"ra o da rb\"><div class=\"rc o ao\"><div class=\"rd l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/@bengansky?source=post_internal_links---------0----------------------------\" rel=\"noopener follow\"><div class=\"l do\"><img alt=\"Ben Gansky\" class=\"l ch gc re rf gg\" height=\"20\" loading=\"lazy\" src=\"https://miro.medium.com/fit/c/40/40/1*v7rHKdcUpzTecNjQ3PfNFg.png\" width=\"20\"/><div class=\"gb gc l re rf gf aq\"></div></div></a></div><div class=\"hf l\"><div><div aria-hidden=\"false\" class=\"ci\"><div class=\"o\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/@bengansky?source=post_internal_links---------0----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">Ben Gansky</p></a></div></div></div></div></div><a href=\"https://medium.com/@bengansky/misunderstanding-ai-we-need-better-conversations-5c13ff766f79?source=post_internal_links---------0----------------------------\" rel=\"noopener follow\"><h2 class=\"bm ih dm bo oc rh od oe ri og oi if gh\"><div>Towards a Better AI Conversation: Why It's Urgent to Distinguish Between ANI &amp; AGI</div></h2></a></div><a href=\"https://medium.com/@bengansky/misunderstanding-ai-we-need-better-conversations-5c13ff766f79?source=post_internal_links---------0----------------------------\" rel=\"noopener follow\"><div class=\"rj l\"><div class=\"m oc l do gg\"><div class=\"rk rl l\"><img alt=\"\" class=\"rm\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/focal/112/112/50/50/1*XZn7KJaBpdBojLOfkw42HA.png\" width=\"56\"/></div></div></div></a></div></div></div></div></div><div class=\"qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs qt qu qv qw qx\"><div class=\"ce ag\"><div class=\"qy l\"><div class=\"qz o da dx\"><div class=\"o gj u\"><div class=\"ra o da rb\"><div class=\"rc o ao\"><div class=\"rd l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/@xavier.basset?source=post_internal_links---------1----------------------------\" rel=\"noopener follow\"><div class=\"l do\"><img alt=\"Xavier Basset\" class=\"l ch gc re rf gg\" height=\"20\" loading=\"lazy\" src=\"https://miro.medium.com/fit/c/40/40/1*i5Zq44YzeuwuASuoez8P7A.jpeg\" width=\"20\"/><div class=\"gb gc l re rf gf aq\"></div></div></a></div><div class=\"hf l\"><div><div aria-hidden=\"false\" class=\"ci\"><div class=\"o\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/@xavier.basset?source=post_internal_links---------1----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">Xavier Basset</p></a></div></div></div></div><div class=\"hf l\"><p class=\"bm b gp bo cn\">in</p></div><div class=\"l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://blog.hoomano.com/?source=post_internal_links---------1----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">Hoomano</p></a></div></div><a href=\"https://medium.com/@xavier.basset/dall-e-pushed-our-creativity-like-nothing-else-before-9f85a597845d?source=post_internal_links---------1----------------------------\" rel=\"noopener follow\"><h2 class=\"bm ih dm bo oc rh od oe ri og oi if gh\"><div>DALL•E pushed our creativity - like nothing else before</div></h2></a></div><a href=\"https://medium.com/@xavier.basset/dall-e-pushed-our-creativity-like-nothing-else-before-9f85a597845d?source=post_internal_links---------1----------------------------\" rel=\"noopener follow\"><div class=\"rj l\"><div class=\"m oc l do gg\"><div class=\"rk rl l\"><img alt=\"\" class=\"rm\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/focal/112/112/50/50/1*KxN_K2E_8B3JiYr6k22P4A.jpeg\" width=\"56\"/></div></div></div></a></div></div></div></div></div><div class=\"qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs qt qu qv qw qx\"><div class=\"ce ag\"><div class=\"qy l\"><div class=\"qz o da dx\"><div class=\"o gj u\"><div class=\"ra o da rb\"><div class=\"rc o ao\"><div class=\"rd l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://nisargpandya09.medium.com/?source=post_internal_links---------2----------------------------\" rel=\"noopener follow\"><div class=\"l do\"><img alt=\"@nisargpandya\" class=\"l ch gc re rf gg\" height=\"20\" loading=\"lazy\" src=\"https://miro.medium.com/fit/c/40/40/1*Iy4G4IZgZ-PeMEfltSICrQ.jpeg\" width=\"20\"/><div class=\"gb gc l re rf gf aq\"></div></div></a></div><div class=\"hf l\"><div><div aria-hidden=\"false\" class=\"ci\"><div class=\"o\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://nisargpandya09.medium.com/?source=post_internal_links---------2----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">@nisargpandya</p></a></div></div></div></div><div class=\"hf l\"><p class=\"bm b gp bo cn\">in</p></div><div class=\"l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/drivebuddyai?source=post_internal_links---------2----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">drivebuddyAI</p></a></div></div><a href=\"https://nisargpandya09.medium.com/driver-behavior-and-profiling-the-driver-centric-approach-4ec7cd71ccfd?source=post_internal_links---------2----------------------------\" rel=\"noopener follow\"><h2 class=\"bm ih dm bo oc rh od oe ri og oi if gh\"><div>Driver Behavior and Profiling — The Driver Centric Approach</div></h2></a></div><a href=\"https://nisargpandya09.medium.com/driver-behavior-and-profiling-the-driver-centric-approach-4ec7cd71ccfd?source=post_internal_links---------2----------------------------\" rel=\"noopener follow\"><div class=\"rj l\"><div class=\"m oc l do gg\"><div class=\"rk rl l\"><img alt=\"\" class=\"rm\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/focal/112/112/50/50/1*9SPTcrbjhnJWk-6kOoLHWQ.png\" width=\"56\"/></div></div></div></a></div></div></div></div></div><div class=\"qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs qt qu qv qw qx\"><div class=\"ce ag\"><div class=\"qy l\"><div class=\"qz o da dx\"><div class=\"o gj u\"><div class=\"ra o da rb\"><div class=\"rc o ao\"><div class=\"rd l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://conradthegray.medium.com/?source=post_internal_links---------3----------------------------\" rel=\"noopener follow\"><div class=\"l do\"><img alt=\"Conrad Gray\" class=\"l ch gc re rf gg\" height=\"20\" loading=\"lazy\" src=\"https://miro.medium.com/fit/c/40/40/1*zQTC-GB1jFGDy3FdvERmNw.jpeg\" width=\"20\"/><div class=\"gb gc l re rf gf aq\"></div></div></a></div><div class=\"hf l\"><div><div aria-hidden=\"false\" class=\"ci\"><div class=\"o\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://conradthegray.medium.com/?source=post_internal_links---------3----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">Conrad Gray</p></a></div></div></div></div><div class=\"hf l\"><p class=\"bm b gp bo cn\">in</p></div><div class=\"l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/h-weekly?source=post_internal_links---------3----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">H+ Weekly</p></a></div></div><a href=\"https://conradthegray.medium.com/h-weekly-issue-288-259d4960b410?source=post_internal_links---------3----------------------------\" rel=\"noopener follow\"><h2 class=\"bm ih dm bo oc rh od oe ri og oi if gh\"><div>H+ Weekly — Issue #288</div></h2></a></div><a href=\"https://conradthegray.medium.com/h-weekly-issue-288-259d4960b410?source=post_internal_links---------3----------------------------\" rel=\"noopener follow\"><div class=\"rj l\"><div class=\"m oc l do gg\"><div class=\"rk rl l\"><img alt=\"\" class=\"rm\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/focal/112/112/50/50/1*UQp8OJUzEanmmpVbU6glrA.png\" width=\"56\"/></div></div></div></a></div></div></div></div></div><div class=\"qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs qt qu qv qw qx\"><div class=\"ce ag\"><div class=\"qy l\"><div class=\"qz o da dx\"><div class=\"o gj u\"><div class=\"ra o da rb\"><div class=\"rc o ao\"><div class=\"rd l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://ensarseker1.medium.com/?source=post_internal_links---------4----------------------------\" rel=\"noopener follow\"><div class=\"l do\"><img alt=\"Ensar Seker\" class=\"l ch gc re rf gg\" height=\"20\" loading=\"lazy\" src=\"https://miro.medium.com/fit/c/40/40/2*qDL_LqQFrjOz6LafhxnyRQ.jpeg\" width=\"20\"/><div class=\"gb gc l re rf gf aq\"></div></div></a></div><div class=\"hf l\"><div><div aria-hidden=\"false\" class=\"ci\"><div class=\"o\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://ensarseker1.medium.com/?source=post_internal_links---------4----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">Ensar Seker</p></a></div></div></div></div><div class=\"hf l\"><p class=\"bm b gp bo cn\">in</p></div><div class=\"l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/swlh?source=post_internal_links---------4----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">The Startup</p></a></div></div><a href=\"https://ensarseker1.medium.com/machine-learning-based-digital-fraud-detection-bad492232eb6?source=post_internal_links---------4----------------------------\" rel=\"noopener follow\"><h2 class=\"bm ih dm bo oc rh od oe ri og oi if gh\"><div>Machine Learning-based Digital Fraud Detection</div></h2></a></div><a href=\"https://ensarseker1.medium.com/machine-learning-based-digital-fraud-detection-bad492232eb6?source=post_internal_links---------4----------------------------\" rel=\"noopener follow\"><div class=\"rj l\"><div class=\"m oc l do gg\"><div class=\"rk rl l\"><img alt=\"\" class=\"rm\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/focal/112/112/50/50/0*72JIHUviJJ9bXwEU\" width=\"56\"/></div></div></div></a></div></div></div></div></div><div class=\"qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs qt qu qv qw qx\"><div class=\"ce ag\"><div class=\"qy l\"><div class=\"qz o da dx\"><div class=\"o gj u\"><div class=\"ra o da rb\"><div class=\"rc o ao\"><div class=\"rd l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://thatgirlenergi.medium.com/?source=post_internal_links---------5----------------------------\" rel=\"noopener follow\"><div class=\"l do\"><img alt=\"Ｅｎｅｒｇｉ\" class=\"l ch gc re rf gg\" height=\"20\" loading=\"lazy\" src=\"https://miro.medium.com/fit/c/40/40/1*aSoBSEIQQWJwr3oOpdBLdw.png\" width=\"20\"/><div class=\"gb gc l re rf gf aq\"></div></div></a></div><div class=\"hf l\"><div><div aria-hidden=\"false\" class=\"ci\"><div class=\"o\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://thatgirlenergi.medium.com/?source=post_internal_links---------5----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">Ｅｎｅｒｇｉ</p></a></div></div></div></div></div><a href=\"https://thatgirlenergi.medium.com/we-fear-ai-because-we-fear-ourselves-128268516eb0?source=post_internal_links---------5----------------------------\" rel=\"noopener follow\"><h2 class=\"bm ih dm bo oc rh od oe ri og oi if gh\"><div>We fear AI because we fear ourselves.</div></h2></a></div><a href=\"https://thatgirlenergi.medium.com/we-fear-ai-because-we-fear-ourselves-128268516eb0?source=post_internal_links---------5----------------------------\" rel=\"noopener follow\"><div class=\"rj l\"><div class=\"m oc l do gg\"><div class=\"rk rl l\"><img alt=\"\" class=\"rm\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/focal/112/112/50/50/1*QLhgYX3h0Q90YWkiKCliPw.jpeg\" width=\"56\"/></div></div></div></a></div></div></div></div></div><div class=\"qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs qt qu qv qw qx\"><div class=\"ce ag\"><div class=\"qy l\"><div class=\"qz o da dx\"><div class=\"o gj u\"><div class=\"ra o da rb\"><div class=\"rc o ao\"><div class=\"rd l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://ryoung19-86531.medium.com/?source=post_internal_links---------6----------------------------\" rel=\"noopener follow\"><div class=\"l do\"><img alt=\"Ryan Young\" class=\"l ch gc re rf gg\" height=\"20\" loading=\"lazy\" src=\"https://miro.medium.com/fit/c/40/40/0*xMwHLJfFmD2qGY-H.jpg\" width=\"20\"/><div class=\"gb gc l re rf gf aq\"></div></div></a></div><div class=\"hf l\"><div><div aria-hidden=\"false\" class=\"ci\"><div class=\"o\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://ryoung19-86531.medium.com/?source=post_internal_links---------6----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">Ryan Young</p></a></div></div></div></div></div><a href=\"https://ryoung19-86531.medium.com/our-modernized-world-c89c4fc7aad0?source=post_internal_links---------6----------------------------\" rel=\"noopener follow\"><h2 class=\"bm ih dm bo oc rh od oe ri og oi if gh\"><div>Our Modernized World</div></h2></a></div><a href=\"https://ryoung19-86531.medium.com/our-modernized-world-c89c4fc7aad0?source=post_internal_links---------6----------------------------\" rel=\"noopener follow\"><div class=\"rj l\"></div></a></div></div></div></div></div><div class=\"qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs qt qu qv qw qx\"><div class=\"ce ag\"><div class=\"qy l\"><div class=\"qz o da dx\"><div class=\"o gj u\"><div class=\"ra o da rb\"><div class=\"rc o ao\"><div class=\"rd l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/@sahika.betul?source=post_internal_links---------7----------------------------\" rel=\"noopener follow\"><div class=\"l do\"><img alt=\"Sahika Betul Yayli\" class=\"l ch gc re rf gg\" height=\"20\" loading=\"lazy\" src=\"https://miro.medium.com/fit/c/40/40/1*NRiSGM8ZQQKNMJZ9VZGCjg.png\" width=\"20\"/><div class=\"gb gc l re rf gf aq\"></div></div></a></div><div class=\"hf l\"><div><div aria-hidden=\"false\" class=\"ci\"><div class=\"o\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/@sahika.betul?source=post_internal_links---------7----------------------------\" rel=\"noopener follow\"><p class=\"bm b gp bo oc rg od oe of og oh oi gh\">Sahika Betul Yayli</p></a></div></div></div></div></div><a href=\"https://medium.com/@sahika.betul/is-speech-the-new-blood-recent-progress-in-ai-based-disease-detection-from-audio-in-a-nutshell-370e6a0c93e3?source=post_internal_links---------7----------------------------\" rel=\"noopener follow\"><h2 class=\"bm ih dm bo oc rh od oe ri og oi if gh\"><div>Is Speech the New Blood? Recent Progress in AI-Based Disease Detection From Audio in a Nutshell</div></h2></a></div><a href=\"https://medium.com/@sahika.betul/is-speech-the-new-blood-recent-progress-in-ai-based-disease-detection-from-audio-in-a-nutshell-370e6a0c93e3?source=post_internal_links---------7----------------------------\" rel=\"noopener follow\"><div class=\"rj l\"><div class=\"m oc l do gg\"><div class=\"rk rl l\"><img alt=\"\" class=\"rm\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/focal/112/112/50/50/1*ZN03ZQQnBpfKJW3l0ARDrA.png\" width=\"56\"/></div></div></div></a></div></div></div></div></div></div></section></div></div></div></div></div><div class=\"d\"><div class=\"rn ro rp l mb ar ed as rq rr\"><div class=\"o dx\"><div class=\"ey ez rs rt ru rv em ce\"><a aria-label=\"Go to homepage\" class=\"au av aw ax ay az ba bb bc bd rw rx bg ry rz\" href=\"https://medium.com/?source=post_page-----cdda75df592--------------------------------\" rel=\"noopener follow\"><svg class=\"gs sa\" viewbox=\"0 0 3940 610\"><path d=\"M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z\"></path></svg></a><div class=\"sb l\"><p class=\"bm b gp bo sc\"><a class=\"au av aw ax ay az ba bb bc bd sd bg ry rz se\" href=\"https://medium.com/about?autoplay=1&amp;source=post_page-----cdda75df592--------------------------------\" rel=\"noopener follow\">About</a><a class=\"au av aw ax ay az ba bb bc bd sd bg ry rz se\" href=\"https://help.medium.com/hc/en-us?source=post_page-----cdda75df592--------------------------------\" rel=\"noopener follow\">Help</a><a class=\"au av aw ax ay az ba bb bc bd sd bg ry rz se\" href=\"https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cdda75df592--------------------------------\" rel=\"noopener follow\">Terms</a><a class=\"au av aw ax ay az ba bb bc bd sd bg ry rz\" href=\"https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cdda75df592--------------------------------\" rel=\"noopener follow\">Privacy</a></p></div><div class=\"j i d\"><hr aria-hidden=\"true\" class=\"dr ds sf sg\"/><h2 class=\"bm np dm bo if sc\">Get the Medium app</h2><div class=\"sb o\"><div class=\"ga l\"><a class=\"au av aw ax ay az ba bb bc bd rw rx bg ry rz\" href=\"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----cdda75df592--------------------------------\" rel=\"noopener follow\"><img alt=\"A button that says 'Download on the App Store', and if clicked it will lead you to the iOS App store\" class=\"\" height=\"41\" loading=\"lazy\" src=\"https://miro.medium.com/max/270/1*Crl55Tm6yDNMoucPo1tvDg.png\" width=\"135\"/></a></div><a class=\"au av aw ax ay az ba bb bc bd rw rx bg ry rz\" href=\"https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----cdda75df592--------------------------------\" rel=\"noopener follow\"><img alt=\"A button that says 'Get it on, Google Play', and if clicked it will lead you to the Google Play store\" class=\"\" height=\"41\" loading=\"lazy\" src=\"https://miro.medium.com/max/270/1*W_RAPQ62h0em559zluJLdQ.png\" width=\"135\"/></a></div></div></div></div></div></div></div></main><div class=\"eo ch c ep h k j i cu eq er es\"><div class=\"ag ce ci do\"><div class=\"l db aq\"><div class=\"eq o da\"><div class=\"l md\"><div class=\"sh si sj l\"><div class=\"l\"><div class=\"o ao\"><div class=\"pw-susi-button l md\"><span><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fguvencetinkaya.medium.com%2Fvisual-odometry-vs-visual-slam-cdda75df592&amp;source=post_page---three_column_layout_sidebar-----------------------three_column_layout_nav-----------\" rel=\"noopener follow\"><button aria-label=\"sign up\" class=\"bm b bn bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd ce cf cg ch ci cj\">Get started</button></a></span></div></div></div></div><div class=\"sh si sj l\"><div class=\"o hq sk sl\"><div aria-describedby=\"searchResults\" aria-hidden=\"false\" aria-labelledby=\"searchResults\" class=\"ci\"></div><span class=\"sq l\"><svg fill=\"rgba(8, 8, 8, 1)\" height=\"25\" viewbox=\"0 0 25 25\" width=\"25\"><path d=\"M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z\"></path></svg></span><input aria-controls=\"searchResults\" aria-expanded=\"false\" aria-label=\"search\" class=\"dz sm bm bn bo ce sn so gh sp\" placeholder=\"Search\" role=\"combobox\" tabindex=\"0\" value=\"\"/></div></div><div class=\"sh si sj l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"/?source=---three_column_layout_sidebar----------------------------------\" rel=\"noopener follow\"><div class=\"l do\"><img alt=\"GUVEN CETINKAYA\" class=\"l ch gc sr ss gg\" height=\"88\" loading=\"lazy\" src=\"https://miro.medium.com/fit/c/176/176/1*xPqPM3D2mX-B8q8EvI9g8g@2x.jpeg\" width=\"88\"/><div class=\"gb gc l sr ss gf aq\"></div></div></a><div class=\"st l\"></div><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"/?source=---three_column_layout_sidebar----------------------------------\" rel=\"noopener follow\"><h2 class=\"pw-author-name bm np dm bo if gh\">GUVEN CETINKAYA</h2></a><div class=\"su l\"></div><span class=\"pw-follower-count bm b dm dn cn\"><button class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\">10 Followers</button></span><div class=\"sv l\"></div><p class=\"bm b bn bo cn\">Software Design Engineer</p><div class=\"sw l\"></div><div class=\"sx o\"><span><button class=\"bm b bn bo gq bq gs gt gu gv gw bd bz gx gy gz cd sy cf cg ch ci cj\">Follow</button></span><div class=\"oj l\"><div><div><div aria-hidden=\"false\" class=\"ci\"><div class=\"l\"><span><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F653f9727ece4&amp;operation=register&amp;redirect=https%3A%2F%2Fguvencetinkaya.medium.com%2Fvisual-odometry-vs-visual-slam-cdda75df592&amp;newsletterV3=9d17c73e6cfe&amp;newsletterV3Id=653f9727ece4&amp;user=GUVEN+CETINKAYA&amp;userId=9d17c73e6cfe&amp;source=---three_column_layout_sidebar-----------------------subscribe_user-----------\" rel=\"noopener follow\"><button aria-label=\"Subscribe\" class=\"bm b bn bo bp bb br bs bt bu bv bw bx by bz gx gy gz cd cf cg ch ci cj\"><svg class=\"ok ol om\" fill=\"none\" height=\"38\" viewbox=\"0 0 38 38\" width=\"38\"><rect height=\"6.5\" rx=\"0.25\" width=\"0.5\" x=\"26.25\" y=\"9.25\"></rect><rect height=\"6.5\" rx=\"0.25\" transform=\"rotate(90 29.75 12.25)\" width=\"0.5\" x=\"29.75\" y=\"12.25\"></rect><path d=\"M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5\"></path><path d=\"M11.5 14.5L19 20l4-3\"></path></svg></button></a></span></div></div></div></div></div></div></div><div class=\"sh si sj l\"></div></div><div class=\"sz o gj ha\"><div class=\"ta l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://help.medium.com/hc/en-us?source=---three_column_layout_sidebar----------------------------------\" rel=\"noopener follow\"><p class=\"bm b tb tc cn\">Help</p></a></div><div class=\"ta l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.statuspage.io/?source=---three_column_layout_sidebar----------------------------------\" rel=\"noopener follow\"><p class=\"bm b tb tc cn\">Status</p></a></div><div class=\"ta l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://about.medium.com/creators/?source=---three_column_layout_sidebar----------------------------------\" rel=\"noopener follow\"><p class=\"bm b tb tc cn\">Writers</p></a></div><div class=\"ta l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://blog.medium.com/?source=---three_column_layout_sidebar----------------------------------\" rel=\"noopener follow\"><p class=\"bm b tb tc cn\">Blog</p></a></div><div class=\"ta l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---three_column_layout_sidebar----------------------------------\" rel=\"noopener follow\"><p class=\"bm b tb tc cn\">Careers</p></a></div><div class=\"ta l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---three_column_layout_sidebar----------------------------------\" rel=\"noopener follow\"><p class=\"bm b tb tc cn\">Privacy</p></a></div><div class=\"ta l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---three_column_layout_sidebar----------------------------------\" rel=\"noopener follow\"><p class=\"bm b tb tc cn\">Terms</p></a></div><div class=\"ta l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://medium.com/about?autoplay=1&amp;source=---three_column_layout_sidebar----------------------------------\" rel=\"noopener follow\"><p class=\"bm b tb tc cn\">About</p></a></div><div class=\"ta l\"><a class=\"au av aw ax ay az ba bb bc bd be bf bg bh bi\" href=\"https://knowable.fyi/?source=---three_column_layout_sidebar----------------------------------\" rel=\"noopener follow\"><p class=\"bm b tb tc cn\">Knowable</p></a></div></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__=\"main-20221007-234535-14691d346c\"</script><script>window.__GRAPHQL_URI__ = \"https://guvencetinkaya.medium.com/_/graphql\"</script><script>window.__PRELOADED_STATE__ = {\"algolia\":{\"queries\":{}},\"auroraPage\":{\"isAuroraPageEnabled\":false},\"cache\":{\"experimentGroupSet\":true,\"reason\":\"\",\"group\":\"enabled\",\"tags\":[\"group-edgeCachePosts\",\"post-cdda75df592\",\"user-9d17c73e6cfe\"],\"serverVariantState\":\"44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\",\"middlewareEnabled\":true,\"cacheStatus\":\"DYNAMIC\",\"shouldUseCache\":true,\"vary\":[],\"inDisabledExperiment\":false,\"zenEnabled\":false},\"client\":{\"hydrated\":false,\"isUs\":false,\"isNativeMedium\":false,\"isSafariMobile\":false,\"isSafari\":false,\"isFirefox\":false,\"routingEntity\":{\"type\":\"USER\",\"id\":\"9d17c73e6cfe\",\"explicit\":true},\"viewerIsBot\":false},\"debug\":{\"requestId\":\"3238e87d-023c-4528-acea-a3c8f56d20ff\",\"hybridDevServices\":[],\"originalSpanCarrier\":{\"ot-tracer-spanid\":\"06d9472662c95f58\",\"ot-tracer-traceid\":\"322243d8f28f93f0\",\"ot-tracer-sampled\":\"true\"}},\"meter\":{},\"multiVote\":{\"clapsPerPost\":{}},\"navigation\":{\"branch\":{\"show\":null,\"hasRendered\":null,\"blockedByCTA\":false},\"hideGoogleOneTap\":false,\"hasRenderedAlternateUserBanner\":null,\"currentLocation\":\"https:\\u002F\\u002Fguvencetinkaya.medium.com\\u002Fvisual-odometry-vs-visual-slam-cdda75df592\",\"host\":\"guvencetinkaya.medium.com\",\"hostname\":\"guvencetinkaya.medium.com\",\"referrer\":\"\",\"hasSetReferrer\":false,\"susiModal\":{\"step\":null,\"operation\":\"register\"},\"postRead\":false,\"queryString\":\"\",\"currentHash\":\"\"},\"config\":{\"nodeEnv\":\"production\",\"version\":\"main-20221007-234535-14691d346c\",\"target\":\"production\",\"productName\":\"Medium\",\"publicUrl\":\"https:\\u002F\\u002Fcdn-client.medium.com\\u002Flite\",\"authDomain\":\"medium.com\",\"authGoogleClientId\":\"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com\",\"favicon\":\"production\",\"glyphUrl\":\"https:\\u002F\\u002Fglyph.medium.com\",\"branchKey\":\"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm\",\"algolia\":{\"appId\":\"MQ57UUUQZ2\",\"apiKeySearch\":\"394474ced050e3911ae2249ecc774921\",\"indexPrefix\":\"medium_\",\"host\":\"-dsn.algolia.net\"},\"recaptchaKey\":\"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk\",\"recaptcha3Key\":\"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5\",\"datadog\":{\"applicationId\":\"6702d87d-a7e0-42fe-bbcb-95b469547ea0\",\"clientToken\":\"pub853ea8d17ad6821d9f8f11861d23dfed\",\"rumToken\":\"pubf9cc52896502b9413b68ba36fc0c7162\",\"context\":{\"deployment\":{\"target\":\"production\",\"tag\":\"main-20221007-234535-14691d346c\",\"commit\":\"14691d346cddb7c050b0c6443f673965389dddd2\"}},\"datacenter\":\"us\"},\"googleAnalyticsCode\":\"UA-24232453-2\",\"googlePay\":{\"apiVersion\":\"2\",\"apiVersionMinor\":\"0\",\"merchantId\":\"BCR2DN6TV7EMTGBM\",\"merchantName\":\"Medium\",\"instanceMerchantId\":\"13685562959212738550\"},\"applePay\":{\"version\":3},\"signInWallCustomDomainCollectionIds\":[\"3a8144eabfe3\",\"336d898217ee\",\"61061eb0c96b\",\"138adf9c44c\",\"819cc2aaeee0\"],\"mediumOwnedAndOperatedCollectionIds\":[\"8a9336e5bb4\",\"b7e45b22fec3\",\"193b68bd4fba\",\"8d6b8a439e32\",\"54c98c43354d\",\"3f6ecf56618\",\"d944778ce714\",\"92d2092dc598\",\"ae2a65f35510\",\"1285ba81cada\",\"544c7006046e\",\"fc8964313712\",\"40187e704f1c\",\"88d9857e584e\",\"7b6769f2748b\",\"bcc38c8f6edf\",\"cef6983b292\",\"cb8577c9149e\",\"444d13b52878\",\"713d7dbc99b0\",\"ef8e90590e66\",\"191186aaafa0\",\"55760f21cdc5\",\"9dc80918cc93\",\"bdc4052bbdba\",\"8ccfed20cbb2\"],\"tierOneDomains\":[\"medium.com\",\"thebolditalic.com\",\"arcdigital.media\",\"towardsdatascience.com\",\"uxdesign.cc\",\"codeburst.io\",\"psiloveyou.xyz\",\"writingcooperative.com\",\"entrepreneurshandbook.co\",\"prototypr.io\",\"betterhumans.coach.me\",\"theascent.pub\"],\"topicsToFollow\":[\"d61cf867d93f\",\"8a146bc21b28\",\"1eca0103fff3\",\"4d562ee63426\",\"aef1078a3ef5\",\"e15e46793f8d\",\"6158eb913466\",\"55f1c20aba7a\",\"3d18b94f6858\",\"4861fee224fd\",\"63c6f1f93ee\",\"1d98b3a9a871\",\"decb52b64abf\",\"ae5d4995e225\",\"830cded25262\"],\"topicToTagMappings\":{\"accessibility\":\"accessibility\",\"addiction\":\"addiction\",\"android-development\":\"android-development\",\"art\":\"art\",\"artificial-intelligence\":\"artificial-intelligence\",\"astrology\":\"astrology\",\"basic-income\":\"basic-income\",\"beauty\":\"beauty\",\"biotech\":\"biotech\",\"blockchain\":\"blockchain\",\"books\":\"books\",\"business\":\"business\",\"cannabis\":\"cannabis\",\"cities\":\"cities\",\"climate-change\":\"climate-change\",\"comics\":\"comics\",\"coronavirus\":\"coronavirus\",\"creativity\":\"creativity\",\"cryptocurrency\":\"cryptocurrency\",\"culture\":\"culture\",\"cybersecurity\":\"cybersecurity\",\"data-science\":\"data-science\",\"design\":\"design\",\"digital-life\":\"digital-life\",\"disability\":\"disability\",\"economy\":\"economy\",\"education\":\"education\",\"equality\":\"equality\",\"family\":\"family\",\"feminism\":\"feminism\",\"fiction\":\"fiction\",\"film\":\"film\",\"fitness\":\"fitness\",\"food\":\"food\",\"freelancing\":\"freelancing\",\"future\":\"future\",\"gadgets\":\"gadgets\",\"gaming\":\"gaming\",\"gun-control\":\"gun-control\",\"health\":\"health\",\"history\":\"history\",\"humor\":\"humor\",\"immigration\":\"immigration\",\"ios-development\":\"ios-development\",\"javascript\":\"javascript\",\"justice\":\"justice\",\"language\":\"language\",\"leadership\":\"leadership\",\"lgbtqia\":\"lgbtqia\",\"lifestyle\":\"lifestyle\",\"machine-learning\":\"machine-learning\",\"makers\":\"makers\",\"marketing\":\"marketing\",\"math\":\"math\",\"media\":\"media\",\"mental-health\":\"mental-health\",\"mindfulness\":\"mindfulness\",\"money\":\"money\",\"music\":\"music\",\"neuroscience\":\"neuroscience\",\"nonfiction\":\"nonfiction\",\"outdoors\":\"outdoors\",\"parenting\":\"parenting\",\"pets\":\"pets\",\"philosophy\":\"philosophy\",\"photography\":\"photography\",\"podcasts\":\"podcast\",\"poetry\":\"poetry\",\"politics\":\"politics\",\"privacy\":\"privacy\",\"product-management\":\"product-management\",\"productivity\":\"productivity\",\"programming\":\"programming\",\"psychedelics\":\"psychedelics\",\"psychology\":\"psychology\",\"race\":\"race\",\"relationships\":\"relationships\",\"religion\":\"religion\",\"remote-work\":\"remote-work\",\"san-francisco\":\"san-francisco\",\"science\":\"science\",\"self\":\"self\",\"self-driving-cars\":\"self-driving-cars\",\"sexuality\":\"sexuality\",\"social-media\":\"social-media\",\"society\":\"society\",\"software-engineering\":\"software-engineering\",\"space\":\"space\",\"spirituality\":\"spirituality\",\"sports\":\"sports\",\"startups\":\"startup\",\"style\":\"style\",\"technology\":\"technology\",\"transportation\":\"transportation\",\"travel\":\"travel\",\"true-crime\":\"true-crime\",\"tv\":\"tv\",\"ux\":\"ux\",\"venture-capital\":\"venture-capital\",\"visual-design\":\"visual-design\",\"work\":\"work\",\"world\":\"world\",\"writing\":\"writing\"},\"defaultImages\":{\"avatar\":{\"imageId\":\"1*dmbNkD5D-u45r44go_cf0g.png\",\"height\":150,\"width\":150},\"orgLogo\":{\"imageId\":\"1*OMF3fSqH8t4xBJ9-6oZDZw.png\",\"height\":106,\"width\":545},\"postLogo\":{\"imageId\":\"1*kFrc4tBFM_tCis-2Ic87WA.png\",\"height\":810,\"width\":1440},\"postPreviewImage\":{\"imageId\":\"1*hn4v1tCaJy7cWMyb0bpNpQ.png\",\"height\":386,\"width\":579}},\"collectionStructuredData\":{\"8d6b8a439e32\":{\"name\":\"Elemental\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F980\\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png\",\"width\":980,\"height\":159}}},\"3f6ecf56618\":{\"name\":\"Forge\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F596\\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png\",\"width\":596,\"height\":183}}},\"ae2a65f35510\":{\"name\":\"GEN\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F264\\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png\",\"width\":264,\"height\":140}}},\"88d9857e584e\":{\"name\":\"LEVEL\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png\",\"width\":540,\"height\":108}}},\"7b6769f2748b\":{\"name\":\"Marker\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F383\\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png\",\"width\":383,\"height\":92}}},\"444d13b52878\":{\"name\":\"OneZero\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*cw32fIqCbRWzwJaoQw6BUg.png\",\"width\":540,\"height\":123}}},\"8ccfed20cbb2\":{\"name\":\"Zora\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png\",\"width\":540,\"height\":106}}}},\"embeddedPostIds\":{\"coronavirus\":\"cd3010f9d81f\"},\"sharedCdcMessaging\":{\"COVID_APPLICABLE_TAG_SLUGS\":[],\"COVID_APPLICABLE_TOPIC_NAMES\":[],\"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE\":[],\"COVID_MESSAGES\":{\"tierA\":{\"text\":\"For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":66,\"end\":73,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"tierB\":{\"text\":\"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.\",\"markups\":[{\"start\":37,\"end\":45,\"href\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Fcategories\\u002F201931128-Policies-Safety\"},{\"start\":125,\"end\":132,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"paywall\":{\"text\":\"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":56,\"end\":70,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fmembership\"},{\"start\":138,\"end\":145,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"unbound\":{\"text\":\"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":45,\"end\":59,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fmembership\"},{\"start\":127,\"end\":134,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]}},\"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST\":[\"3b31a67bff4a\"]},\"sharedVoteMessaging\":{\"TAGS\":[\"politics\",\"election-2020\",\"government\",\"us-politics\",\"election\",\"2020-presidential-race\",\"trump\",\"donald-trump\",\"democrats\",\"republicans\",\"congress\",\"republican-party\",\"democratic-party\",\"biden\",\"joe-biden\",\"maga\"],\"TOPICS\":[\"politics\",\"election\"],\"MESSAGE\":{\"text\":\"Find out more about the U.S. election results here.\",\"markups\":[{\"start\":46,\"end\":50,\"href\":\"https:\\u002F\\u002Fcookpolitical.com\\u002F2020-national-popular-vote-tracker\"}]},\"EXCLUDE_POSTS\":[\"397ef29e3ca5\"]},\"embedPostRules\":[],\"recircOptions\":{\"v1\":{\"limit\":3},\"v2\":{\"limit\":8}},\"braintreeClientKey\":\"production_zjkj96jm_m56f8fqpf7ngnrd4\",\"braintree\":{\"enabled\":true,\"merchantId\":\"m56f8fqpf7ngnrd4\",\"merchantAccountId\":{\"usd\":\"AMediumCorporation_instant\",\"eur\":\"amediumcorporation_EUR\",\"cad\":\"amediumcorporation_CAD\"},\"publicKey\":\"ds2nn34bg2z7j5gd\",\"braintreeEnvironment\":\"production\",\"dashboardUrl\":\"https:\\u002F\\u002Fwww.braintreegateway.com\\u002Fmerchants\",\"gracePeriodDurationInDays\":14,\"mediumMembershipPlanId\":{\"monthly\":\"ce105f8c57a3\",\"monthlyWithTrial\":\"d5ee3dbe3db8\",\"yearly\":\"a40ad4a43185\",\"yearlyStaff\":\"d74fb811198a\",\"yearlyWithTrial\":\"b3bc7350e5c7\",\"monthlyCad\":\"p52orjkaceei\",\"yearlyCad\":\"h4q9g2up9ktt\"},\"braintreeDiscountId\":{\"oneMonthFree\":\"MONTHS_FREE_01\",\"threeMonthsFree\":\"MONTHS_FREE_03\",\"sixMonthsFree\":\"MONTHS_FREE_06\"},\"3DSecureVersion\":\"2\",\"defaultCurrency\":\"usd\",\"providerPlanIdCurrency\":{\"4ycw\":\"usd\",\"rz3b\":\"usd\",\"3kqm\":\"usd\",\"jzw6\":\"usd\",\"c2q2\":\"usd\",\"nnsw\":\"usd\",\"fx7w\":\"cad\",\"nwf2\":\"cad\"}},\"paypalClientId\":\"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v\",\"paypal\":{\"host\":\"https:\\u002F\\u002Fapi.paypal.com:443\",\"clientMode\":\"production\",\"serverMode\":\"live\",\"webhookId\":\"4G466076A0294510S\",\"monthlyPlan\":{\"planId\":\"P-9WR0658853113943TMU5FDQA\",\"name\":\"Medium Membership (Monthly) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"yearlyPlan\":{\"planId\":\"P-7N8963881P8875835MU5JOPQ\",\"name\":\"Medium Membership (Annual) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"oneYearGift\":{\"name\":\"Medium Membership (1 Year, Digital Gift Code)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\\u002Fredeem.\",\"price\":\"50.00\",\"currency\":\"USD\",\"sku\":\"membership-gift-1-yr\"},\"oldMonthlyPlan\":{\"planId\":\"P-96U02458LM656772MJZUVH2Y\",\"name\":\"Medium Membership (Monthly)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"oldYearlyPlan\":{\"planId\":\"P-59P80963JF186412JJZU3SMI\",\"name\":\"Medium Membership (Annual)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"monthlyPlanWithTrial\":{\"planId\":\"P-66C21969LR178604GJPVKUKY\",\"name\":\"Medium Membership (Monthly) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"yearlyPlanWithTrial\":{\"planId\":\"P-6XW32684EX226940VKCT2MFA\",\"name\":\"Medium Membership (Annual) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"oldMonthlyPlanNoSetupFee\":{\"planId\":\"P-4N046520HR188054PCJC7LJI\",\"name\":\"Medium Membership (Monthly)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"oldYearlyPlanNoSetupFee\":{\"planId\":\"P-7A4913502Y5181304CJEJMXQ\",\"name\":\"Medium Membership (Annual)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"sdkUrl\":\"https:\\u002F\\u002Fwww.paypal.com\\u002Fsdk\\u002Fjs\"},\"stripePublishableKey\":\"pk_live_7FReX44VnNIInZwrIIx6ghjl\",\"log\":{\"json\":true,\"level\":\"info\"},\"webpRamp\":[]},\"session\":{\"xsrf\":\"\"}}</script><script>window.__APOLLO_STATE__ = {\"ROOT_QUERY\":{\"__typename\":\"Query\",\"user({\\\"id\\\":\\\"9d17c73e6cfe\\\"})\":{\"__ref\":\"User:9d17c73e6cfe\"},\"userResult({\\\"id\\\":\\\"9d17c73e6cfe\\\"})\":{\"__ref\":\"User:9d17c73e6cfe\"},\"collectionByDomainOrSlug({\\\"domainOrSlug\\\":\\\"guvencetinkaya.medium.com\\\"})\":null,\"postResult({\\\"id\\\":\\\"cdda75df592\\\"})\":{\"__ref\":\"Post:cdda75df592\"}},\"User:9d17c73e6cfe\":{\"__typename\":\"User\",\"id\":\"9d17c73e6cfe\",\"customStyleSheet\":null,\"name\":\"GUVEN CETINKAYA\",\"username\":\"guvencetinkaya\",\"newsletterV3\":{\"__ref\":\"NewsletterV3:653f9727ece4\"},\"imageId\":\"1*xPqPM3D2mX-B8q8EvI9g8g@2x.jpeg\",\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":10,\"followingCount\":16,\"collectionFollowingCount\":5},\"bio\":\"Software Design Engineer\",\"isPartnerProgramEnrolled\":false,\"viewerEdge\":{\"__ref\":\"UserViewerEdge:userId:9d17c73e6cfe-viewerId:lo_b385833e69ac\"},\"viewerIsUser\":false,\"postSubscribeMembershipUpsellShownAt\":0,\"customDomainState\":{\"__typename\":\"CustomDomainState\",\"live\":{\"__typename\":\"CustomDomain\",\"domain\":\"guvencetinkaya.medium.com\",\"status\":\"ACTIVE\",\"isSubdomain\":true}},\"hasSubdomain\":true,\"mediumMemberAt\":1584433675000,\"about\":\"\",\"homepagePostsConnection:{\\\"paging\\\":{\\\"limit\\\":1}}\":{\"__typename\":\"PostConnection\",\"posts\":[{\"__ref\":\"Post:cdda75df592\"}]},\"isSuspended\":false,\"allowNotes\":true,\"isAuroraVisible\":true,\"twitterScreenName\":\"g_cetin\",\"atsQualifiedAt\":1634736785121},\"UserViewerEdge:userId:9d17c73e6cfe-viewerId:lo_b385833e69ac\":{\"__typename\":\"UserViewerEdge\",\"id\":\"userId:9d17c73e6cfe-viewerId:lo_b385833e69ac\",\"isFollowing\":false,\"isUser\":false},\"NewsletterV3:653f9727ece4\":{\"__typename\":\"NewsletterV3\",\"id\":\"653f9727ece4\",\"type\":\"NEWSLETTER_TYPE_AUTHOR\",\"slug\":\"9d17c73e6cfe\",\"name\":\"9d17c73e6cfe\",\"collection\":null,\"user\":{\"__ref\":\"User:9d17c73e6cfe\"},\"description\":\"\",\"promoHeadline\":\"\",\"promoBody\":\"\",\"showPromo\":false,\"subscribersCount\":1},\"Post:cdda75df592\":{\"__typename\":\"Post\",\"id\":\"cdda75df592\",\"firstPublishedAt\":1634908535721,\"visibility\":\"PUBLIC\",\"creator\":{\"__ref\":\"User:9d17c73e6cfe\"},\"canonicalUrl\":\"\",\"collection\":null,\"content({\\\"postMeteringOptions\\\":{\\\"forceTruncation\\\":false}})\":{\"__typename\":\"PostContent\",\"isLockedPreviewOnly\":false,\"validatedShareKey\":\"\",\"bodyModel\":{\"__typename\":\"RichText\",\"sections\":[{\"__typename\":\"Section\",\"name\":\"fdcf\",\"startIndex\":0,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null}],\"paragraphs\":[{\"__ref\":\"Paragraph:7977e879a762_0\"},{\"__ref\":\"Paragraph:7977e879a762_1\"},{\"__ref\":\"Paragraph:7977e879a762_2\"},{\"__ref\":\"Paragraph:7977e879a762_3\"},{\"__ref\":\"Paragraph:7977e879a762_4\"},{\"__ref\":\"Paragraph:7977e879a762_5\"},{\"__ref\":\"Paragraph:7977e879a762_6\"},{\"__ref\":\"Paragraph:7977e879a762_7\"},{\"__ref\":\"Paragraph:7977e879a762_8\"},{\"__ref\":\"Paragraph:7977e879a762_9\"},{\"__ref\":\"Paragraph:7977e879a762_10\"},{\"__ref\":\"Paragraph:7977e879a762_11\"},{\"__ref\":\"Paragraph:7977e879a762_12\"}]}},\"customStyleSheet\":null,\"isPublished\":true,\"isLocked\":false,\"license\":\"ALL_RIGHTS_RESERVED\",\"collaborators\":[],\"isMarkedPaywallOnly\":false,\"lockedSource\":\"LOCKED_POST_SOURCE_NONE\",\"mediumUrl\":\"https:\\u002F\\u002Fguvencetinkaya.medium.com\\u002Fvisual-odometry-vs-visual-slam-cdda75df592\",\"primaryTopic\":null,\"topics\":[{\"__typename\":\"Topic\",\"slug\":\"artificial-intelligence\",\"name\":\"Artificial Intelligence\"},{\"__typename\":\"Topic\",\"slug\":\"machine-learning\",\"name\":\"Machine Learning\"}],\"latestPublishedVersion\":\"7977e879a762\",\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":0},\"allowResponses\":true,\"isLimitedState\":false,\"voterCount\":5,\"recommenders\":[],\"title\":\"Visual Odometry vs. Visual SLAM vs. Structure-from-Motion\",\"clapCount\":26,\"statusForCollection\":null,\"pinnedAt\":0,\"pinnedByCreatorAt\":0,\"curationEligibleAt\":0,\"responseDistribution\":\"NOT_DISTRIBUTED\",\"inResponseToPostResult\":null,\"inResponseToCatalogResult\":null,\"pendingCollection\":null,\"isNewsletter\":false,\"isAuthorNewsletter\":false,\"layerCake\":0,\"tags\":[{\"__ref\":\"Tag:visual-slam\"},{\"__ref\":\"Tag:visual-odometry\"},{\"__ref\":\"Tag:structure-from-motion\"}],\"sequence\":null,\"readingTime\":2.380188679245283,\"inResponseToEntityType\":null,\"isSeries\":false,\"uniqueSlug\":\"visual-odometry-vs-visual-slam-cdda75df592\",\"socialTitle\":\"\",\"socialDek\":\"\",\"noIndex\":null,\"curationStatus\":\"CURATION_STATUS_DISABLED\",\"metaDescription\":\"\",\"latestPublishedAt\":1654241454240,\"previewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"The main goal of SLAM (Simultanous Localization and Mapping) is to obtain a global, consistent estimate of the robot path. The map of the…\"},\"previewImage\":{\"__ref\":\"ImageMetadata:1*O9_muE5oiSyDsOVfgcuGwQ.jpeg\"},\"isShortform\":false,\"seoTitle\":\"\",\"updatedAt\":1654241466337,\"shortformType\":\"SHORTFORM_TYPE_LINK\",\"seoDescription\":\"\",\"isIndexable\":true,\"isSuspended\":false,\"responseRootPost\":{\"__typename\":\"ResponseRootPost\",\"post\":{\"__ref\":\"Post:cdda75df592\"}},\"internalLinks({\\\"paging\\\":{\\\"limit\\\":8}})\":{\"__typename\":\"InternalLinksConnection\",\"items\":[{\"__ref\":\"Post:5c13ff766f79\"},{\"__ref\":\"Post:9f85a597845d\"},{\"__ref\":\"Post:4ec7cd71ccfd\"},{\"__ref\":\"Post:259d4960b410\"},{\"__ref\":\"Post:bad492232eb6\"},{\"__ref\":\"Post:128268516eb0\"},{\"__ref\":\"Post:c89c4fc7aad0\"},{\"__ref\":\"Post:370e6a0c93e3\"}]},\"awards:countToShowAwardBadge(type:STAFF_PICK,limit:1)\":{\"__typename\":\"AwardConnection\",\"totalCount\":0,\"awards\":[]}},\"Paragraph:7977e879a762_0\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_0\",\"name\":\"b86b\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Visual Odometry vs. Visual SLAM vs. Structure-from-Motion\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"Paragraph:7977e879a762_1\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_1\",\"name\":\"660e\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The main goal of SLAM (Simultanous Localization and Mapping) is to obtain a global, consistent estimate of the robot path. The map of the environment is usually kept just for helping localization. Map information is utilized in Visual Odometry and Loop Clouse blocks. When a loop closure is detected , this information is used to reduce the drift in both the map and pose. Loop detection and loop closure are two main issues in SLAM besides localization[1].\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":23,\"end\":24,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":35,\"end\":36,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":48,\"end\":49,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":52,\"end\":53,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"Paragraph:7977e879a762_2\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_2\",\"name\":\"6a16\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Visual Odometry aims at recovering the path incrementally, pose after pose, and potentially optimizing only over the last n poses of the path (windowed bundle adjustment). In VO, local consistency of the trajectory is the main concern and local map is used to obtain a more accurate estimate of the local trajectory. But SLAM is concerned with the global map consistency [1].\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"EM\",\"start\":122,\"end\":123,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"Paragraph:7977e879a762_3\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_3\",\"name\":\"9b47\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"When we look at Figure-1, the map built by VO resides in leftside and map built by SLAM resides in right side. In SLAM, at point “B”, by the help of loop detection and loop closure, the robot understands that it has passed over that place before and it really understands the real topology of the environment. In VO, the robot feels like moving in an infinite corrider and keeps exploring new areas indefinitely [2].\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"EM\",\"start\":130,\"end\":131,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"ImageMetadata:1*O9_muE5oiSyDsOVfgcuGwQ.jpeg\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*O9_muE5oiSyDsOVfgcuGwQ.jpeg\",\"originalHeight\":226,\"originalWidth\":464,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:7977e879a762_4\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_4\",\"name\":\"62b0\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:1*O9_muE5oiSyDsOVfgcuGwQ.jpeg\"},\"text\":\"Figure-1: Odometry vs. SLAM [2]\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"Paragraph:7977e879a762_5\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_5\",\"name\":\"c19a\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Visual Odometry is one of the main building blocks of Visual SLAM. The other main blocks are Loop-Closure, Backend Optimization and Reconstruction (see Figure-2). Reconstruction block may seem unnecessary in classical SLAM approaches, but some applications need a dense representation of the environment and this is provided by the Reconstruction block. The dense map can be utilized for navigation, obstacle avoidance and interaction purposes in robotic applications [3].\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"ImageMetadata:1*I0mPuSVuQaWa66l5viHB2A.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*I0mPuSVuQaWa66l5viHB2A.png\",\"originalHeight\":647,\"originalWidth\":1458,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:7977e879a762_6\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_6\",\"name\":\"37be\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:1*I0mPuSVuQaWa66l5viHB2A.png\"},\"text\":\"Figure-2: Building blocks of Visual SLAM\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"Paragraph:7977e879a762_7\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_7\",\"name\":\"68fb\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Structure from Motion (SfM) is a more general concept compared to Visual SLAM but there are many commonalities as well. SfM is usually performed offline using unordered sequences of images. SfM is mostly concerned with creating a map of the environment using several images taken from different perspectives. In SfM, images can even be taken from different cameras. Visual SLAM is about solving the localization problem while constructing the map of the environment. The image sequences must be ordered in Visual SLAM and usually they are taken from the same camera. The relation between SfM, Visual SLAM and Visual Odometry is summarized in Figure-3.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"ImageMetadata:1*wzDT0gqiuoaKqmDloO-cNA.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*wzDT0gqiuoaKqmDloO-cNA.png\",\"originalHeight\":428,\"originalWidth\":1190,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:7977e879a762_8\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_8\",\"name\":\"50fd\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:1*wzDT0gqiuoaKqmDloO-cNA.png\"},\"text\":\"Figure-3: SfM vs. V-SLAM vs. VO\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"Paragraph:7977e879a762_9\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_9\",\"name\":\"34bb\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"References\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"Paragraph:7977e879a762_10\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_10\",\"name\":\"8329\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"[1] D. Scaramuzza and F. Fraundorfer, “Visual Odometry Part1: The First 30 Years and Fundamentals”\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"Paragraph:7977e879a762_11\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_11\",\"name\":\"ab1b\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"[2] C. Cadena and L. Carlone and H. Carrillo and Y. Latif and D. Scaramuzza and J. Neira and I. Reid and J.J. Leonard, “Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age”, in IEEE Transactions on Robotics 32 (6) pp 1309–1332, 2016\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"Paragraph:7977e879a762_12\":{\"__typename\":\"Paragraph\",\"id\":\"7977e879a762_12\",\"name\":\"48ea\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"[3] Xiang Gao, Tao Zhang, Yi Liu, and Qinrui Yan.14 Lectureson Visual SLAM: From Theory to Practice. Publishing Houseof Electronics Industry, 2017.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null,\"codeBlockMetadata\":null},\"Tag:visual-slam\":{\"__typename\":\"Tag\",\"id\":\"visual-slam\"},\"Tag:visual-odometry\":{\"__typename\":\"Tag\",\"id\":\"visual-odometry\"},\"Tag:structure-from-motion\":{\"__typename\":\"Tag\",\"id\":\"structure-from-motion\"},\"User:5e1737c0244e\":{\"__typename\":\"User\",\"id\":\"5e1737c0244e\",\"imageId\":\"1*v7rHKdcUpzTecNjQ3PfNFg.png\",\"mediumMemberAt\":0,\"name\":\"Ben Gansky\",\"username\":\"bengansky\",\"customDomainState\":null,\"hasSubdomain\":false,\"bio\":\"Ben Gansky translates complex ideas into accessible, imaginative, and emotionally-rich experiences. His focus is the intersection of tech, policy, and culture.\"},\"ImageMetadata:1*XZn7KJaBpdBojLOfkw42HA.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*XZn7KJaBpdBojLOfkw42HA.png\",\"alt\":null,\"focusPercentX\":null,\"focusPercentY\":null},\"Post:5c13ff766f79\":{\"__typename\":\"Post\",\"id\":\"5c13ff766f79\",\"visibility\":\"PUBLIC\",\"previewContent\":{\"__typename\":\"PreviewContent\",\"isFullContent\":false,\"subtitle\":\"\\\"Perhaps you could just be a little bit less ambitious,\\\" chided UN High Commissioner for Human Rights Zeid Ra'ad Al Hussein.\"},\"collection\":null,\"title\":\"Towards a Better AI Conversation: Why It's Urgent to Distinguish Between ANI & AGI\",\"mediumUrl\":\"https:\\u002F\\u002Fmedium.com\\u002F@bengansky\\u002Fmisunderstanding-ai-we-need-better-conversations-5c13ff766f79\",\"creator\":{\"__ref\":\"User:5e1737c0244e\"},\"previewImage\":{\"__ref\":\"ImageMetadata:1*XZn7KJaBpdBojLOfkw42HA.png\"},\"clapCount\":70,\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"misunderstanding-ai-we-need-better-conversations-5c13ff766f79\"},\"Collection:b4fd82a10a57\":{\"__typename\":\"Collection\",\"id\":\"b4fd82a10a57\",\"slug\":\"hoomano\",\"name\":\"Hoomano\",\"domain\":\"blog.hoomano.com\"},\"User:f10992570cd5\":{\"__typename\":\"User\",\"id\":\"f10992570cd5\",\"imageId\":\"1*i5Zq44YzeuwuASuoez8P7A.jpeg\",\"mediumMemberAt\":1633784755000,\"name\":\"Xavier Basset\",\"username\":\"xavier.basset\",\"customDomainState\":null,\"hasSubdomain\":false,\"bio\":\"Happy Entrepreneur, Developer and Dad. CEO, co-founder at Hoomano. Enjoy #running in the #mountain then appreciate #food & #wine\"},\"ImageMetadata:1*KxN_K2E_8B3JiYr6k22P4A.jpeg\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*KxN_K2E_8B3JiYr6k22P4A.jpeg\",\"alt\":null,\"focusPercentX\":null,\"focusPercentY\":null},\"Post:9f85a597845d\":{\"__typename\":\"Post\",\"id\":\"9f85a597845d\",\"visibility\":\"PUBLIC\",\"previewContent\":{\"__typename\":\"PreviewContent\",\"isFullContent\":false,\"subtitle\":\"In this blog post, we will discuss how we’ve used DALL•E 2 to nurture our own creativity while building Mojo, the emotional digital…\"},\"collection\":{\"__ref\":\"Collection:b4fd82a10a57\"},\"title\":\"DALL•E pushed our creativity - like nothing else before\",\"mediumUrl\":\"https:\\u002F\\u002Fblog.hoomano.com\\u002Fdall-e-pushed-our-creativity-like-nothing-else-before-9f85a597845d\",\"creator\":{\"__ref\":\"User:f10992570cd5\"},\"previewImage\":{\"__ref\":\"ImageMetadata:1*KxN_K2E_8B3JiYr6k22P4A.jpeg\"},\"clapCount\":0,\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"dall-e-pushed-our-creativity-like-nothing-else-before-9f85a597845d\"},\"Collection:30e36b74270b\":{\"__typename\":\"Collection\",\"id\":\"30e36b74270b\",\"slug\":\"drivebuddyai\",\"name\":\"drivebuddyAI\",\"domain\":null},\"User:10930538d4a1\":{\"__typename\":\"User\",\"id\":\"10930538d4a1\",\"imageId\":\"1*Iy4G4IZgZ-PeMEfltSICrQ.jpeg\",\"mediumMemberAt\":0,\"name\":\"@nisargpandya\",\"username\":\"nisargpandya09\",\"customDomainState\":{\"__typename\":\"CustomDomainState\",\"live\":{\"__typename\":\"CustomDomain\",\"domain\":\"nisargpandya09.medium.com\"}},\"hasSubdomain\":true,\"bio\":\"Entrepreneur, Startup Founder & CEO @drivebuddyAI. Learning from human drivers for the future drivers.\"},\"ImageMetadata:1*9SPTcrbjhnJWk-6kOoLHWQ.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*9SPTcrbjhnJWk-6kOoLHWQ.png\",\"alt\":null,\"focusPercentX\":null,\"focusPercentY\":null},\"Post:4ec7cd71ccfd\":{\"__typename\":\"Post\",\"id\":\"4ec7cd71ccfd\",\"visibility\":\"PUBLIC\",\"previewContent\":{\"__typename\":\"PreviewContent\",\"isFullContent\":false,\"subtitle\":\"Sharing economy, reduction in the personal vehicle ownership, electric vehicles are the revolutionary terms of Indian transportation…\"},\"collection\":{\"__ref\":\"Collection:30e36b74270b\"},\"title\":\"Driver Behavior and Profiling — The Driver Centric Approach\",\"mediumUrl\":\"https:\\u002F\\u002Fmedium.com\\u002Fdrivebuddyai\\u002Fdriver-behavior-and-profiling-the-driver-centric-approach-4ec7cd71ccfd\",\"creator\":{\"__ref\":\"User:10930538d4a1\"},\"previewImage\":{\"__ref\":\"ImageMetadata:1*9SPTcrbjhnJWk-6kOoLHWQ.png\"},\"clapCount\":21,\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"driver-behavior-and-profiling-the-driver-centric-approach-4ec7cd71ccfd\"},\"Collection:74d3d7d95404\":{\"__typename\":\"Collection\",\"id\":\"74d3d7d95404\",\"slug\":\"h-weekly\",\"name\":\"H+ Weekly\",\"domain\":null},\"User:e60a556ba1d4\":{\"__typename\":\"User\",\"id\":\"e60a556ba1d4\",\"imageId\":\"1*zQTC-GB1jFGDy3FdvERmNw.jpeg\",\"mediumMemberAt\":1590217063000,\"name\":\"Conrad Gray\",\"username\":\"conradthegray\",\"customDomainState\":{\"__typename\":\"CustomDomainState\",\"live\":{\"__typename\":\"CustomDomain\",\"domain\":\"conradthegray.medium.com\"}},\"hasSubdomain\":true,\"bio\":\"I write about biotech, learning, computing and futurism | http:\\u002F\\u002Fconradthegray.com\"},\"ImageMetadata:1*UQp8OJUzEanmmpVbU6glrA.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*UQp8OJUzEanmmpVbU6glrA.png\",\"alt\":null,\"focusPercentX\":null,\"focusPercentY\":null},\"Post:259d4960b410\":{\"__typename\":\"Post\",\"id\":\"259d4960b410\",\"visibility\":\"PUBLIC\",\"previewContent\":{\"__typename\":\"PreviewContent\",\"isFullContent\":false,\"subtitle\":\"Hyundai to buy Boston Dynamics; mind-controlled beer pong; how to start a robot revolution; AI that makes pyjamas safe for work; and more!\"},\"collection\":{\"__ref\":\"Collection:74d3d7d95404\"},\"title\":\"H+ Weekly — Issue #288\",\"mediumUrl\":\"https:\\u002F\\u002Fmedium.com\\u002Fh-weekly\\u002Fh-weekly-issue-288-259d4960b410\",\"creator\":{\"__ref\":\"User:e60a556ba1d4\"},\"previewImage\":{\"__ref\":\"ImageMetadata:1*UQp8OJUzEanmmpVbU6glrA.png\"},\"clapCount\":0,\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"h-weekly-issue-288-259d4960b410\"},\"Collection:f5af2b715248\":{\"__typename\":\"Collection\",\"id\":\"f5af2b715248\",\"slug\":\"swlh\",\"name\":\"The Startup\",\"domain\":null},\"User:82e29aec1699\":{\"__typename\":\"User\",\"id\":\"82e29aec1699\",\"imageId\":\"2*qDL_LqQFrjOz6LafhxnyRQ.jpeg\",\"mediumMemberAt\":0,\"name\":\"Ensar Seker\",\"username\":\"ensarseker1\",\"customDomainState\":{\"__typename\":\"CustomDomainState\",\"live\":{\"__typename\":\"CustomDomain\",\"domain\":\"ensarseker1.medium.com\"}},\"hasSubdomain\":true,\"bio\":\"Cybersecurity | Artificial Intelligence | Blockchain\"},\"ImageMetadata:0*72JIHUviJJ9bXwEU\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*72JIHUviJJ9bXwEU\",\"alt\":null,\"focusPercentX\":null,\"focusPercentY\":null},\"Post:bad492232eb6\":{\"__typename\":\"Post\",\"id\":\"bad492232eb6\",\"visibility\":\"LOCKED\",\"previewContent\":{\"__typename\":\"PreviewContent\",\"isFullContent\":false,\"subtitle\":\"Payment fraud has a long history and is the most common form of online fraud in the United States and the world. Recently, however…\"},\"collection\":{\"__ref\":\"Collection:f5af2b715248\"},\"title\":\"Machine Learning-based Digital Fraud Detection\",\"mediumUrl\":\"https:\\u002F\\u002Fmedium.com\\u002Fswlh\\u002Fmachine-learning-based-digital-fraud-detection-bad492232eb6\",\"creator\":{\"__ref\":\"User:82e29aec1699\"},\"previewImage\":{\"__ref\":\"ImageMetadata:0*72JIHUviJJ9bXwEU\"},\"clapCount\":164,\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"machine-learning-based-digital-fraud-detection-bad492232eb6\"},\"User:17aae94724d0\":{\"__typename\":\"User\",\"id\":\"17aae94724d0\",\"imageId\":\"1*aSoBSEIQQWJwr3oOpdBLdw.png\",\"mediumMemberAt\":0,\"name\":\"Ｅｎｅｒｇｉ\",\"username\":\"thatgirlenergi\",\"customDomainState\":{\"__typename\":\"CustomDomainState\",\"live\":{\"__typename\":\"CustomDomain\",\"domain\":\"thatgirlenergi.medium.com\"}},\"hasSubdomain\":true,\"bio\":\"Technology writer for brands | AI, VR, Blockchain | A well-designed life is the only one worth living.\"},\"ImageMetadata:1*QLhgYX3h0Q90YWkiKCliPw.jpeg\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*QLhgYX3h0Q90YWkiKCliPw.jpeg\",\"alt\":null,\"focusPercentX\":null,\"focusPercentY\":null},\"Post:128268516eb0\":{\"__typename\":\"Post\",\"id\":\"128268516eb0\",\"visibility\":\"LOCKED\",\"previewContent\":{\"__typename\":\"PreviewContent\",\"isFullContent\":false,\"subtitle\":\"The mention of artificial intelligence causes most people to type “how to prepare for the robot apocalypse” into Google.\"},\"collection\":null,\"title\":\"We fear AI because we fear ourselves.\",\"mediumUrl\":\"https:\\u002F\\u002Fthatgirlenergi.medium.com\\u002Fwe-fear-ai-because-we-fear-ourselves-128268516eb0\",\"creator\":{\"__ref\":\"User:17aae94724d0\"},\"previewImage\":{\"__ref\":\"ImageMetadata:1*QLhgYX3h0Q90YWkiKCliPw.jpeg\"},\"clapCount\":0,\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"we-fear-ai-because-we-fear-ourselves-128268516eb0\"},\"User:7a0f88aa2b40\":{\"__typename\":\"User\",\"id\":\"7a0f88aa2b40\",\"imageId\":\"0*xMwHLJfFmD2qGY-H.jpg\",\"mediumMemberAt\":0,\"name\":\"Ryan Young\",\"username\":\"ryoung19-86531\",\"customDomainState\":{\"__typename\":\"CustomDomainState\",\"live\":{\"__typename\":\"CustomDomain\",\"domain\":\"ryoung19-86531.medium.com\"}},\"hasSubdomain\":true,\"bio\":\"\"},\"ImageMetadata:\":{\"__typename\":\"ImageMetadata\",\"id\":\"\",\"alt\":null,\"focusPercentX\":null,\"focusPercentY\":null},\"Post:c89c4fc7aad0\":{\"__typename\":\"Post\",\"id\":\"c89c4fc7aad0\",\"visibility\":\"PUBLIC\",\"previewContent\":{\"__typename\":\"PreviewContent\",\"isFullContent\":false,\"subtitle\":\"With Innovations and new technology on the rise, our future has never looked more modernized. New inventions such as the iPhone…\"},\"collection\":null,\"title\":\"Our Modernized World\",\"mediumUrl\":\"https:\\u002F\\u002Fryoung19-86531.medium.com\\u002Four-modernized-world-c89c4fc7aad0\",\"creator\":{\"__ref\":\"User:7a0f88aa2b40\"},\"previewImage\":{\"__ref\":\"ImageMetadata:\"},\"clapCount\":0,\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"our-modernized-world-c89c4fc7aad0\"},\"User:4f2df6c5b577\":{\"__typename\":\"User\",\"id\":\"4f2df6c5b577\",\"imageId\":\"1*NRiSGM8ZQQKNMJZ9VZGCjg.png\",\"mediumMemberAt\":1662722649000,\"name\":\"Sahika Betul Yayli\",\"username\":\"sahika.betul\",\"customDomainState\":null,\"hasSubdomain\":false,\"bio\":\"Medical Doctor | AI & ML Engineer\"},\"ImageMetadata:1*ZN03ZQQnBpfKJW3l0ARDrA.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*ZN03ZQQnBpfKJW3l0ARDrA.png\",\"alt\":null,\"focusPercentX\":null,\"focusPercentY\":null},\"Post:370e6a0c93e3\":{\"__typename\":\"Post\",\"id\":\"370e6a0c93e3\",\"visibility\":\"PUBLIC\",\"previewContent\":{\"__typename\":\"PreviewContent\",\"isFullContent\":false,\"subtitle\":\"AI4HEALTH Article Reviews #05\"},\"collection\":null,\"title\":\"Is Speech the New Blood? Recent Progress in AI-Based Disease Detection From Audio in a Nutshell\",\"mediumUrl\":\"https:\\u002F\\u002Fmedium.com\\u002F@sahika.betul\\u002Fis-speech-the-new-blood-recent-progress-in-ai-based-disease-detection-from-audio-in-a-nutshell-370e6a0c93e3\",\"creator\":{\"__ref\":\"User:4f2df6c5b577\"},\"previewImage\":{\"__ref\":\"ImageMetadata:1*ZN03ZQQnBpfKJW3l0ARDrA.png\"},\"clapCount\":0,\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"is-speech-the-new-blood-recent-progress-in-ai-based-disease-detection-from-audio-in-a-nutshell-370e6a0c93e3\"}}</script><script>window.__MIDDLEWARE_STATE__={\"session\":{\"xsrf\":\"\"},\"cache\":{\"cacheStatus\":\"HIT\",\"inDisabledExperiment\":false}}</script><script src=\"https://cdn-client.medium.com/lite/static/js/manifest.fd985ecc.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/221.eb6d4e84.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/main.1550b08e.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/instrumentation.c71f0248.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/4800.b97019a4.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/7371.4a3c1218.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/9470.78d87a02.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/2837.0014ff33.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/AppLayout.b90dfcce.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/reporting.bbdcaa9d.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/2953.6a4984da.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/1752.a348f767.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/7794.9590314e.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/8316.18f2a6aa.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/5472.5f6d4371.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/4330.73510d98.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/2981.c8b67800.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/3115.cd279c61.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/5758.4d052c2f.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/4869.c26b42a4.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/9401.492bc814.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/2307.b2a54ca4.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/7070.6c864471.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/9442.5291e270.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/4483.0a43a5ce.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/210.1b33e4a9.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/864.dc58ca67.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/9841.1bb423da.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/3610.44f23015.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/1018.3d424dd7.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/9304.78e04611.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/763.3dd24340.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/8051.c536c001.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/9241.b09496de.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/5887.70c709b9.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/5754.6687b8d5.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.3e4f03fd.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/1987.e87f9d80.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/8237.1eb3b71c.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/7994.7409a9e8.chunk.js\"></script>\n",
              "<script src=\"https://cdn-client.medium.com/lite/static/js/PostPage.RightColumnContent.3109f2af.chunk.js\"></script><script>window.main();</script><script crossorigin=\"anonymous\" data-cf-beacon='{\"rayId\":\"757c6fab7906b130\",\"token\":\"0b5f665943484354a59c39c6833f7078\",\"version\":\"2022.8.1\",\"si\":100}' defer=\"\" integrity=\"sha512-Gi7xpJR8tSkrpF7aordPZQlW2DLtzUlZcumS8dMQjwDHEnw9I7ZLyiOj/6tZStRBGtGgN6ceN6cMH8z7etPGlw==\" src=\"https://static.cloudflareinsights.com/beacon.min.js/v652eace1692a40cfa3763df669d7439c1639079717194\"></script>\n",
              "</body></html>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2SkzYAS9_AC",
        "outputId": "a0cdab71-248f-4e8a-ae42-603806a1a42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visual Odometry vs. Visual SLAM vs. Structure-from-Motion\n"
          ]
        }
      ],
      "source": [
        "# getting the title\n",
        "print(sp.find(\"h1\", {\"class\" : \"pw-post-title ie if ig bm ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc gh\"}).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7L3kMIEZRJ3",
        "outputId": "363899e9-c7d3-4e8d-bd00-f813f182a4bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oct 22, 2021\n"
          ]
        }
      ],
      "source": [
        "# Release date.\n",
        "print(sp.find(\"p\", {\"class\" : \"pw-published-date bm b bn bo cn\"}).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YBEETJjZy8a",
        "outputId": "971ffc42-3831-4c22-b9c9-ce3fc7b49abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The main goal of SLAM (Simultanous Localization and Mapping) is to obtain a global, consistent estimate of the robot path. The map of the environment is usually kept just for helping localization. Map information is utilized in Visual Odometry and Loop Clouse blocks. When a loop closure is detected , this information is used to reduce the drift in both the map and pose. Loop detection and loop closure are two main issues in SLAM besides localization[1].\n",
            "\n",
            "\n",
            "Visual Odometry aims at recovering the path incrementally, pose after pose, and potentially optimizing only over the last n poses of the path (windowed bundle adjustment). In VO, local consistency of the trajectory is the main concern and local map is used to obtain a more accurate estimate of the local trajectory. But SLAM is concerned with the global map consistency [1].\n",
            "\n",
            "\n",
            "When we look at Figure-1, the map built by VO resides in leftside and map built by SLAM resides in right side. In SLAM, at point “B”, by the help of loop detection and loop closure, the robot understands that it has passed over that place before and it really understands the real topology of the environment. In VO, the robot feels like moving in an infinite corrider and keeps exploring new areas indefinitely [2].\n",
            "\n",
            "\n",
            "Visual Odometry is one of the main building blocks of Visual SLAM. The other main blocks are Loop-Closure, Backend Optimization and Reconstruction (see Figure-2). Reconstruction block may seem unnecessary in classical SLAM approaches, but some applications need a dense representation of the environment and this is provided by the Reconstruction block. The dense map can be utilized for navigation, obstacle avoidance and interaction purposes in robotic applications [3].\n",
            "\n",
            "\n",
            "Structure from Motion (SfM) is a more general concept compared to Visual SLAM but there are many commonalities as well. SfM is usually performed offline using unordered sequences of images. SfM is mostly concerned with creating a map of the environment using several images taken from different perspectives. In SfM, images can even be taken from different cameras. Visual SLAM is about solving the localization problem while constructing the map of the environment. The image sequences must be ordered in Visual SLAM and usually they are taken from the same camera. The relation between SfM, Visual SLAM and Visual Odometry is summarized in Figure-3.\n",
            "\n",
            "\n",
            "[1] D. Scaramuzza and F. Fraundorfer, “Visual Odometry Part1: The First 30 Years and Fundamentals”\n",
            "\n",
            "\n",
            "[2] C. Cadena and L. Carlone and H. Carrillo and Y. Latif and D. Scaramuzza and J. Neira and I. Reid and J.J. Leonard, “Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age”, in IEEE Transactions on Robotics 32 (6) pp 1309–1332, 2016\n",
            "\n",
            "\n",
            "[3] Xiang Gao, Tao Zhang, Yi Liu, and Qinrui Yan.14 Lectureson Visual SLAM: From Theory to Practice. Publishing Houseof Electronics Industry, 2017.\n"
          ]
        }
      ],
      "source": [
        "# getting the sub heading\n",
        "body = sp.find(\"div\", {\"class\" : \"hz ia ib ic id\"})\n",
        "for p in body.find_all('p'):\n",
        "    print('\\n')\n",
        "    print(p.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpaKXREudxSI"
      },
      "source": [
        "# Question 4: \n",
        "Also explore the scrapy library to perform webscraping apart from the three discussed above in the demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "ir33LlN_d7hO",
        "outputId": "7a7653ce-e015-4c5b-c64a-91c74eebb02b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.7/dist-packages (2.6.3)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (22.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (2.0.1)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.7/dist-packages (from scrapy) (0.2.1)\n",
            "Requirement already satisfied: zope.interface>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (5.4.0)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.1.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy) (57.4.0)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.9.1)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.1.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.6.0)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.7/dist-packages (from scrapy) (3.4.0)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.7/dist-packages (from scrapy) (2.0.6)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.7/dist-packages (from scrapy) (38.0.1)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.0.6)\n",
            "Requirement already satisfied: Twisted>=18.9.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (22.8.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.6.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.3->scrapy) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.7/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (22.1.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (20.2.0)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (21.3.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (15.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (4.1.1)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=18.9.0->scrapy) (2.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (2.23.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (3.8.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (1.5.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.12\n",
            "    Uninstalling urllib3-1.26.12:\n",
            "      Successfully uninstalled urllib3-1.26.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "selenium 4.5.0 requires urllib3[socks]~=1.26, but you have urllib3 1.25.11 which is incompatible.\u001b[0m\n",
            "Successfully installed urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install scrapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAbFU0Nrj-kS",
        "outputId": "a57f29d5-09b1-4ccb-a0ea-90a5298ee376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: crochet in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from crochet) (1.14.1)\n",
            "Requirement already satisfied: Twisted>=16.0 in /usr/local/lib/python3.7/dist-packages (from crochet) (22.8.0)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (21.3.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (20.2.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (21.0.0)\n",
            "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (5.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (22.1.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (15.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Automat>=0.8.0->Twisted>=16.0->crochet) (1.15.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=16.0->crochet) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.4.2->Twisted>=16.0->crochet) (57.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install crochet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tiLnXscjpIo"
      },
      "outputs": [],
      "source": [
        "# Settings for notebook\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "# Show Python version\n",
        "import platform\n",
        "platform.python_version()\n",
        "from crochet import setup, wait_for\n",
        "setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjYR3c1qigaQ"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoMlZSn_ilJu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "class JsonWriterPipeline(object):\n",
        "\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('quoteresult.jl', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcmI_7Adin4y"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "class QuotesSpider(scrapy.Spider):\n",
        "    name = \"quotes\"\n",
        "    start_urls = [\n",
        "        'http://quotes.toscrape.com/page/1/',\n",
        "        'http://quotes.toscrape.com/page/2/',\n",
        "    ]\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
        "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
        "        'FEED_URI': 'quoteresult.json'                        # Used for pipeline 2\n",
        "    }\n",
        "    \n",
        "    def parse(self, response):\n",
        "        for quote in response.css('div.quote'):\n",
        "            yield {\n",
        "                'text': quote.css('span.text::text').extract_first(),\n",
        "                'author': quote.css('span small::text').extract_first(),\n",
        "                'tags': quote.css('div.tags a.tag::text').extract(),\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FTLggMPip0b",
        "outputId": "7ef784af-8cd7-47e8-d7cf-e5eaeddeba12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:scrapy.utils.log:Scrapy 2.6.3 started (bot: scrapybot)\n",
            "2022-10-10 03:59:13 [scrapy.utils.log] INFO: Scrapy 2.6.3 started (bot: scrapybot)\n",
            "INFO:scrapy.utils.log:Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.7.14 (default, Sep  8 2022, 00:06:44) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-10-10 03:59:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.7.14 (default, Sep  8 2022, 00:06:44) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "INFO:scrapy.crawler:Overridden settings:\n",
            "{'LOG_LEVEL': 30}\n",
            "2022-10-10 03:59:13 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'LOG_LEVEL': 30}\n",
            "DEBUG:scrapy.utils.log:Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "INFO:scrapy.extensions.telnet:Telnet Password: 9ff08670a37928cd\n",
            "INFO:scrapy.middleware:Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "INFO:scrapy.middleware:Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "INFO:scrapy.middleware:Enabled item pipelines:\n",
            "['__main__.JsonWriterPipeline']\n",
            "INFO:scrapy.core.engine:Spider opened\n",
            "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Deferred at 0x7f492fe72190>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# process = CrawlerProcess({\n",
        "#     'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "# })\n",
        "process = CrawlerProcess()\n",
        "process.crawl(QuotesSpider)\n",
        "# process.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNAJK_X-it1p",
        "outputId": "12ac0403-4b46-4478-ed78-70089aa03746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root  5551 Oct 10 03:59 quoteresult.jl\n",
            "-rw-r--r-- 1 root 11146 Oct 10 03:59 quoteresult.json\n"
          ]
        }
      ],
      "source": [
        "ll quoteresult.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8laepCsOizw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139effa4-980d-49ca-a276-6d19eeb79bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\": \"\\u201cGood friends, good books, and a sleepy conscience: this is the ideal life.\\u201d\", \"author\": \"Mark Twain\", \"tags\": [\"books\", \"contentment\", \"friends\", \"friendship\", \"life\"]}\n",
            "{\"text\": \"\\u201cLife is what happens to us while we are making other plans.\\u201d\", \"author\": \"Allen Saunders\", \"tags\": [\"fate\", \"life\", \"misattributed-john-lennon\", \"planning\", \"plans\"]}\n"
          ]
        }
      ],
      "source": [
        "!tail -n 2 quoteresult.jl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMd4qMzMi3NC",
        "outputId": "6c9d7ac8-cc8a-4fb1-8550-9a8e1e12c1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\": \"\\u201cLife is what happens to us while we are making other plans.\\u201d\", \"author\": \"Allen Saunders\", \"tags\": [\"fate\", \"life\", \"misattributed-john-lennon\", \"planning\", \"plans\"]}\n",
            "]"
          ]
        }
      ],
      "source": [
        "!tail -n 2 quoteresult.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfjl = pd.read_json('quoteresult.jl', lines=True)\n",
        "dfjl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "u325tXsFDPMx",
        "outputId": "d2a47ed2-52b0-4591-eb8f-04507685e33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 text               author  \\\n",
              "0   “The world as we have created it is a process ...      Albert Einstein   \n",
              "1   “It is our choices, Harry, that show what we t...         J.K. Rowling   \n",
              "2   “There are only two ways to live your life. On...      Albert Einstein   \n",
              "3   “The person, be it gentleman or lady, who has ...          Jane Austen   \n",
              "4   “Imperfection is beauty, madness is genius and...       Marilyn Monroe   \n",
              "5   “This life is what you make it. No matter what...       Marilyn Monroe   \n",
              "6   “It takes a great deal of bravery to stand up ...         J.K. Rowling   \n",
              "7   “Try not to become a man of success. Rather be...      Albert Einstein   \n",
              "8   “It is better to be hated for what you are tha...           André Gide   \n",
              "9   “If you can't explain it to a six year old, yo...      Albert Einstein   \n",
              "10  “You may not be her first, her last, or her on...           Bob Marley   \n",
              "11  “I like nonsense, it wakes up the brain cells....            Dr. Seuss   \n",
              "12  “I have not failed. I've just found 10,000 way...     Thomas A. Edison   \n",
              "13  “A woman is like a tea bag; you never know how...    Eleanor Roosevelt   \n",
              "14  “A day without sunshine is like, you know, nig...         Steve Martin   \n",
              "15  “I may not have gone where I intended to go, b...        Douglas Adams   \n",
              "16  “The opposite of love is not hate, it's indiff...          Elie Wiesel   \n",
              "17  “It is not a lack of love, but a lack of frien...  Friedrich Nietzsche   \n",
              "18  “Good friends, good books, and a sleepy consci...           Mark Twain   \n",
              "19  “Life is what happens to us while we are makin...       Allen Saunders   \n",
              "\n",
              "                                                 tags  \n",
              "0            [change, deep-thoughts, thinking, world]  \n",
              "1                                [abilities, choices]  \n",
              "2      [inspirational, life, live, miracle, miracles]  \n",
              "3                  [aliteracy, books, classic, humor]  \n",
              "4                        [be-yourself, inspirational]  \n",
              "5   [friends, heartbreak, inspirational, life, lov...  \n",
              "6                                  [courage, friends]  \n",
              "7                         [adulthood, success, value]  \n",
              "8                                        [life, love]  \n",
              "9                            [simplicity, understand]  \n",
              "10                                             [love]  \n",
              "11                                          [fantasy]  \n",
              "12      [edison, failure, inspirational, paraphrased]  \n",
              "13                  [misattributed-eleanor-roosevelt]  \n",
              "14                           [humor, obvious, simile]  \n",
              "15                                 [life, navigation]  \n",
              "16  [activism, apathy, hate, indifference, inspira...  \n",
              "17  [friendship, lack-of-friendship, lack-of-love,...  \n",
              "18    [books, contentment, friends, friendship, life]  \n",
              "19  [fate, life, misattributed-john-lennon, planni...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f7919ed9-14bb-4aa1-beca-7e70e2cddadb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>“The world as we have created it is a process ...</td>\n",
              "      <td>Albert Einstein</td>\n",
              "      <td>[change, deep-thoughts, thinking, world]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>“It is our choices, Harry, that show what we t...</td>\n",
              "      <td>J.K. Rowling</td>\n",
              "      <td>[abilities, choices]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>“There are only two ways to live your life. On...</td>\n",
              "      <td>Albert Einstein</td>\n",
              "      <td>[inspirational, life, live, miracle, miracles]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>“The person, be it gentleman or lady, who has ...</td>\n",
              "      <td>Jane Austen</td>\n",
              "      <td>[aliteracy, books, classic, humor]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>“Imperfection is beauty, madness is genius and...</td>\n",
              "      <td>Marilyn Monroe</td>\n",
              "      <td>[be-yourself, inspirational]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>“This life is what you make it. No matter what...</td>\n",
              "      <td>Marilyn Monroe</td>\n",
              "      <td>[friends, heartbreak, inspirational, life, lov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>“It takes a great deal of bravery to stand up ...</td>\n",
              "      <td>J.K. Rowling</td>\n",
              "      <td>[courage, friends]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>“Try not to become a man of success. Rather be...</td>\n",
              "      <td>Albert Einstein</td>\n",
              "      <td>[adulthood, success, value]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>“It is better to be hated for what you are tha...</td>\n",
              "      <td>André Gide</td>\n",
              "      <td>[life, love]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>“If you can't explain it to a six year old, yo...</td>\n",
              "      <td>Albert Einstein</td>\n",
              "      <td>[simplicity, understand]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>“You may not be her first, her last, or her on...</td>\n",
              "      <td>Bob Marley</td>\n",
              "      <td>[love]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>“I like nonsense, it wakes up the brain cells....</td>\n",
              "      <td>Dr. Seuss</td>\n",
              "      <td>[fantasy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>“I have not failed. I've just found 10,000 way...</td>\n",
              "      <td>Thomas A. Edison</td>\n",
              "      <td>[edison, failure, inspirational, paraphrased]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>“A woman is like a tea bag; you never know how...</td>\n",
              "      <td>Eleanor Roosevelt</td>\n",
              "      <td>[misattributed-eleanor-roosevelt]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>“A day without sunshine is like, you know, nig...</td>\n",
              "      <td>Steve Martin</td>\n",
              "      <td>[humor, obvious, simile]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>“I may not have gone where I intended to go, b...</td>\n",
              "      <td>Douglas Adams</td>\n",
              "      <td>[life, navigation]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>“The opposite of love is not hate, it's indiff...</td>\n",
              "      <td>Elie Wiesel</td>\n",
              "      <td>[activism, apathy, hate, indifference, inspira...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>“It is not a lack of love, but a lack of frien...</td>\n",
              "      <td>Friedrich Nietzsche</td>\n",
              "      <td>[friendship, lack-of-friendship, lack-of-love,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>“Good friends, good books, and a sleepy consci...</td>\n",
              "      <td>Mark Twain</td>\n",
              "      <td>[books, contentment, friends, friendship, life]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>“Life is what happens to us while we are makin...</td>\n",
              "      <td>Allen Saunders</td>\n",
              "      <td>[fate, life, misattributed-john-lennon, planni...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7919ed9-14bb-4aa1-beca-7e70e2cddadb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f7919ed9-14bb-4aa1-beca-7e70e2cddadb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f7919ed9-14bb-4aa1-beca-7e70e2cddadb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GMNz7SPlsDp"
      },
      "source": [
        "# Question 5:\n",
        "Pick a website that has tabular data (can be one of the two selected above) and try to scrap it using the tools studied during the demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl3QiXnvluB8",
        "outputId": "1e8f1a09-f27c-44c4-85ee-4bba1293770c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Websites Popularity(unique visitors per month)[1]  Front-end(Client-side)  \\\n",
            "0  Google[2]                               2500000000  JavaScript, TypeScript   \n",
            "1   Facebook                               1120000000              JavaScript   \n",
            "2    YouTube                               1100000000  JavaScript, TypeScript   \n",
            "3      Yahoo                                750000000              JavaScript   \n",
            "4       Etsy      516,000,000 (Total, not unique)[15]              JavaScript   \n",
            "\n",
            "                               Back-end(Server-side)  \\\n",
            "0                  C, C++, Go,[3] Java, Python, Node   \n",
            "1  Hack, PHP (HHVM), Python, C++, Java, Erlang, D...   \n",
            "2                   C, C++, Python, Java,[11] Go[12]   \n",
            "3                                                PHP   \n",
            "4                                        PHP[16][17]   \n",
            "\n",
            "                                     Database  \\\n",
            "0                     Bigtable,[4] MariaDB[5]   \n",
            "1     MariaDB, MySQL,[9] HBase, Cassandra[10]   \n",
            "2            Vitess, BigTable, MariaDB[5][13]   \n",
            "3  PostgreSQL, HBase, Cassandra, MongoDB,[14]   \n",
            "4                            MySQL, Redis[18]   \n",
            "\n",
            "                                               Notes  \n",
            "0           The most used search engine in the world  \n",
            "1            The most visited social networking site  \n",
            "2  The most popular video sharing site [YouTube i...  \n",
            "3                                                NaN  \n",
            "4                                E-commerce website.  \n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "url = \"https://en.wikipedia.org/wiki/Programming_languages_used_in_most_popular_websites\"\n",
        "\n",
        "with urllib.request.urlopen(url) as i:\n",
        "    html = i.read()\n",
        "    \n",
        "data = pd.read_html(html)[0]\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCi4BHGSlxni"
      },
      "outputs": [],
      "source": [
        "data.to_csv(\"programming.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vYWf8KnEl3JQ",
        "outputId": "73c04e1e-33e7-4509-96f6-323b34b1d76a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0       Websites Popularity(unique visitors per month)[1]  \\\n",
              "0            0      Google[2]                               2500000000   \n",
              "1            1       Facebook                               1120000000   \n",
              "2            2        YouTube                               1100000000   \n",
              "3            3          Yahoo                                750000000   \n",
              "4            4           Etsy      516,000,000 (Total, not unique)[15]   \n",
              "5            5         Amazon                                500000000   \n",
              "6            6      Wikipedia                                475000000   \n",
              "7            7         Fandom                          315,000,000[22]   \n",
              "8            8        Twitter                                290000000   \n",
              "9            9           Bing                                285000000   \n",
              "10          10           eBay                                285000000   \n",
              "11          11            MSN                                280000000   \n",
              "12          12       LinkedIn                                260000000   \n",
              "13          13      Pinterest                                250000000   \n",
              "14          14  WordPress.com                                240000000   \n",
              "\n",
              "    Front-end(Client-side)                              Back-end(Server-side)  \\\n",
              "0   JavaScript, TypeScript                  C, C++, Go,[3] Java, Python, Node   \n",
              "1               JavaScript  Hack, PHP (HHVM), Python, C++, Java, Erlang, D...   \n",
              "2   JavaScript, TypeScript                   C, C++, Python, Java,[11] Go[12]   \n",
              "3               JavaScript                                                PHP   \n",
              "4               JavaScript                                        PHP[16][17]   \n",
              "5               JavaScript                                Java, C++, Perl[19]   \n",
              "6               JavaScript                                                PHP   \n",
              "7               JavaScript                                                PHP   \n",
              "8               JavaScript                     C++, Java,[23] Scala,[24] Ruby   \n",
              "9               JavaScript                                            C++, C#   \n",
              "10              JavaScript                Java,[26] JavaScript,[27] Scala[28]   \n",
              "11              JavaScript                                                 C#   \n",
              "12              JavaScript                        Java, JavaScript,[29] Scala   \n",
              "13              JavaScript            Python (Django),[32] Erlang, Elixir[33]   \n",
              "14              JavaScript                                            PHP[35]   \n",
              "\n",
              "                                      Database  \\\n",
              "0                      Bigtable,[4] MariaDB[5]   \n",
              "1      MariaDB, MySQL,[9] HBase, Cassandra[10]   \n",
              "2             Vitess, BigTable, MariaDB[5][13]   \n",
              "3   PostgreSQL, HBase, Cassandra, MongoDB,[14]   \n",
              "4                             MySQL, Redis[18]   \n",
              "5           DynamoDB, RDS/Aurora, Redshift[20]   \n",
              "6                                  MariaDB[21]   \n",
              "7                                        MySQL   \n",
              "8                                    MySQL[25]   \n",
              "9              Microsoft SQL Server, Cosmos DB   \n",
              "10                             Oracle Database   \n",
              "11                        Microsoft SQL Server   \n",
              "12                              Venice[30][31]   \n",
              "13                            MySQL, Redis[34]   \n",
              "14                                 MariaDB[36]   \n",
              "\n",
              "                                                Notes  \n",
              "0            The most used search engine in the world  \n",
              "1             The most visited social networking site  \n",
              "2   The most popular video sharing site [YouTube i...  \n",
              "3                                                 NaN  \n",
              "4                                 E-commerce website.  \n",
              "5          The most used e-commerce site in the world  \n",
              "6   A free online encyclopedia based on MediaWiki,...  \n",
              "7                               Wiki hosting service.  \n",
              "8                              Popular social network  \n",
              "9                       Search engine from Microsoft.  \n",
              "10                              Online auction house.  \n",
              "11  An email client, for simple use. Previously kn...  \n",
              "12              World's largest professional network.  \n",
              "13                           Search engine for ideas.  \n",
              "14                          Website manager software.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11c19fd8-a325-4062-8b9c-39d7816dbe77\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Websites</th>\n",
              "      <th>Popularity(unique visitors per month)[1]</th>\n",
              "      <th>Front-end(Client-side)</th>\n",
              "      <th>Back-end(Server-side)</th>\n",
              "      <th>Database</th>\n",
              "      <th>Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Google[2]</td>\n",
              "      <td>2500000000</td>\n",
              "      <td>JavaScript, TypeScript</td>\n",
              "      <td>C, C++, Go,[3] Java, Python, Node</td>\n",
              "      <td>Bigtable,[4] MariaDB[5]</td>\n",
              "      <td>The most used search engine in the world</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Facebook</td>\n",
              "      <td>1120000000</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>Hack, PHP (HHVM), Python, C++, Java, Erlang, D...</td>\n",
              "      <td>MariaDB, MySQL,[9] HBase, Cassandra[10]</td>\n",
              "      <td>The most visited social networking site</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>YouTube</td>\n",
              "      <td>1100000000</td>\n",
              "      <td>JavaScript, TypeScript</td>\n",
              "      <td>C, C++, Python, Java,[11] Go[12]</td>\n",
              "      <td>Vitess, BigTable, MariaDB[5][13]</td>\n",
              "      <td>The most popular video sharing site [YouTube i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Yahoo</td>\n",
              "      <td>750000000</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>PHP</td>\n",
              "      <td>PostgreSQL, HBase, Cassandra, MongoDB,[14]</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Etsy</td>\n",
              "      <td>516,000,000 (Total, not unique)[15]</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>PHP[16][17]</td>\n",
              "      <td>MySQL, Redis[18]</td>\n",
              "      <td>E-commerce website.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>Amazon</td>\n",
              "      <td>500000000</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>Java, C++, Perl[19]</td>\n",
              "      <td>DynamoDB, RDS/Aurora, Redshift[20]</td>\n",
              "      <td>The most used e-commerce site in the world</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>Wikipedia</td>\n",
              "      <td>475000000</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>PHP</td>\n",
              "      <td>MariaDB[21]</td>\n",
              "      <td>A free online encyclopedia based on MediaWiki,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>Fandom</td>\n",
              "      <td>315,000,000[22]</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>PHP</td>\n",
              "      <td>MySQL</td>\n",
              "      <td>Wiki hosting service.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>Twitter</td>\n",
              "      <td>290000000</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>C++, Java,[23] Scala,[24] Ruby</td>\n",
              "      <td>MySQL[25]</td>\n",
              "      <td>Popular social network</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>Bing</td>\n",
              "      <td>285000000</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>C++, C#</td>\n",
              "      <td>Microsoft SQL Server, Cosmos DB</td>\n",
              "      <td>Search engine from Microsoft.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>eBay</td>\n",
              "      <td>285000000</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>Java,[26] JavaScript,[27] Scala[28]</td>\n",
              "      <td>Oracle Database</td>\n",
              "      <td>Online auction house.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>MSN</td>\n",
              "      <td>280000000</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>C#</td>\n",
              "      <td>Microsoft SQL Server</td>\n",
              "      <td>An email client, for simple use. Previously kn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>LinkedIn</td>\n",
              "      <td>260000000</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>Java, JavaScript,[29] Scala</td>\n",
              "      <td>Venice[30][31]</td>\n",
              "      <td>World's largest professional network.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>Pinterest</td>\n",
              "      <td>250000000</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>Python (Django),[32] Erlang, Elixir[33]</td>\n",
              "      <td>MySQL, Redis[34]</td>\n",
              "      <td>Search engine for ideas.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>WordPress.com</td>\n",
              "      <td>240000000</td>\n",
              "      <td>JavaScript</td>\n",
              "      <td>PHP[35]</td>\n",
              "      <td>MariaDB[36]</td>\n",
              "      <td>Website manager software.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11c19fd8-a325-4062-8b9c-39d7816dbe77')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11c19fd8-a325-4062-8b9c-39d7816dbe77 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11c19fd8-a325-4062-8b9c-39d7816dbe77');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "pd.read_csv('programming.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using beautifulSoup with differnt site."
      ],
      "metadata": {
        "id": "iwgx7IJmDl7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Trial 2 Different site\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "DwvSsqf5ruap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBtPN9qCmCsv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08593ec8-3093-45f8-c9e1-372f649c2a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): pt.wikipedia.org:443\n",
            "DEBUG:urllib3.connectionpool:https://pt.wikipedia.org:443 \"GET /wiki/Lista_de_bairros_de_Manaus HTTP/1.1\" 200 22793\n"
          ]
        }
      ],
      "source": [
        "url = \"https://pt.wikipedia.org/wiki/Lista_de_bairros_de_Manaus\"\n",
        "data = requests.get(url).text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating BeautifulSoup object\n",
        "soup = BeautifulSoup(data, 'html.parser')"
      ],
      "metadata": {
        "id": "OoacscEFr1HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Classes of each table:')\n",
        "for table in soup.find_all('table'):\n",
        "    print(table.get('class'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KLZDAf5rCKP",
        "outputId": "74fbefa6-ebcc-4789-f04c-56b453392944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes of each table:\n",
            "['box-Desatualizado', 'plainlinks', 'metadata', 'ambox', 'ambox-content']\n",
            "['wikitable', 'sortable']\n",
            "['nowraplinks', 'collapsible', 'collapsed', 'navbox-inner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating list with all tables\n",
        "tables = soup.find_all('table')\n",
        "\n",
        "#  Looking for the table with the classes 'wikitable' and 'sortable'\n",
        "table = soup.find('table', class_='wikitable sortable')"
      ],
      "metadata": {
        "id": "f7kbLEVur4P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining of the dataframe\n",
        "df = pd.DataFrame(columns=['Neighborhood', 'Zone', 'Area', 'Population', 'Density', 'Homes_count'])\n",
        "\n",
        "# Collecting Ddata\n",
        "for row in table.tbody.find_all('tr'):    \n",
        "    # Find all data for each column\n",
        "    columns = row.find_all('td')\n",
        "    \n",
        "    if(columns != []):\n",
        "        neighborhood = columns[0].text.strip()\n",
        "        zone = columns[1].text.strip()\n",
        "        area = columns[2].span.contents[0].strip('&0.')\n",
        "        population = columns[3].span.contents[0].strip('&0.')\n",
        "        density = columns[4].span.contents[0].strip('&0.')\n",
        "        homes_count = columns[5].span.contents[0].strip('&0.')\n",
        "\n",
        "        df = df.append({'Neighborhood': neighborhood,  'Zone': zone, 'Area': area, 'Population': population, 'Density': density, 'Homes_count': homes_count}, ignore_index=True)"
      ],
      "metadata": {
        "id": "zLCHEbfXsBO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EDcGQn-ksDxU",
        "outputId": "7d976958-7bae-4866-ecfc-e72db16c15f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Neighborhood          Zone    Area Population   Density Homes_count\n",
              "0    Adrianópolis    Centro-Sul  248.45      10459   3560.88        3224\n",
              "1          Aleixo    Centro-Sul  618.34      24417    3340.4        6101\n",
              "2        Alvorada  Centro-Oeste  553.18      76392  11681.73       18193\n",
              "3  Armando Mendes         Leste  307.65      33441   9194.86        7402\n",
              "4         Betânia           Sul   52.51       1294  20845.55        3119"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-479f614a-0188-495f-9d18-6349ec7c68ad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Zone</th>\n",
              "      <th>Area</th>\n",
              "      <th>Population</th>\n",
              "      <th>Density</th>\n",
              "      <th>Homes_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adrianópolis</td>\n",
              "      <td>Centro-Sul</td>\n",
              "      <td>248.45</td>\n",
              "      <td>10459</td>\n",
              "      <td>3560.88</td>\n",
              "      <td>3224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aleixo</td>\n",
              "      <td>Centro-Sul</td>\n",
              "      <td>618.34</td>\n",
              "      <td>24417</td>\n",
              "      <td>3340.4</td>\n",
              "      <td>6101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alvorada</td>\n",
              "      <td>Centro-Oeste</td>\n",
              "      <td>553.18</td>\n",
              "      <td>76392</td>\n",
              "      <td>11681.73</td>\n",
              "      <td>18193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Armando Mendes</td>\n",
              "      <td>Leste</td>\n",
              "      <td>307.65</td>\n",
              "      <td>33441</td>\n",
              "      <td>9194.86</td>\n",
              "      <td>7402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Betânia</td>\n",
              "      <td>Sul</td>\n",
              "      <td>52.51</td>\n",
              "      <td>1294</td>\n",
              "      <td>20845.55</td>\n",
              "      <td>3119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-479f614a-0188-495f-9d18-6349ec7c68ad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-479f614a-0188-495f-9d18-6349ec7c68ad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-479f614a-0188-495f-9d18-6349ec7c68ad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quesiton 6: \n",
        "Explore further the python-pptx library and check how to differentiate between texts coming from different components such as title, subtitle and paragraphs. \n"
      ],
      "metadata": {
        "id": "p5bX7UUesHq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-pptx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s99n63eMsL-J",
        "outputId": "aab9f9bc-76d3-43b2-df2d-c3b2ec2b7d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.7/dist-packages (0.6.21)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from python-pptx) (4.9.1)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx) (7.1.2)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.7/dist-packages (from python-pptx) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing library\n",
        "from pptx import Presentation"
      ],
      "metadata": {
        "id": "Hw82XNbPsnVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting texts slide wise and section wise\n",
        "# open the PPT in parallel to check the outcome\n",
        "prs = Presentation(\"/content/drive/MyDrive/297 NLP/Your big idea.pptx\")\n",
        "counter_slide = 1\n",
        "for slide in prs.slides:\n",
        "    print(\"slide:\", counter_slide, \"\\n\")\n",
        "    counter_content = 1\n",
        "    for shape in slide.shapes:\n",
        "        try: \n",
        "            print(\"content:\", counter_content, shape.text, \"\\n\")\n",
        "            counter_content += 1\n",
        "        except: continue\n",
        "    print(\"\\n\\n\")\n",
        "    counter_slide += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmiAg1TxsqRy",
        "outputId": "1f12f3d8-6183-483d-b7f6-0aab9d7433df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slide: 1 \n",
            "\n",
            "content: 1 Making Presentations That Stick \n",
            "\n",
            "content: 2 A guide by Chip Heath & Dan Heath \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 2 \n",
            "\n",
            "content: 1 Selling your idea \n",
            "\n",
            "content: 2 Created in partnership with Chip and Dan Heath, authors of the bestselling book Made To Stick, this template advises users on how to build and deliver a memorable presentation of a new product, service, or idea. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 3 \n",
            "\n",
            "content: 1 1. Intro \n",
            "\n",
            "content: 2 Choose one approach to grab the audience’s attention right from the start: unexpected, emotional, or simple.\n",
            "Unexpected\u000bHighlight what’s new, unusual, or surprising.\n",
            "Emotional\u000bGive people a reason to care.\n",
            "Simple\u000bProvide a simple unifying message for what is to come \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 4 \n",
            "\n",
            "content: 1 How many languages do \u000byou need to know to communicate with \u000bthe rest of the world? \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 5 \n",
            "\n",
            "content: 1 Just one! Your own.\n",
            "(With a little help from your smart phone) \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 6 \n",
            "\n",
            "content: 1 The Google Translate app can repeat anything you say in up to NINETY LANGUAGES from German and Japanese  to Czech and Zulu \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 7 \n",
            "\n",
            "content: 1 2. Examples \n",
            "\n",
            "content: 2 By the end of this section, your audience should be able to visualize: \n",
            "What\u000bWhat is the pain you cure with your solution?\n",
            "Who\u000bShow them a specific person who would benefit from your solution. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 8 \n",
            "\n",
            "content: 1 Meet Alberto. \n",
            "He recently moved from Spain to a small town in Northern Ireland.\n",
            "He loved soccer, but feared he had no way to talk to a coach or teammates.  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 9 \n",
            "\n",
            "content: 1 Meet Marcos.\n",
            "He recently opened a camera shop near the Louvre in Paris. \n",
            "Visitors to his store, mostly tourists, speak many different languages making anything beyond a simple transaction a challenge. \n",
            "\n",
            "content: 2 Story for illustration purposes only \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 10 \n",
            "\n",
            "content: 1 A translation barrier left Alberto feeling lonely and hurt Marco’s business. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 11 \n",
            "\n",
            "content: 1  \n",
            "\n",
            "content: 2 Then, Marcos discovered Google Translate\n",
            "He has his visiting customers speak their camera issues into the app. \n",
            "He’s able to give them a friendly,  personalized experience by understanding exactly what they need. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 12 \n",
            "\n",
            "content: 1 A simple gesture\n",
            "Coaches Gary and Glen knew no Spanish.  \n",
            "They used Google Translate to invite Alberto to join in... “Do you want to play?”... “Can you defend the left side?” \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 13 \n",
            "\n",
            "content: 1 From outsider to star\n",
            "Alberto scored 30 goals in 21 games.  He is now being scouted by several professional clubs in the Premier League.  And he’s a favorite of the other boys on the team.\n",
            "\n",
            "See a short video on Alberto’s story \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 14 \n",
            "\n",
            "content: 1 3. Examples \n",
            "\n",
            "content: 2 People need to understand how rare or frequent your examples are. \n",
            "Pick 1 or 2 statistics and make them as concrete as possible. Stats are generally not sticky, but here are a few tactics: \n",
            "Relate\u000bDeliver data within the context of a story you’ve already told\n",
            "Compare\u000bMake big numbers digestible by putting them in the context of something familiar \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 15 \n",
            "\n",
            "content: 1 It’s no surprise Marcos uses Google Translate in his shop regularly.\n",
            "There are 23 officially recognized languages in the EU. \n",
            "\n",
            "content: 2 Source: theguardian.com \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 16 \n",
            "\n",
            "content: 1 More than 50 million Americans travelled abroad in 2015\n",
            "\n",
            "THAT’S MORE THAN THE POPULATION OF \n",
            "CALIFORNIA AND \u000bTEXAS COMBINED \n",
            "\n",
            "content: 2 Source: travel.trade.gov \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 17 \n",
            "\n",
            "content: 1 4. Closing \n",
            "\n",
            "content: 2 Build confidence around your product or idea by including at least one of the these slides:\n",
            "Milestones\u000bWhat has been accomplished and what might be left to tackle?\n",
            "Testimonials\u000bWho supports your idea (or doesn’t)?\n",
            "What’s next?\u000bHow can the audience get involved or find out more? \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 18 \n",
            "\n",
            "content: 1 Milestones \n",
            "\n",
            "content: 2 October 2014 \n",
            "\n",
            "content: 3 Translate web pages with Chrome extension\n",
            " \n",
            "\n",
            "content: 4 August 2015 \n",
            "\n",
            "content: 5 Translate conversations through your Android watch \n",
            "\n",
            "content: 6 October 2015 \n",
            "\n",
            "content: 7 Translate text within an app \n",
            "\n",
            "content: 8 November 2015 \n",
            "\n",
            "content: 9 Translate written text from English or German to Arabic with the click of a camera \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 19 \n",
            "\n",
            "content: 1 What people are saying \n",
            "\n",
            "content: 2  \n",
            "\n",
            "content: 3  \n",
            "\n",
            "content: 4  \n",
            "\n",
            "content: 5 Translate has officially inspired me to learn French \n",
            "Abby Author, NYC \n",
            "\n",
            "content: 6 With this app, I’m confident to plan a trip to rural Vietnam\n",
            "Wendy Writer, CA \n",
            "\n",
            "content: 7 Visual translation feels like magic\n",
            "Ronny Reader, NYC \n",
            "\n",
            "content: 8 Quotes for illustration purposes only \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 20 \n",
            "\n",
            "content: 1 Know a 2nd language? \u000bMake Google Translate  even better by joining \u000bthe community. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 21 \n",
            "\n",
            "content: 1 Good luck! \n",
            "\n",
            "content: 2 We hope you’ll use these tips to go out and deliver a memorable pitch for your product \u000bor service!\n",
            "For more (free) presentation tips relevant to other types of messages, go to\u000bheathbrothers.com/presentations \n",
            "\n",
            "content: 3 For more about making your ideas stick with others, check out our book! \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slide: 22 \n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check subtitles\n",
        "counter = 1\n",
        "for slide in prs.slides:\n",
        "    try:\n",
        "        title = slide.placeholders.text\n",
        "        print(\"Slide : \",counter)\n",
        "        print(title)\n",
        "    except:\n",
        "        print(\"Slide : \",counter)\n",
        "        print(\"No specific subtitles available on this slid\")\n",
        "    counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALHUpDCdswcp",
        "outputId": "c35db97e-bbdf-4e27-b8bf-7527ef16bbbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slide :  1\n",
            "No specific subtitles available on this slid\n",
            "Slide :  2\n",
            "No specific subtitles available on this slid\n",
            "Slide :  3\n",
            "No specific subtitles available on this slid\n",
            "Slide :  4\n",
            "No specific subtitles available on this slid\n",
            "Slide :  5\n",
            "No specific subtitles available on this slid\n",
            "Slide :  6\n",
            "No specific subtitles available on this slid\n",
            "Slide :  7\n",
            "No specific subtitles available on this slid\n",
            "Slide :  8\n",
            "No specific subtitles available on this slid\n",
            "Slide :  9\n",
            "No specific subtitles available on this slid\n",
            "Slide :  10\n",
            "No specific subtitles available on this slid\n",
            "Slide :  11\n",
            "No specific subtitles available on this slid\n",
            "Slide :  12\n",
            "No specific subtitles available on this slid\n",
            "Slide :  13\n",
            "No specific subtitles available on this slid\n",
            "Slide :  14\n",
            "No specific subtitles available on this slid\n",
            "Slide :  15\n",
            "No specific subtitles available on this slid\n",
            "Slide :  16\n",
            "No specific subtitles available on this slid\n",
            "Slide :  17\n",
            "No specific subtitles available on this slid\n",
            "Slide :  18\n",
            "No specific subtitles available on this slid\n",
            "Slide :  19\n",
            "No specific subtitles available on this slid\n",
            "Slide :  20\n",
            "No specific subtitles available on this slid\n",
            "Slide :  21\n",
            "No specific subtitles available on this slid\n",
            "Slide :  22\n",
            "No specific subtitles available on this slid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check titles.\n",
        "counter = 1\n",
        "for slide in prs.slides:\n",
        "    # counter = 1\n",
        "    try:\n",
        "        title = slide.shapes.title.text\n",
        "        print(\"Slide : \", counter)\n",
        "        print(title)\n",
        "    except:\n",
        "        print(\"Slide : \", counter)\n",
        "        print(\"No title available on this slide\")\n",
        "    counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nsYn793xpi_",
        "outputId": "d49410c6-606f-4da3-cab6-92c3b4bc0c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slide :  1\n",
            "Making Presentations That Stick\n",
            "Slide :  2\n",
            "No title available on this slide\n",
            "Slide :  3\n",
            "No title available on this slide\n",
            "Slide :  4\n",
            "How many languages do \u000byou need to know to communicate with \u000bthe rest of the world?\n",
            "Slide :  5\n",
            "Just one! Your own.\n",
            "(With a little help from your smart phone)\n",
            "Slide :  6\n",
            "The Google Translate app can repeat anything you say in up to NINETY LANGUAGES from German and Japanese  to Czech and Zulu\n",
            "Slide :  7\n",
            "No title available on this slide\n",
            "Slide :  8\n",
            "No title available on this slide\n",
            "Slide :  9\n",
            "No title available on this slide\n",
            "Slide :  10\n",
            "A translation barrier left Alberto feeling lonely and hurt Marco’s business.\n",
            "Slide :  11\n",
            "No title available on this slide\n",
            "Slide :  12\n",
            "No title available on this slide\n",
            "Slide :  13\n",
            "From outsider to star\n",
            "Alberto scored 30 goals in 21 games.  He is now being scouted by several professional clubs in the Premier League.  And he’s a favorite of the other boys on the team.\n",
            "\n",
            "See a short video on Alberto’s story\n",
            "Slide :  14\n",
            "No title available on this slide\n",
            "Slide :  15\n",
            "It’s no surprise Marcos uses Google Translate in his shop regularly.\n",
            "There are 23 officially recognized languages in the EU.\n",
            "Slide :  16\n",
            "More than 50 million Americans travelled abroad in 2015\n",
            "\n",
            "THAT’S MORE THAN THE POPULATION OF \n",
            "CALIFORNIA AND \u000bTEXAS COMBINED\n",
            "Slide :  17\n",
            "No title available on this slide\n",
            "Slide :  18\n",
            "Milestones\n",
            "Slide :  19\n",
            "What people are saying\n",
            "Slide :  20\n",
            "Know a 2nd language? \u000bMake Google Translate  even better by joining \u000bthe community.\n",
            "Slide :  21\n",
            "No title available on this slide\n",
            "Slide :  22\n",
            "No title available on this slide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7\n",
        "\n",
        "Extract table from a PPT using the same library.\n",
        "\n",
        "Note : Table is manually added by me. Without that answer was [2014, 2015] only."
      ],
      "metadata": {
        "id": "r2ceEMdq3QSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the table and give your path to new updated ppt\n",
        "\n",
        "prs = Presentation(\"/content/drive/MyDrive/297 NLP/Your big idea.pptx\")\n",
        "# text_runs will be populated with a list of strings,\n",
        "# one for each text run in presentation\n",
        "text_runs = []\n",
        "for slide in prs.slides:\n",
        "    for shape in slide.shapes:\n",
        "        if not shape.has_table:\n",
        "            continue    \n",
        "        tbl = shape.table\n",
        "        row_count = len(tbl.rows)\n",
        "        col_count = len(tbl.columns)\n",
        "        for r in range(0, row_count):\n",
        "            for c in range(0, col_count):\n",
        "                cell = tbl.cell(r,c)\n",
        "                paragraphs = cell.text_frame.paragraphs \n",
        "                for paragraph in paragraphs:\n",
        "                    for run in paragraph.runs:\n",
        "                        text_runs.append(run.text)\n",
        "\n",
        "print(text_runs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R08pg2YQ5nqY",
        "outputId": "a7df0412-926e-4487-cf0a-342e07dc64af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2014', '2015', 'Priyank', 'Thakkar', 'Will ', 'Get', 'The ', 'Tesla', 'This', 'Year', 'For ', 'sure', 'Thank', 'You.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8:\n",
        "Research and find some more libraries to extract text from PDFs and show basic implementation of any one of them."
      ],
      "metadata": {
        "id": "eeA7HZ606Q-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing the library\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXVzU1Uy6Usd",
        "outputId": "c7413d7a-e155-4657-8d3e-908a82416263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.7/dist-packages (2.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from PyPDF2) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the library\n",
        "import PyPDF2"
      ],
      "metadata": {
        "id": "UUR1YU5Q8T4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the file\n",
        "pdfFileObj = open('/content/drive/MyDrive/297 NLP/Evaluation_of_Sentiment_Analysis_in_Finance_From_Lexicons_to_Transformers.pdf', 'rb')"
      ],
      "metadata": {
        "id": "583elFSk8WcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# passing the file to PyPDF\n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)"
      ],
      "metadata": {
        "id": "feaFMih-8fXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the number of pages\n",
        "print(pdfReader.numPages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1UQ20xa8g8X",
        "outputId": "060b5522-413b-45d4-af8f-3ee37f432707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the first page\n",
        "pageObj = pdfReader.getPage(1)"
      ],
      "metadata": {
        "id": "pmhUvNia8kOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting text from the first page\n",
        "print(pageObj.extractText())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si2xXLxp8mHV",
        "outputId": "aa3e0084-66a1-4204-9309-dd840c65eb4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "decisions. The sentiments expressed in news and tweets in\u001du-\n",
            "ence stock prices and brand reputation, hence, constant mea-\n",
            "surement and tracking of these sentiments is becoming one of\n",
            "the most important activities for investors. Studies have used\n",
            "sentiment analysis based on \u001cnancial news to forecast stock\n",
            "prices [6]\u0015[8], foreign exchange and global \u001cnancial market\n",
            "trends [9], [10] as well as to predict corporate earnings [11].\n",
            "Given that the \u001cnancial sector uses its own jargon, it is\n",
            "not suitable to apply generic sentiment analysis in \u001cnance\n",
            "because many of the words differ from their general meaning.\n",
            "For example, ``liability'' is generally a negative word, but\n",
            "in the \u001cnancial domain it has a neutral meaning. The term\n",
            "``share'' usually has a positive meaning, but in the \u001cnancial\n",
            "domain, share represents a \u001cnancial asset or a stock, which\n",
            "is a neutral word. Furthermore, ``bull'' is neutral in general,\n",
            "but in \u001cnance, it is strictly positive, while ``bear'' is neutral in\n",
            "general, but negative in \u001cnance. These examples emphasize\n",
            "the need for development of dedicated models, which will\n",
            "extract sentiments from \u001cnancial texts.\n",
            "Sentiment analysis in \u001cnance has become an important\n",
            "research topic, connecting quantitative and qualitative mea-\n",
            "sures of \u001cnancial performance. A seminal study by Loughran\n",
            "and McDonald [12] shows that word lists developed for\n",
            "other disciplines misclassify common words in \u001cnancial\n",
            "texts. Hence, Loughran and McDonald created an expert\n",
            "annotated lexicon of positive, negative, and neutral words\n",
            "in \u001cnance, which better re\u001dect sentiments in \u001cnancial texts.\n",
            "In [13], the authors introduce a Twitter-speci\u001cc lexicon,\n",
            "which, in combination with the DAN2 machine learning\n",
            "approach, produces more accurate sentiment classi\u001ccation\n",
            "results than support vector machine (SVM) approach while\n",
            "using the same Twitter-speci\u001cc lexicon.\n",
            "Machine learning methods for sentiment extraction have\n",
            "been applied on datasets of tweets or news [14]\u0015[18]. In [15],\n",
            "the authors use various machine-learning binary classi\u001cers\n",
            "to obtain StockTwits tweets sentiments. They show that the\n",
            "SVM classi\u001cer is more accurate compared to Decision Trees\n",
            "and Naïve Bayes classi\u001cer. In [16], Atzeni et al. test the\n",
            "performance of various regression models in combination\n",
            "with statistical and semantic methods for feature extraction\n",
            "to predict a real-valued sentiment score in micro-blogs and\n",
            "news headlines, and show that semantic methods improve\n",
            "classi\u001ccation accuracy.\n",
            "Researchers have used lexicon-based approaches in com-\n",
            "bination with machine-learning models. The authors in [18]\n",
            "show that such combinations are more ef\u001ccient for senti-\n",
            "ment extraction than using single models. However, regular\n",
            "machine-learning methods are unable to extract complex fea-\n",
            "tures and to keep the order of words in a sentence. These tasks\n",
            "require the use of deep-learning approaches, which allow for\n",
            "complex feature extraction, location identi\u001ccation, and order\n",
            "information [19].\n",
            "Deep-learning methods [20] use a cascade of multiple lay-\n",
            "ers of non-linear processing units for complex feature extrac-\n",
            "tion and transformation. Each successive layer uses the output\n",
            "from the previous layer as input, thus extracting complexfeatures which in many cases can be useful for generating\n",
            "learning patterns and relationships beyond immediate neigh-\n",
            "bors in the sequence. Many studies con\u001crm the ef\u001cciency\n",
            "of deep-learning models, including recurrent neural network\n",
            "(RNN) [21], [22], convolutional neural networks [23]\u0015[25]\n",
            "and attention mechanism [19], [26] in sentiment extraction\n",
            "in \u001cnance. The great success of deep-learning approaches\n",
            "in NLP is mainly due to the introduction and improvement\n",
            "of text representation methods, such as word [27]\u0015[29] and\n",
            "sentence encoders [30]\u0015[33]. These convert words/sentences\n",
            "into vector representation, making them suitable as input for\n",
            "neural networks. These representations keep the semantic\n",
            "information coded into words and sentences, which is crucial\n",
            "for sentiment extraction.\n",
            "Recent developments in NLP, deep-learning, and transfer-\n",
            "learning have signi\u001ccantly improved the sentiment extrac-\n",
            "tion from \u001cnancial news and texts [17], [34]\u0015[37]. In [35],\n",
            "Yang et al. incorporate inductive transfer-learning meth-\n",
            "ods such as ULMFiT [38] for sentiment analysis in\n",
            "\u001cnance, and the results show improvements in senti-\n",
            "ment classi\u001ccation compared to traditional transfer-learning\n",
            "approaches. The superior performance of recent NLP\n",
            "transformers, BERT and RoBERTA, in sentiment analy-\n",
            "sis is evaluated in [37], where the effectiveness of using\n",
            "the RoBERTa model is compared to dictionary-based\n",
            "models.\n",
            "Studies have used sentiment analysis based on \u001cnancial\n",
            "news to forecast stock prices [6]\u0015[8], foreign exchange and\n",
            "global \u001cnancial market trends [9], [10] as well as to predict\n",
            "corporate earnings [11].\n",
            "This paper aims to survey approaches to sentiment\n",
            "analysis, including combinations of machine-learning and\n",
            "deep-learning models with lexicon-based feature extrac-\n",
            "tion methods and word and sentence encoders, up to the\n",
            "most recent NLP transformers. The goal is to apply these\n",
            "approaches to \u001cnance. We evaluate and compare model effec-\n",
            "tiveness when trained under same conditions and on the same\n",
            "dataset. The main contribution of this paper is the develop-\n",
            "ment of an evaluation platform, which we use to assess the\n",
            "performance of NLP methodologies for text feature extrac-\n",
            "tion in \u001cnance.\n",
            "We show that recent advances in deep-learning and\n",
            "transfer-learning methods in NLP increase the accuracy of\n",
            "sentiment analysis based on \u001cnancial headlines. Moreover,\n",
            "our results indicate that lexicon-based approaches can be\n",
            "ef\u001cciently replaced by modern NLP transformers.\n",
            "The rest of the paper is organized as follows. Section II\n",
            "provides an overview of NLP methods for text representation:\n",
            "lexicon-based and statistical, as well as word and sentence\n",
            "encoders. Section IIIpresents NLP transformers, their archi-\n",
            "tectures and objectives, as a separate group of deep-learning\n",
            "models for text classi\u001ccation, which we evaluate in extrac-\n",
            "tion of \u001cnance text sentiments. Section IVdescribes the\n",
            "dataset that we created to evaluate text representation meth-\n",
            "ods. Section Vpresents the evaluation platform that we build\n",
            "for measuring model performances. Section VIreports the\n",
            "VOLUME 8, 2020 131663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using differnt library : PDFminer"
      ],
      "metadata": {
        "id": "dn2p2jUH8nmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMXbCJYV8r8y",
        "outputId": "30fcd3f3-9e08-4c48-d15c-cca7781701ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (2.1.1)\n",
            "Collecting cryptography>=36.0.0\n",
            "  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 37.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-38.0.1 pdfminer.six-20220524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "\n",
        "output_string = StringIO()\n",
        "with open('/content/drive/MyDrive/297 NLP/Evaluation_of_Sentiment_Analysis_in_Finance_From_Lexicons_to_Transformers.pdf', 'rb') as in_file:\n",
        "    parser = PDFParser(in_file)\n",
        "    doc = PDFDocument(parser)\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    for page in PDFPage.create_pages(doc):\n",
        "        interpreter.process_page(page)\n",
        "\n",
        "print(output_string.getvalue())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hwlVzW481Hv",
        "outputId": "f1426edf-8830-45df-eded-00dcc69924d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received June 13, 2020, accepted July 1, 2020, date of publication July 16, 2020, date of current version July 29, 2020.\n",
            "\n",
            "Digital Object Identifier 10.1109/ACCESS.2020.3009626\n",
            "\n",
            "Evaluation of Sentiment Analysis in Finance:\n",
            "From Lexicons to Transformers\n",
            "\n",
            "KOSTADIN MISHEV 1, ANA GJORGJEVIKJ 1, IRENA VODENSKA2, LUBOMIR T. CHITKUSHEV2,\n",
            "AND DIMITAR TRAJANOV 1, (Member, IEEE)\n",
            "1Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, 1000 Skopje, North Macedonia\n",
            "2Financial Informatics Lab, Metropolitan College, Boston University, Boston, MA 02215, USA\n",
            "\n",
            "Corresponding author: Kostadin Mishev (kostadin.mishev@ﬁnki.ukim.mk)\n",
            "\n",
            "This work was supported in part by the Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, Skopje.\n",
            "\n",
            "ABSTRACT Financial and economic news is continuously monitored by ﬁnancial market participants.\n",
            "According to the efﬁcient market hypothesis, all past information is reﬂected in stock prices and new infor-\n",
            "mation is instantaneously absorbed in determining future stock prices. Hence, prompt extraction of positive\n",
            "or negative sentiments from news is very important for investment decision-making by traders, portfolio\n",
            "managers and investors. Sentiment analysis models can provide an efﬁcient method for extracting actionable\n",
            "signals from the news. However, ﬁnancial sentiment analysis is challenging due to domain-speciﬁc language\n",
            "and unavailability of large labeled datasets. General sentiment analysis models are ineffective when applied\n",
            "to speciﬁc domains such as ﬁnance. To overcome these challenges, we design an evaluation platform\n",
            "which we use to assess the effectiveness and performance of various sentiment analysis approaches, based\n",
            "on combinations of text representation methods and machine-learning classiﬁers. We perform more than\n",
            "one hundred experiments using publicly available datasets, labeled by ﬁnancial experts. We start the\n",
            "evaluation with speciﬁc lexicons for sentiment analysis in ﬁnance and gradually build the study to include\n",
            "word and sentence encoders, up to the latest available NLP transformers. The results show improved\n",
            "efﬁciency of contextual embeddings in sentiment analysis compared to lexicons and ﬁxed word and sentence\n",
            "encoders, even when large datasets are not available. Furthermore, distilled versions of NLP transformers\n",
            "produce comparable results to their larger teacher models, which makes them suitable for use in production\n",
            "environments.\n",
            "\n",
            "INDEX TERMS Sentiment analysis, ﬁnance, natural language processing, text representations, deep-\n",
            "learning, encoders, word embedding, sentence embedding, transfer-learning, transformers, survey.\n",
            "\n",
            "I. INTRODUCTION\n",
            "The latest advances in Natural Language Processing (NLP)\n",
            "have received signiﬁcant attention due to their efﬁciency\n",
            "in language modeling. These language models are ﬁnding\n",
            "applications in various industries as they provide powerful\n",
            "mechanisms for real-time, reliable, and semantic-oriented\n",
            "text analysis. Sentiment analysis is one of the NLP tasks that\n",
            "leverages language modeling advancements and is achiev-\n",
            "ing improved results. According to the Oxford University\n",
            "Press dictionary,1 sentiment analysis is deﬁned as the pro-\n",
            "cess of computationally identifying and categorizing opin-\n",
            "ions expressed in a text, primarily to determine whether\n",
            "the writer’s attitude towards a particular topic or product is\n",
            "\n",
            "1https://lexico.com\n",
            "The associate editor coordinating the review of this manuscript and\n",
            "\n",
            "approving it for publication was K. C. Santosh\n",
            "\n",
            ".\n",
            "\n",
            "positive, negative, or neutral. Sentiment analysis is becoming\n",
            "an essential tool for transforming emotions and attitudes into\n",
            "actionable information.\n",
            "\n",
            "Designing and building deep-learning-based sentiment\n",
            "analysis models require substantial datasets for training and\n",
            "testing. While there are several large, publicly available\n",
            "sentiment-annotated datasets, they are mostly related to prod-\n",
            "ucts and movies. Many sentiment analysis models [1]–[4]\n",
            "use these datasets and achieve good performance in related\n",
            "domains. However, the application of these models in differ-\n",
            "ent domains is challenging because each domain has a unique\n",
            "set of words for emotion expression.\n",
            "\n",
            "The ﬁnancial domain is characterized by a unique vocab-\n",
            "ulary, which calls for domain-speciﬁc sentiment analysis.\n",
            "Prices observed in ﬁnancial markets reﬂect all available\n",
            "information related to traded assets [5], hence new informa-\n",
            "tion allows stakeholders to make well-informed and timely\n",
            "\n",
            "131662\n",
            "\n",
            "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "decisions. The sentiments expressed in news and tweets inﬂu-\n",
            "ence stock prices and brand reputation, hence, constant mea-\n",
            "surement and tracking of these sentiments is becoming one of\n",
            "the most important activities for investors. Studies have used\n",
            "sentiment analysis based on ﬁnancial news to forecast stock\n",
            "prices [6]–[8], foreign exchange and global ﬁnancial market\n",
            "trends [9], [10] as well as to predict corporate earnings [11].\n",
            "Given that the ﬁnancial sector uses its own jargon, it is\n",
            "not suitable to apply generic sentiment analysis in ﬁnance\n",
            "because many of the words differ from their general meaning.\n",
            "For example, ‘‘liability’’ is generally a negative word, but\n",
            "in the ﬁnancial domain it has a neutral meaning. The term\n",
            "‘‘share’’ usually has a positive meaning, but in the ﬁnancial\n",
            "domain, share represents a ﬁnancial asset or a stock, which\n",
            "is a neutral word. Furthermore, ‘‘bull’’ is neutral in general,\n",
            "but in ﬁnance, it is strictly positive, while ‘‘bear’’ is neutral in\n",
            "general, but negative in ﬁnance. These examples emphasize\n",
            "the need for development of dedicated models, which will\n",
            "extract sentiments from ﬁnancial texts.\n",
            "\n",
            "Sentiment analysis in ﬁnance has become an important\n",
            "research topic, connecting quantitative and qualitative mea-\n",
            "sures of ﬁnancial performance. A seminal study by Loughran\n",
            "and McDonald [12] shows that word lists developed for\n",
            "other disciplines misclassify common words in ﬁnancial\n",
            "texts. Hence, Loughran and McDonald created an expert\n",
            "annotated lexicon of positive, negative, and neutral words\n",
            "in ﬁnance, which better reﬂect sentiments in ﬁnancial texts.\n",
            "In [13], the authors introduce a Twitter-speciﬁc lexicon,\n",
            "which, in combination with the DAN2 machine learning\n",
            "approach, produces more accurate sentiment classiﬁcation\n",
            "results than support vector machine (SVM) approach while\n",
            "using the same Twitter-speciﬁc lexicon.\n",
            "\n",
            "Machine learning methods for sentiment extraction have\n",
            "been applied on datasets of tweets or news [14]–[18]. In [15],\n",
            "the authors use various machine-learning binary classiﬁers\n",
            "to obtain StockTwits tweets sentiments. They show that the\n",
            "SVM classiﬁer is more accurate compared to Decision Trees\n",
            "and Naïve Bayes classiﬁer. In [16], Atzeni et al. test the\n",
            "performance of various regression models in combination\n",
            "with statistical and semantic methods for feature extraction\n",
            "to predict a real-valued sentiment score in micro-blogs and\n",
            "news headlines, and show that semantic methods improve\n",
            "classiﬁcation accuracy.\n",
            "\n",
            "Researchers have used lexicon-based approaches in com-\n",
            "bination with machine-learning models. The authors in [18]\n",
            "show that such combinations are more efﬁcient for senti-\n",
            "ment extraction than using single models. However, regular\n",
            "machine-learning methods are unable to extract complex fea-\n",
            "tures and to keep the order of words in a sentence. These tasks\n",
            "require the use of deep-learning approaches, which allow for\n",
            "complex feature extraction, location identiﬁcation, and order\n",
            "information [19].\n",
            "\n",
            "Deep-learning methods [20] use a cascade of multiple lay-\n",
            "ers of non-linear processing units for complex feature extrac-\n",
            "tion and transformation. Each successive layer uses the output\n",
            "from the previous layer as input, thus extracting complex\n",
            "\n",
            "features which in many cases can be useful for generating\n",
            "learning patterns and relationships beyond immediate neigh-\n",
            "bors in the sequence. Many studies conﬁrm the efﬁciency\n",
            "of deep-learning models, including recurrent neural network\n",
            "(RNN) [21], [22], convolutional neural networks [23]–[25]\n",
            "and attention mechanism [19], [26] in sentiment extraction\n",
            "in ﬁnance. The great success of deep-learning approaches\n",
            "in NLP is mainly due to the introduction and improvement\n",
            "of text representation methods, such as word [27]–[29] and\n",
            "sentence encoders [30]–[33]. These convert words/sentences\n",
            "into vector representation, making them suitable as input for\n",
            "neural networks. These representations keep the semantic\n",
            "information coded into words and sentences, which is crucial\n",
            "for sentiment extraction.\n",
            "\n",
            "Recent developments in NLP, deep-learning, and transfer-\n",
            "learning have signiﬁcantly improved the sentiment extrac-\n",
            "tion from ﬁnancial news and texts [17], [34]–[37]. In [35],\n",
            "Yang et al. incorporate inductive transfer-learning meth-\n",
            "ods such as ULMFiT [38]\n",
            "for sentiment analysis in\n",
            "ﬁnance, and the results show improvements in senti-\n",
            "ment classiﬁcation compared to traditional transfer-learning\n",
            "recent NLP\n",
            "approaches. The superior performance of\n",
            "transformers, BERT and RoBERTA, in sentiment analy-\n",
            "sis is evaluated in [37], where the effectiveness of using\n",
            "the RoBERTa model\n",
            "is compared to dictionary-based\n",
            "models.\n",
            "\n",
            "Studies have used sentiment analysis based on ﬁnancial\n",
            "news to forecast stock prices [6]–[8], foreign exchange and\n",
            "global ﬁnancial market trends [9], [10] as well as to predict\n",
            "corporate earnings [11].\n",
            "\n",
            "This paper aims to survey approaches to sentiment\n",
            "analysis, including combinations of machine-learning and\n",
            "deep-learning models with lexicon-based feature extrac-\n",
            "tion methods and word and sentence encoders, up to the\n",
            "most recent NLP transformers. The goal is to apply these\n",
            "approaches to ﬁnance. We evaluate and compare model effec-\n",
            "tiveness when trained under same conditions and on the same\n",
            "dataset. The main contribution of this paper is the develop-\n",
            "ment of an evaluation platform, which we use to assess the\n",
            "performance of NLP methodologies for text feature extrac-\n",
            "tion in ﬁnance.\n",
            "\n",
            "We show that recent advances in deep-learning and\n",
            "transfer-learning methods in NLP increase the accuracy of\n",
            "sentiment analysis based on ﬁnancial headlines. Moreover,\n",
            "our results indicate that lexicon-based approaches can be\n",
            "efﬁciently replaced by modern NLP transformers.\n",
            "\n",
            "The rest of the paper is organized as follows. Section II\n",
            "provides an overview of NLP methods for text representation:\n",
            "lexicon-based and statistical, as well as word and sentence\n",
            "encoders. Section III presents NLP transformers, their archi-\n",
            "tectures and objectives, as a separate group of deep-learning\n",
            "models for text classiﬁcation, which we evaluate in extrac-\n",
            "tion of ﬁnance text sentiments. Section IV describes the\n",
            "dataset that we created to evaluate text representation meth-\n",
            "ods. Section V presents the evaluation platform that we build\n",
            "for measuring model performances. Section VI reports the\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "131663\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "results, and section VII concludes the paper and considers\n",
            "future applications.\n",
            "\n",
            "II. TEXT REPRESENTATION METHODS\n",
            "A. LEXICON-BASED KNOWLEDGE EXTRACTION\n",
            "Lexicon-based sentiment analysis methods rely on domain-\n",
            "speciﬁc knowledge represented as a lexicon or dictionary.\n",
            "The process of sentiment calculation is based on identifying\n",
            "and keeping words that hold useful information while remov-\n",
            "ing words that are not related to sentiments in ﬁnance.\n",
            "\n",
            "Commonly used lexicons and dictionaries in ﬁnance are\n",
            "General Inquirer (GI), Harvard IV-4 (HIV4) [39], Diction\n",
            "[40], [41], and Loughran and McDonald’s (LM) [12] word\n",
            "lists.\n",
            "\n",
            "To infer\n",
            "\n",
            "the sentiment, we evaluate the Loughran-\n",
            "McDonald lexicon (a ﬁnancial lexical rule-based tool) and the\n",
            "general-purpose Harvard IV-4 dictionary (general sentiment\n",
            "dictionary). We calculate the sentiment polarity using the\n",
            "Lydia system [42]. Each of the words in the sentences is\n",
            "categorized into either a positive or a negative group based\n",
            "on its sentiment in the lexicon (Eq. 1). If polarity>0, then the\n",
            "sentence is classiﬁed as positive, and if polarity<0, then the\n",
            "sentence is classiﬁed as negative.\n",
            "\n",
            "Polarity =\n",
            "\n",
            "Pos − Neg\n",
            "Pos + Neg\n",
            "\n",
            "(1)\n",
            "\n",
            "When using machine-learning (ML) and deep-learning\n",
            "(DL) classiﬁers, we extract the headline features by replacing\n",
            "the words in the sentence with the sentiment value, speci-\n",
            "ﬁed in the dictionary. Next, we input the newly generated\n",
            "sequence into the neural network to classify the text. The\n",
            "DL’s output soft-max layer calculates the probability that the\n",
            "sequence belongs to either the positive or negative sentiment\n",
            "labels.\n",
            "\n",
            "B. STATISTICAL METHODS\n",
            "1) COUNT VECTORS\n",
            "Count Vectorizer (CV) is a simple statistical approach to\n",
            "text representation which converts a collection of text doc-\n",
            "uments into a matrix of token counts, thus reducing the entire\n",
            "sentence into a single vector. The positions in the vector\n",
            "represent the number of appearances of each word in the\n",
            "sentence. The CV algorithm performs feature extraction by\n",
            "using a vocabulary of words (tokens) which can be built\n",
            "from the same text corpus, or input manually (a-priori) from\n",
            "an external resource. The vocabulary limits the number of\n",
            "features which can be extracted from the text.\n",
            "\n",
            "The CV approach for text representation has some draw-\n",
            "backs. First, the ordering information gets lost due to the\n",
            "methodology for term ‘‘squeezing.’’ Second, the contextual\n",
            "information of the sentence is hidden, although it is crucial\n",
            "for sentiment extraction. These issues can be partially solved\n",
            "by using n-gram vectorizers where two, three or more consec-\n",
            "utive words are put together in order to form tokens. Another\n",
            "issue with CV is that it shadows the important words that hold\n",
            "decision-making features for classiﬁers, because it pays more\n",
            "\n",
            "attention to general, frequent words such as ‘‘like,’’ ‘‘but,’’\n",
            "and, ‘‘or,’’ which do not add meaningful information. As a\n",
            "result, important text features may vanish, which calls for\n",
            "more sophisticated algorithms.\n",
            "\n",
            "2) TF-IDF TERM WEIGHTING\n",
            "TF-IDF (Term frequency - inverse document frequency) is\n",
            "an algorithm for statistical measurement, which evaluates\n",
            "the relevance of a word in a document within a corpus\n",
            "of documents. It addresses the feature-vanishing issue of\n",
            "CV algorithms by re-weighting the count frequencies of the\n",
            "words (tokens) in the sentence according to the number of\n",
            "appearances of each token. The algorithm works by multi-\n",
            "plying two metrics: term frequency (TF), which calculates\n",
            "the number of occurrences of a term in the sequence (Eq.3),\n",
            "and inverse document frequency (IDF), which penalizes the\n",
            "feature count of the term if it appears in more sentences within\n",
            "the corpus (Eq.4),\n",
            "\n",
            "tﬁdf (t, d, D) = tf (t, d) · idf (t, D)\n",
            "tf (t, d) = log(1 + freq(t, d))\n",
            "\n",
            "idf (t, D) = log(\n",
            "\n",
            "N\n",
            "count(d ∈ D : t ∈ d)\n",
            "\n",
            ")\n",
            "\n",
            "(2)\n",
            "(3)\n",
            "\n",
            "(4)\n",
            "\n",
            "where t denotes the term, d denotes the document, D denotes\n",
            "the corpus of documents and N is the total number of\n",
            "documents.\n",
            "\n",
            "In this study, we assess the feature extraction perfor-\n",
            "mance of the uni-gram and 2-gram count vectorizers as\n",
            "well as the TF-IDF term weighting in combination with\n",
            "machine-learning classiﬁers and deep-neural networks.\n",
            "\n",
            "C. WORD ENCODERS\n",
            "Statistical features do not provide semantics of the contex-\n",
            "tually close words, which means that words with similar\n",
            "meaning will not have similar codes. Many NLP tasks such\n",
            "as sentiment analysis, question-answering and text generation\n",
            "require detailed semantic knowledge that is not provided by\n",
            "CV and TF-IDF. To overcome these challenges, researchers\n",
            "have introduced word encoders [43] to convert discrete words\n",
            "into high-dimensional vectors composed of real numbers,\n",
            "using a procedure called word embedding. Word encoders\n",
            "help with understanding the context of the sentences, which\n",
            "improves the extracted features. These models are based on\n",
            "the principle of distributional hypothesis [44], in which the\n",
            "meaning of words is evidenced by the context. This approach\n",
            "establishes a new area of research in NLP called distribu-\n",
            "tional semantics, which is the core of many contemporary\n",
            "NLP techniques, including word encoders. These methods\n",
            "are called distributional semantic models (DSM), also known\n",
            "in the literature as vector space or semantic space models of\n",
            "meaning [45]–[47].\n",
            "\n",
            "The word encoders classify the words that appear in the\n",
            "same context as semantically similar to one another, hence\n",
            "assigning similar vectors to them. This retained semantic\n",
            "information is very useful for classiﬁers or neural networks.\n",
            "\n",
            "131664\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "methods for distributed word representations: global matrix\n",
            "factorization and Skip-grams are used to extract better fea-\n",
            "tures by examining the relationships between words. The\n",
            "global matrix factorization method can capture the overall\n",
            "statistics and relationships between words. On the other hand,\n",
            "Word2Vec Skip-gram’s method is efﬁcient in extracting the\n",
            "local context and capturing the word analogy. Both meth-\n",
            "ods are successfully incorporated into the GloVe encoder,\n",
            "thus outperforming Word2Vec in many NLP tasks. GloVe\n",
            "is widely used as a word encoder for NLP-based sentiment\n",
            "analysis [49]–[51].\n",
            "\n",
            "3) FastText\n",
            "In 2016, the Facebook research laboratory introduced a\n",
            "novel method for word encoding called FastText, which\n",
            "tackles the generalization problem of unknown words [29],\n",
            "[48]. FastText differs from previous models in its ability\n",
            "to build word embeddings at a deeper level by harnessing\n",
            "sub-words and characters. In this method, words become a\n",
            "context and word embedding is calculated based on com-\n",
            "binations of lower-level embeddings. Each word is repre-\n",
            "sented as a bag of character n-grams. For example, the word\n",
            "‘‘ﬁnance,’’ given n = 3, will be represented by the fol-\n",
            "lowing character n-grams: < ﬁ, ﬁn, ina, nan, anc, nce, ce >.\n",
            "The main algorithm behind FastText is Word2Vec. Learning\n",
            "the sub-word information enables training of embeddings\n",
            "on smaller datasets and generalization to unknown words.\n",
            "FastText shows improved results in text classiﬁcation [52],\n",
            "even in structurally rich languages such as Turkish [53] and\n",
            "Arabic [54], which require morphological analysis instead of\n",
            "assigning a distinct vector to each word.\n",
            "\n",
            "We evaluate pre-trained FastText vectors in order to assess\n",
            "their performance on ﬁnancial\n",
            "texts. We use the wiki-\n",
            "news-300d-1M pre-trained model, which wraps 1 million\n",
            "word vectors trained on Wikipedia’s 2017 corpus and the\n",
            "statmt.org2 news dataset, where each embedding consists\n",
            "of 300 dimensions.\n",
            "\n",
            "4) ELMo\n",
            "In 2018, a team of researchers at Allen Institute for Artiﬁ-\n",
            "cial Intelligence developed an advanced word encoder called\n",
            "ELMo (Embeddings from Language Models) [55], whose\n",
            "word embeddings are learned from a deep bidirectional lan-\n",
            "guage model (biLM), pre-trained on large corpora of textual\n",
            "data. The essential feature, which makes ELMo different\n",
            "from previous word encoders, is that it produces contextual\n",
            "word embeddings considering the whole context in which\n",
            "the word is used. Hence, we can obtain different embed-\n",
            "ding for the same word in a different context, a major\n",
            "improvement from previous encoders, which always pro-\n",
            "duce a static embedding. To tackle out-of-vocabulary (OOV)\n",
            "tokens, ELMo uses character-derived embedding, leveraging\n",
            "the morphological clues of words, thus improving the quality\n",
            "of word representations.\n",
            "\n",
            "2http://statmt.org/\n",
            "\n",
            "FIGURE 1. Word2Vec CBOW and Skip-Grams architectures [43].\n",
            "\n",
            "In this section, we provide an overview of the most popular\n",
            "word encoders: Word2Vec [43], GloVe [28] and FastText\n",
            "[29], [48], which exemplify different approaches in modeling\n",
            "word embeddings.\n",
            "\n",
            "1) Word2Vec\n",
            "In 2013, a team of researchers at Google, led by Tomas\n",
            "Mikolov, introduced the breakthrough model for word rep-\n",
            "resentation called Word2Vec [27], [43], which marked the\n",
            "beginning of a spectacular evolution in NLP. Mikolov and his\n",
            "collaborators proposed two model architectures for comput-\n",
            "ing continuous vector representations of words by using the\n",
            "unsupervised approach: Continuous Bag-of-Words (CBOW)\n",
            "and Continuous Skip-gram Model (Fig. 1). The CBOW archi-\n",
            "tecture predicts the current word based on the context, while\n",
            "in the Skip-Gram architecture, the distributed representation\n",
            "of the input word is used to predict the context [43]. The\n",
            "authors show the effectiveness of the proposed methodology\n",
            "experimentally, using several NLP applications, including\n",
            "sentiment analysis. Additionally, they demonstrate that the\n",
            "Skip-gram architecture gives more accurate results for large\n",
            "datasets because it generates more general contexts.\n",
            "\n",
            "The main drawback of Word2Vec is its inability to handle\n",
            "unknown or out-of-vocabulary (OOV) words. If the model\n",
            "has not encountered a word before, it will be unable to inter-\n",
            "pret it or build a vector for it. Additionally, Word2Vec does\n",
            "not support shared representations at sub-word level, which\n",
            "means that it will create two completely different vector\n",
            "representations for words which are morphologically similar,\n",
            "like agree/agreement or worth/worthwhile [29].\n",
            "\n",
            "In our analysis, we use a pre-trained version of Word2Vec\n",
            "on the Google News corpus, which contains almost 3 million\n",
            "English words represented by 300-dimensional vectors.\n",
            "\n",
            "2) GloVe\n",
            "In 2014, a team of researchers at Stanford University pro-\n",
            "posed GloVe, an improved methodology for word encoding,\n",
            "based on a solid mathematical approach [28]. GloVe over-\n",
            "comes the drawbacks of Word2Vec in the training phase,\n",
            "improving the generated embeddings. It emphasizes the\n",
            "importance of considering the co-occurrence probabilities\n",
            "between the words rather than single word occurrence prob-\n",
            "abilities themselves. The model combines two classes of\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "131665\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "D. SENTENCE ENCODERS\n",
            "In 2014, the idea of encoding entire sentences surpassed\n",
            "word encoding. The primary purpose of sentence encoders\n",
            "is to learn ﬁxed-length feature vectors that encode the syntax\n",
            "and semantic properties of variable-length sentences. While a\n",
            "simple sentence embedding model can be built by averaging\n",
            "the individual word embeddings for every word of the sen-\n",
            "tence, this approach loses the inherent context and sequence\n",
            "of words as valuable information that should be retained in\n",
            "many tasks.\n",
            "\n",
            "The main weakness of using sentence encoders to han-\n",
            "dle variable-length text input is related to the ﬁxed size of\n",
            "the produced vectors. Long and short sentences are treated\n",
            "equally, producing the same number of extracted features,\n",
            "thus diluting the embeddings.\n",
            "\n",
            "In this section, we outline recent and most prevalent sen-\n",
            "tence encoders [2], [30]–[33], to assess their ability to extract\n",
            "important features in sentence representation of ﬁnancial\n",
            "headlines.\n",
            "\n",
            "1) Doc2Vec\n",
            "In 2014, the ﬁrst successful sentence encoder, Doc2Vec\n",
            "[30] introduced an approach for representing variable-length\n",
            "fragments of texts (sentences, paragraphs, and documents)\n",
            "as ﬁxed-size dense vectors, a.k.a. paragraph vectors. These\n",
            "vectors are trained to predict words in documents. Their\n",
            "primary goal is to make an appropriate distributed repre-\n",
            "sentation of large texts, overcoming the weaknesses of bag-\n",
            "of-words methods. Paragraph vectors combine word vectors\n",
            "to build phrase-level or sentence-level representations. They\n",
            "epitomize a distributed memory model, holding the context\n",
            "of the paragraph and contributing to the prediction task of the\n",
            "next word in combination with word vectors. Additionally,\n",
            "paragraph vectors can be used as features for the paragraph,\n",
            "which can be fed as input to a classiﬁer or to a neural network,\n",
            "making them appropriate for evaluation of sentiment anal-\n",
            "ysis in ﬁnancial headlines. To obtain sentence embeddings,\n",
            "we use a Doc2Vec approach, which is pre-trained on English\n",
            "Wikipedia texts.\n",
            "\n",
            "2) SKIP-THOUGHT VECTORS\n",
            "Skip-Thought Vectors [31] are models that use encoder-\n",
            "decoder architecture for sequence modeling based on unsu-\n",
            "pervised learning. These models use continuity of texts,\n",
            "extracted from books, to train an encoder-decoder method.\n",
            "The model tries to reconstruct the surrounding sentences of an\n",
            "encoded passage in order to remap their syntactic and seman-\n",
            "tic meaning into similar vector representations. The encoder\n",
            "generates a sentence vector, and the decoder is used to gen-\n",
            "erate the surrounding sentences. The model uses a Recur-\n",
            "rent Neural Networks (RNN) encoder with Gated Recurrent\n",
            "Unit (GRU) [56] activations, and an RNN decoder uses a\n",
            "conditional GRU. The use of the attention layer provides\n",
            "for a dynamic change of the source sentence representation.\n",
            "\n",
            "FIGURE 2. InferSent training scheme [32].\n",
            "\n",
            "Depending on the encoder type, two separate models are\n",
            "trained: uni-skip and bi-skip. Uni-skip passes sentences in the\n",
            "correct order and extracts 2400 features. The Bi-skip model\n",
            "uses two encoders. One of them passes the sentence in the cor-\n",
            "rect order and the other passes the sentence in reverse order,\n",
            "extracting a total of 2400 features. Due to their generative\n",
            "nature, Skip-Thought vectors are appropriate and effective for\n",
            "neural machine translation and classiﬁcation tasks. The main\n",
            "shortcoming of this approach is the arduous task assigned\n",
            "to the decoder [57], as the next sentence prediction requires\n",
            "modeling aspects that are, in most cases, irrelevant to the\n",
            "meaning of the sentence.\n",
            "\n",
            "3) InferSent\n",
            "InferSent [32] is a supervised approach to learning sentence\n",
            "embeddings using natural language inference (NLI) data.\n",
            "NLI captures universally useful features, thus learning uni-\n",
            "versal sentence embeddings in a supervised manner. The\n",
            "training dataset used by this model is the Stanford Natu-\n",
            "ral Language Inference (SNLI) dataset that contains 570k\n",
            "human-generated English sentence pairs, manually anno-\n",
            "tated with one of the three labels: entailment, contradiction,\n",
            "or neutral.\n",
            "\n",
            "Fig. 2 shows a shared encoder used for encoding the\n",
            "premise u and the hypothesis v. In order to extract relations\n",
            "between u and v, three matching methods are applied: con-\n",
            "catenation (u, v), element-wise product u ∗ v and absolute\n",
            "element-wise difference |u − v|. Next, the resulting feature\n",
            "vector is applied as input to the 3-class classiﬁer to evaluate\n",
            "the relationship between u and v based on the extracted\n",
            "features. Experimentally, the best architecture for the encoder\n",
            "is shown to be the BiLSTM network with max pooling. This\n",
            "approach outperforms Skip-Thought vectors in many NLP\n",
            "tasks.\n",
            "\n",
            "In our study, we assess the performances of two publicly\n",
            "available versions of InferSent. The ﬁrst version is trained\n",
            "with Stanford’s GloVe as word encoder and the second is\n",
            "trained with Facebook’s FastText.\n",
            "\n",
            "131666\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "FIGURE 3. USE based on DAN architecture [2].\n",
            "\n",
            "4) UNIVERSAL SENTENCE ENCODER\n",
            "In March 2018, Google researchers published their ﬁrst ver-\n",
            "sion of a model which converts variable-length sentences into\n",
            "512-dimensional vectors, called Universal Sentence Encoder\n",
            "(USE) [2]. The model is able to embed not only sentences,\n",
            "but also words and entire paragraphs. USE uses the concept\n",
            "of transfer-learning to leverage the knowledge extracted from\n",
            "large datasets to improve the results when limited training\n",
            "data is available.\n",
            "\n",
            "We evaluate the USE encoder, which is based on Deep\n",
            "Averaging Network (DAN) architecture as shown in Fig.3.\n",
            "Input embeddings for words and bi-grams are ﬁrst averaged\n",
            "and then passed through a feed-forward deep neural net-\n",
            "work (DNN) to produce sentence embeddings. The compu-\n",
            "tational time is linear in the length of the input sentence.\n",
            "\n",
            "USE models are trained on a variety of data sources:\n",
            "Wikipedia, news, question-answer pages, and discussion\n",
            "forums. These models are based on transfer-learning exper-\n",
            "iments with several datasets to evaluate the efﬁciency of the\n",
            "encoder. The results show that sentence encoders outperform\n",
            "transfer-learning methodologies that use word-level embed-\n",
            "dings alone.\n",
            "\n",
            "The main issues with USE (DAN model) are related to the\n",
            "use of averaging techniques that cannot recognize negation\n",
            "phrases like ‘‘not good.’’ This refers to using contextualized\n",
            "embeddings, which considers the inﬂuence of other words in\n",
            "producing sentence embedding.\n",
            "\n",
            "In our analysis, we assess the two latest versions of\n",
            "USE (4 and 5) that can be found at the TensorFlow Hub\n",
            "repository.3\n",
            "\n",
            "5) LANGUAGE-AGNOSTIC SENTENCE REPRESENTATIONS\n",
            "(LASER)\n",
            "In 2019, Facebook researchers [33] introduced an architec-\n",
            "ture for universal language-agnostic multilingual sentence\n",
            "representations (LASER) for 93 languages by using a single\n",
            "BiLSTM encoder with a shared Byte Pair Encoding (BPE)\n",
            "vocabulary for different languages. The main contribution\n",
            "of the LASER methodology is that it provides a framework\n",
            "for zero-shot transfer-learning. LASER leverages one model,\n",
            "trained on one language, to be used in another language\n",
            "\n",
            "FIGURE 4. LASER architecture [33].\n",
            "\n",
            "without the need for pre-training. This is accomplished by\n",
            "LASER’s ability to bring semantically similar sentences,\n",
            "written in different languages, close to each other in the\n",
            "embedding space.\n",
            "\n",
            "Sentence embeddings are obtained by applying a\n",
            "max-pooling operation to the output of the BiLSTM encoder.\n",
            "The same encoder is used for all 93 languages. The byte-pair\n",
            "encoding (BPE) vocabulary is learned based on the con-\n",
            "catenation of all training corpora, hence, it does not require\n",
            "speciﬁc information about the input language. LASER’s\n",
            "encoder architecture, illustrated in Fig. 4, is shown to be\n",
            "efﬁcient even for low-resource languages.\n",
            "\n",
            "In this study, we evaluate LASER on English texts, though\n",
            "the same model that we build here can be used for sentiment\n",
            "analysis in texts written in the other 92 languages supported\n",
            "by LASER.\n",
            "\n",
            "III. NLP TRANSFORMERS\n",
            "The pre-trained word and sentence embeddings show good\n",
            "performance for NLP tasks due to their ability to retain the\n",
            "semantics and the syntax of the words in the sentence. The\n",
            "transfer-learning task, in this case, allows for the information\n",
            "that has been learned from unlabeled data to be used in tasks\n",
            "with relatively small labeled data to achieve higher accu-\n",
            "racy. Although such embeddings have proven to be powerful,\n",
            "they lack context-based mutability. Word2Vec, GloVe, and\n",
            "FastText use ﬁxed embeddings for each of the words, thus\n",
            "producing one-to-one mapping, which in many cases is not\n",
            "appropriate and requires additional attention. Recent research\n",
            "studies have proposed methods that produce different embed-\n",
            "dings for the same word, taking into consideration speciﬁc\n",
            "contexts [3], [55], [58]. As an illustration of context impor-\n",
            "tance, we analyze the following two sentences that contain\n",
            "the word ‘‘Apple’’: ‘‘Apple Inc performed well this year.’’\n",
            "and ‘‘Apple fruits are exported to various countries.’’ In the\n",
            "ﬁrst sentence, Apple refers to the technology company Apple,\n",
            "headquartered in the US, while in the second sentence, apple\n",
            "refers to the fruit, with a completely different meaning. The\n",
            "encoders, however, will produce the same encoding for both\n",
            "words regardless of the contexts. This problem highlights the\n",
            "need for contextualized embeddings for the word ‘‘Apple.’’\n",
            "\n",
            "A. NLP TRANSFORMER ARCHITECTURE\n",
            "A transformer represents an architecture that transforms one\n",
            "sequence into another by using two models: encoder and\n",
            "decoder. Unlike previously described standard sequence-to-\n",
            "sequence models, which are based on LSTM/GRU units,\n",
            "the paper ‘‘Attention is All You Need’’ [59] introduces a\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "131667\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "A multi-head attention mechanism calculates the scaled\n",
            "dot-product attention multiple times in parallel. The inde-\n",
            "pendent outputs are concatenated and linearly transformed\n",
            "into expected dimensions. Multi-head attention is obtained by\n",
            "using Eq. 7:\n",
            "\n",
            "MultiHead(Q, K , V ) = [head1, head2, . . . headh]W O\n",
            "\n",
            "(7)\n",
            "\n",
            "Each of the headi can be calculated by Eq. 8:\n",
            "\n",
            "headi = Attention(QW Q\n",
            "i\n",
            "\n",
            ", KW K\n",
            "i\n",
            "\n",
            ", VW i\n",
            "V )\n",
            "\n",
            "(8)\n",
            "\n",
            ", W i\n",
            "\n",
            ", W K\n",
            "i\n",
            "\n",
            "where W Q\n",
            "V and W O are parameter matrices, which\n",
            "i\n",
            "the model needs to learn. Multi-head attentions have an\n",
            "important role in obtaining the contextual embeddings when\n",
            "using NLP transformers.\n",
            "\n",
            "A pre-training phase is an unsupervised learning approach\n",
            "where an unlabeled text corpus is introduced into the trans-\n",
            "former architecture to produce text representations based on\n",
            "an objective function used by the transformer. This is a rel-\n",
            "atively expensive task, but the learned token or generic sen-\n",
            "tence representations can be used in many other tasks using\n",
            "transfer-learning. Later, the representation can be ﬁne-tuned\n",
            "in order to recognize the speciﬁcs of the task and to achieve\n",
            "better results. Fine-tuning is performed by adding an addi-\n",
            "tional dense layer after the last hidden state, recommended for\n",
            "using transformers in classiﬁcation and regression tasks [3].\n",
            "The transformer performs supervised learning (ﬁne-tuning)\n",
            "on the labeled sentiment dataset, which is relatively inexpen-\n",
            "sive compared to pre-training.\n",
            "\n",
            "NLP transformers are applicable to many different text\n",
            "classiﬁcation problems, such as binary sentiment classiﬁca-\n",
            "tion, which we use in our analysis.\n",
            "\n",
            "1) BERT\n",
            "In 2018, Devlin et al.\n",
            "leveraged the transformer\n",
            "[3]\n",
            "architecture to introduce a revolutionary language represen-\n",
            "tation model, called BERT (Bidirectional Encoder Repre-\n",
            "sentations from Transformers). This model started the new\n",
            "era in NLP, with state-of-the-art performance achieved on\n",
            "most NLP tasks. BERT leverages the unsupervised learning\n",
            "approach to pre-train deep bidirectional representations from\n",
            "large unlabeled text corpora by using two new pre-training\n",
            "objectives — masked language model (MLM) and next sen-\n",
            "tence prediction (NSP). BERT overcomes the limitation of\n",
            "previous language models, which incorporate only unidirec-\n",
            "tional representations of words in sentences. It builds a bidi-\n",
            "rectional masked language model, which predicts randomly\n",
            "masked words in the sentence, enriching the contextual infor-\n",
            "mation of the words.\n",
            "\n",
            "BERT is based on conventional, auto-regressive (AR) lan-\n",
            "guage modeling. The process of pre-training is performed\n",
            "by maximizing the likelihood between the tokens x in a\n",
            "text sequence x = [x1, . . . , xT ]. Let ˆx denote the same text\n",
            "sentence with masked tokens and x be an array of masked\n",
            "tokens. The training objective for BERT is to reconstruct x\n",
            "\n",
            "FIGURE 5. The Transformer architecture [59].\n",
            "\n",
            "novel, breakthrough transformer architecture based solely\n",
            "on multi-headed self-attention mechanisms. There are three\n",
            "reasons for choosing self-attention instead of recurrent\n",
            "layer: computational complexity, parallelization, and learning\n",
            "long-range dependencies between words in the sequence, all\n",
            "of which are crucial for building contextualized embeddings.\n",
            "By using this approach, transformers have shown improved\n",
            "results in machine translation and other related tasks.\n",
            "\n",
            "This method uses positional embedding to remember the\n",
            "order of words in the sequence. The main building blocks\n",
            "in the encoder/decoder modules are Multi-Head Attention\n",
            "and Feed Forward layers, as shown in the Attention-based\n",
            "transformer architecture (Fig.5).\n",
            "\n",
            "The scaled dot-product attention mechanism is described\n",
            "\n",
            "by equations 5 and 6.\n",
            "\n",
            "a = softmax(\n",
            "\n",
            "QK T\n",
            "√\n",
            "n\n",
            "\n",
            ")\n",
            "\n",
            "(5)\n",
            "\n",
            "In Eq.5, the attention weights a represent the inﬂuence of\n",
            "each word in the sequence (Q) by all the other words (K) in\n",
            "the same sequence. Q is a matrix that contains the query\n",
            "(vector representation of one word in the sequence), K are\n",
            "all keys (all vector representations of all the words in the\n",
            "sequence) and n is dimensionality of the query/key vectors.\n",
            "The softmax function is used to ensure that weights a have a\n",
            "distribution between 0 and 1. Considering a, a self-attention is\n",
            "calculated by using Eq.6, which represents a weighted sum of\n",
            "values (V), where V is the vector obtained from the encoder.\n",
            "\n",
            "Attention(Q, K , V ) = aV\n",
            "\n",
            "(6)\n",
            "\n",
            "131668\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "from ˆx by Eq.9:\n",
            "\n",
            "max\n",
            "θ\n",
            "\n",
            "log pθ (x|ˆx) ≈\n",
            "\n",
            "=\n",
            "\n",
            "T\n",
            "(cid:88)\n",
            "\n",
            "t=1\n",
            "T\n",
            "(cid:88)\n",
            "\n",
            "t=1\n",
            "\n",
            "where,\n",
            "\n",
            "mt log pθ (xt|ˆx)\n",
            "\n",
            "mt log\n",
            "\n",
            "exp (Hθ (ˆx)T\n",
            "x(cid:48) exp(Hθ (ˆx)T\n",
            "\n",
            "t e(xt ))\n",
            "\n",
            "t e(x(cid:48)))\n",
            "\n",
            "(cid:80)\n",
            "\n",
            "sequence x of length T , there are T ! different orders on which\n",
            "the algorithm performs auto-regressive factorizations.\n",
            "\n",
            "Let ZT be the set of permutations of the words in a sentence\n",
            "of length T. xz<t denotes the ﬁrst t − 1 elements of the\n",
            "permutation z ∈ ZT . The PLM objective is given in Eq. 10.\n",
            "\n",
            "(9)\n",
            "\n",
            "max\n",
            "θ\n",
            "\n",
            "Ez∼Z\n",
            "\n",
            "|z|\n",
            "(cid:88)\n",
            "\n",
            "t=c+1\n",
            "\n",
            "log pθ (xzt |xz<t)\n",
            "\n",
            "(10)\n",
            "\n",
            "• e(x(cid:48)) denotes the embedding of the token x;\n",
            "• mt = 1, if xt token of the text sequence x is masked;\n",
            "• Hθ is a Transformer which transforms each token of text\n",
            "\n",
            "sequence into a hidden vector.\n",
            "\n",
            "BERT assumes that all masked tokens x are mutually inde-\n",
            "pendent, which is the main rationale behind the approxi-\n",
            "mation of the joint conditional probability p(x, ˆx) in Eq.9.\n",
            "Another advantage that differentiates BERT from previous\n",
            "AR methods is the ability to increase the context information\n",
            "Hθ (x)t by accessing the tokens placed on the left and the right\n",
            "side of token t.\n",
            "\n",
            "BERT has two versions: BERT-base, with 12 encoder lay-\n",
            "ers, hidden size of 768, 12 multi-head attention heads and\n",
            "110M parameters in total; and BERT-large, with 24 encoder\n",
            "layers, hidden size of 1024, 16 multi-head attention heads and\n",
            "340M parameters. Both of these models have been trained on\n",
            "English Wikipedia and BookCorpus [60].\n",
            "\n",
            "2) FinBERT\n",
            "FinBERT [61] is a version of BERT intended for the ﬁnance\n",
            "domain. It is pre-trained on a ﬁnancial text corpus which con-\n",
            "sists of 1.8M news articles from Reuters TRC2 dataset, pub-\n",
            "lished between 2008 and 2010. Compared to other pre-trained\n",
            "versions of BERT, FinBERT model has achieved a 15%\n",
            "improvement in accuracy in text classiﬁcation tasks specif-\n",
            "ically applied to ﬁnancial texts.\n",
            "\n",
            "3) XLNet\n",
            "The XLNet model, developed by Google Brain and Carnegie\n",
            "Mellon University, addresses the disadvantages of BERT,\n",
            "improves its architectural design for pre-training, and pro-\n",
            "duces results that outperform BERT in 20 different tasks.\n",
            "It utilizes a generalized AR model where the next token is\n",
            "dependent on all previous tokens, thus avoiding corrupted\n",
            "input caused by masking of the words, performed by BERT.\n",
            "The limitations of BERT include neglecting the dependency\n",
            "between masked tokens as it assumes that they are mutually\n",
            "independent variables. On the other hand, XLNet considers\n",
            "these tokens in the process of context building and assumes\n",
            "that masked words are mutually dependent.\n",
            "\n",
            "Additionally, XLNet uses Permutation Language Model-\n",
            "ing (PLM) to capture bidirectional context by maximizing\n",
            "the expected log-likelihood of a sequence given all possible\n",
            "permutations of words in a sentence. This means that XLNet\n",
            "enriches the contextual information of each position by lever-\n",
            "aging the tokens from all the other positions found on the\n",
            "left and on the right sides of the token. Speciﬁcally, for a\n",
            "\n",
            "The hyperparameter c can be derived from the hyperpa-\n",
            "rameter K, where c = |z|(K − 1)/K , and it represents the\n",
            "cutting-point of the division of vector z into non-target z≤c\n",
            "and target z>c subsequences.\n",
            "\n",
            "As shown in Eq.9 and Eq.10, both BERT and XLNet\n",
            "perform partial prediction, due to optimization. The main dif-\n",
            "ference lies in the choice of tokens used for context modeling.\n",
            "BERT predicts the masked tokens, assuming that targets are\n",
            "mutually independent, while XLNet predicts the last token in\n",
            "a factorization order z>c.\n",
            "\n",
            "The following example [Wells, Fargo, is, a, bank, in, USA]\n",
            "explains the difference. Assume that our goal is to predict\n",
            "‘‘Wells Fargo.’’ In order to use [Wells, Fargo] as prediction\n",
            "targets, BERT masks them, and XLNet samples the factor-\n",
            "ization order [is,a,bank,in,USA,Wells,Fargo]. Using Eq. 9,\n",
            "BERT will compute:\n",
            "\n",
            "JBERT = log p(Wells|is, a, bank, in, USA)\n",
            "\n",
            "+ log p(FARGO|is, a, bank, in, USA)\n",
            "\n",
            "(11)\n",
            "\n",
            "Using Eq. 10, XLNet will compute:\n",
            "\n",
            "JXLNet = log p(Wells|is, a, bank, in, USA)\n",
            "\n",
            "+ log p(FARGO|Wells, is, a, bank, in, USA)\n",
            "\n",
            "(12)\n",
            "\n",
            "These examples show that both BERT and XLNet compute\n",
            "the objective differently. XLNet captures important depen-\n",
            "dencies between prediction targets, such as (Wells, Fargo),\n",
            "which BERT omits. Hence, XLNet combines the advantages\n",
            "of AR and auto-encoding methods by using a generalized AR\n",
            "pre-training approach with a permutation language modeling\n",
            "objective, in order to improve the results in NLP.\n",
            "\n",
            "4) XLM\n",
            "The Cross-lingual Language Model (XLM) [62] has a\n",
            "transformer architecture that is mainly used for modeling\n",
            "cross-lingual features. XLM is pre-trained using several\n",
            "objectives:\n",
            "\n",
            "• Causal Language Modeling (CLM)\n",
            "\n",
            "- next\n",
            "\n",
            "token\n",
            "\n",
            "prediction.\n",
            "\n",
            "• Masked Language Modeling (MLM) - approach similar\n",
            "to BERT’s objective for masking random tokens in the\n",
            "sentence.\n",
            "\n",
            "• Translation Language Modeling (TLM) - supervised\n",
            "approach, which harnesses parallel streams of textual\n",
            "data written in different languages in order to improve\n",
            "cross-lingual pre-training support.\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "131669\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "In our analysis, we use XLM for text classiﬁcation tasks to\n",
            "perform sentiment analysis of texts in English. We explore\n",
            "bi-directional context of the tokens in sentences to per-\n",
            "form Masked Language Modeling (MLM), which is the best\n",
            "approach for our evaluation task.\n",
            "\n",
            "5) ALBERT\n",
            "To overcome the shortcomings of using large pre-training\n",
            "natural language representations such as GPU/TPU, mem-\n",
            "ory limitations, and longer training times, in 2019 Google\n",
            "Research and Toyota Technological Institute jointly released\n",
            "a new model that introduces BERT’s smaller and more scal-\n",
            "able successor, called ALBERT [63]. ALBERT is based\n",
            "on two-parameter reduction methods: cross-layer parameter\n",
            "sharing and sentence ordering objectives, in order to lower\n",
            "memory consumption and increase the training speed of\n",
            "BERT. ALBERT outperforms BERT in several tasks, includ-\n",
            "ing text classiﬁcation [64]. ALBERT uses a signiﬁcantly\n",
            "reduced number of parameters in sentiment analysis, com-\n",
            "pared to BERT and XLNet.\n",
            "\n",
            "6) RoBERTa\n",
            "The RoBERTa model, introduced by the Facebook research\n",
            "team in 2019 [4], offers an alternative optimized ver-\n",
            "sion of BERT. Retrained on a dataset\n",
            "ten times larger,\n",
            "with improved training methodology and different hyper-\n",
            "parameters, RoBERTa removes the Next Sentence Predic-\n",
            "tion (NSP) objective and adds dynamic masking of words\n",
            "during the training epochs. These changes and features show\n",
            "better performances compared to BERT in many NLP tasks,\n",
            "including text classiﬁcation.\n",
            "\n",
            "7) DistilBERT\n",
            "DistilBERT, introduced in October 2019 [65], is based on a\n",
            "methodology that reduces the size of a BERT model by 40%,\n",
            "while retaining 97% of its language understanding capa-\n",
            "bilities and being 60% faster. The technique that produces\n",
            "a compression of the original model is known as knowl-\n",
            "edge distillation. The compact (student) model is trained to\n",
            "reproduce the full output distribution of the larger (teacher)\n",
            "model or ensemble of models. Rather than training with a\n",
            "cross-entropy over the hard-targets (one-hot encoding of the\n",
            "classes), the student obtains the knowledge based on a dis-\n",
            "tillation loss over the soft-target probabilities of the teacher.\n",
            "The distillation loss Lce is calculated by using the Eq. 13.\n",
            "\n",
            "Lce = (cid:88)\n",
            "\n",
            "ti ∗ log(si)\n",
            "\n",
            "i\n",
            "\n",
            "(13)\n",
            "\n",
            "where ti and si are the estimated probabilities of the teacher\n",
            "and student respectively. This objective results in a richer\n",
            "training signal, since soft-target probabilities enforce stricter\n",
            "constraints compared to a single hard-target.\n",
            "\n",
            "We assess the performances of\n",
            "\n",
            "three distilled ver-\n",
            "sions (students) of the following transformers (teachers):\n",
            "BERT-base-cased, BERT-base-uncased, and RoBERTa-base.\n",
            "\n",
            "8) XLM-RoBERTa\n",
            "The XLM-RoBERTa (XLM-R) [66] model is a multilingual\n",
            "model trained on one hundred different languages by using\n",
            "2.5TB of ﬁltered CommonCrawl data and it is based on Face-\n",
            "book’s RoBERTa model. XLM-R achieves solid performance\n",
            "gains for a wide range of cross-lingual transfer tasks, includ-\n",
            "ing text classiﬁcation. Additionally, XLM-RoBERTa offers\n",
            "a possibility of multilingual modeling without decreasing\n",
            "per-language performance, which makes it more attractive for\n",
            "evaluation compared to other transformers.\n",
            "\n",
            "XLM-R follows the XLM approach [62], trained with a\n",
            "Masked Language Modeling (MLM) objective with minor\n",
            "changes to the hyper-parameters of the original XLM model.\n",
            "In our analysis, we evaluate the performance of two\n",
            "different pre-trained XLM-R models: XLM − Rbase and\n",
            "XLM − RLarge, which differ in the size of their parameters.\n",
            "\n",
            "9) BART\n",
            "In October 2019, the Facebook research team published a\n",
            "novel transformer called BART [67] with an architecture sim-\n",
            "ilar to both BERT [3] and GPT2 (Generative Pre-Training 2)\n",
            "[68]. BART outperforms other transformers in generation\n",
            "tasks such as text summarizing and question answering.\n",
            "BART leverages the advantages of the bidirectional encoder\n",
            "from BERT and the GPT AR decoder. The auto-regressive\n",
            "approach means that GPT considers left to right dependence\n",
            "of the words in a sentence, which makes it more appropriate\n",
            "for text-generation compared to BERT. BART’s encoder and\n",
            "decoder are connected by cross-attention. Each decoder layer\n",
            "performs attention over the ﬁnal hidden state of the encoder\n",
            "output. This mechanism enables the model to generate output\n",
            "that is closely connected to the original input.\n",
            "\n",
            "The ﬁne-tuned model concatenates the input sentence with\n",
            "the end of sequence (EOS) token and passes these compo-\n",
            "nents as input to the BART encoder and decoder. The repre-\n",
            "sentation of the EOS token is used to classify the sentiment\n",
            "expressed in the sentence. In this study, we ﬁne-tune BART\n",
            "and adapt it to sentiment analysis in ﬁnance.\n",
            "\n",
            "IV. DATASETS\n",
            "We use publicly available datasets that have been labeled\n",
            "by ﬁnancial experts to perform a reliable evaluation of\n",
            "the ML models in predicting sentiments of ﬁnancial head-\n",
            "lines. We perform binary classiﬁcations to designate each\n",
            "of the sentences as bullish (positive) or bearish (negative),\n",
            "as described in the following subsections.\n",
            "\n",
            "A. FINANCIAL PHRASE BANK\n",
            "The Financial Phrase-Bank dataset [69] consists of 4845\n",
            "English sentences selected randomly from ﬁnancial news\n",
            "found on the LexisNexis database. These sentences have been\n",
            "annotated by 16 experts with a background in ﬁnance and\n",
            "business. The annotators were asked to give labels according\n",
            "to how they think the information in the sentence might\n",
            "inﬂuence the mentioned company’s stock price. The dataset\n",
            "\n",
            "131670\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "TABLE 1. Datasets statistics.\n",
            "\n",
            "FIGURE 6. Distribution of number of words in training set.\n",
            "\n",
            "also includes information regarding the agreement levels on\n",
            "sentences among annotators. All sentences are annotated with\n",
            "three labels: Positive, Negative, and Neutral. The distribution\n",
            "of sentiment labels is presented in Table 1.\n",
            "\n",
            "B. SemEval 2017 TASK 5\n",
            "The second dataset used in this paper is provided by the\n",
            "SemEval-2017 task ‘‘Fine-Grained Sentiment Analysis on\n",
            "Financial Microblogs and News’’ [70]. The Financial News\n",
            "Statements and Headlines dataset consists of 2510 news head-\n",
            "lines, gathered from different publicly available sources such\n",
            "as Yahoo Finance. Each headline (instance) is annotated by\n",
            "three independent ﬁnancial experts, and a sentiment score,\n",
            "in the range between -1 and 1, is assigned to each statement.\n",
            "A score of -1 means that the statement (message) is bearish\n",
            "or very negative, and a score of 1 means that the statement\n",
            "is bullish or very positive. We convert these sentiment scores\n",
            "into sentiment labels (bullish/bearish). The conversion pro-\n",
            "cess is performed by using Eq. 14.\n",
            "\n",
            "L =\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Bullish,\n",
            "Bearish,\n",
            "Neutral,\n",
            "\n",
            "if score > 0\n",
            "if score < 0\n",
            "if score = 0\n",
            "\n",
            "FIGURE 7. Distribution of number of words in validation set.\n",
            "\n",
            "(14)\n",
            "\n",
            "After the conversion, the number of sentences per label is\n",
            "\n",
            "presented in Table 1.\n",
            "\n",
            "The dataset used for evaluation is a combination of both\n",
            "datasets. To address the imbalance between positive and\n",
            "negative sentences, we perform a balancing by extract-\n",
            "ing 1093 positive and another 1093 negative sentences, which\n",
            "we merge into one dataset. Additionally, we shufﬂe the\n",
            "datasets and we set aside stratiﬁed 80% of all sentences\n",
            "as a training and stratiﬁed 20% of the remaining sentences\n",
            "as a validation set. At the end, our balanced training set\n",
            "includes 1748 samples, and a balanced validation set consist-\n",
            "ing of 438 samples.\n",
            "\n",
            "C. DATA PRE-PROCESSING\n",
            "Financial headlines, similar to other real world text data,\n",
            "are likely to be inconsistent, incomplete and contain errors.\n",
            "Hence, to prepare the data, we perform initial pre-processing\n",
            "that includes tokenization, stop-word removal, and stem-\n",
            "ming. Additionally, we extract the named entities (organiza-\n",
            "tions and people) from the headlines and replace them with\n",
            "their general nouns. For example, Microsoft is replaced with\n",
            "<CMPY>, or London with <CITY>.\n",
            "\n",
            "We impose a min-max length of sentences to 3-64 words.\n",
            "After this initial ﬁltering, we obtain the distributions of the\n",
            "\n",
            "number of words per sentence for the training set (Fig. 6) and\n",
            "for the validation set (Fig. 7).\n",
            "\n",
            "When evaluating lexicon-based and word encoders,\n",
            "we perform left padding to sentences in order to ﬁx their\n",
            "size, due to their variable length. Considering the maximum\n",
            "size of the sentences given in Figs. 6 and 7, we pad them\n",
            "to 64 word length. When using sentence encoders, we do not\n",
            "pad the sequences due to the ability of the sentence encoders\n",
            "to encode sentences to ﬁxed-size vectors.\n",
            "\n",
            "V. SENTIMENT ANALYSIS PLATFORM\n",
            "We evaluate the sentiment analysis methods by using the\n",
            "general platform, consisting of ﬁve phases shown in Fig. 8,\n",
            "as follows:\n",
            "\n",
            "• In the ﬁrst phase, we create our working dataset based on\n",
            "the Financial Phrase Bank and the SemEval 2017 dataset.\n",
            "• In the second phase we apply data pre-processing func-\n",
            "\n",
            "tions as described in subsection IV-C.\n",
            "\n",
            "• The third phase performs text encoding by using various\n",
            "text representation methods in order to extract features\n",
            "from the pre-processed texts. We evaluate the following\n",
            "text representation methods: domain lexicons, statistical\n",
            "models for feature extraction, word encoders, sentence\n",
            "encoders and NLP transformers.\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "131671\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "FIGURE 8. Sentiment analysis platform architecture.\n",
            "\n",
            "TABLE 2. Average performances of models grouped by text representation method.\n",
            "\n",
            "• In the fourth phase, these embeddings are fed as input\n",
            "to various machine-learning or deep-learning classiﬁers,\n",
            "\n",
            "thus enabling us to evaluate many encoding-classiﬁer\n",
            "combinations.\n",
            "\n",
            "131672\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "TABLE 3. Lexical Rule-based approach results.\n",
            "\n",
            "• In the ﬁfth phase, we compare the real and predicted\n",
            "labels using several binary classiﬁcation performance\n",
            "metrics.\n",
            "\n",
            "In the following subsections, we present the details of\n",
            "machine-learning and deep-learning classiﬁers, ﬁne-tuning\n",
            "of NLP transformers and evaluation metrics.\n",
            "\n",
            "The sentiment analysis platform is implemented in Python\n",
            "3.6. The shallow models are developed using Tensorﬂow\n",
            "Keras [71] while the pre-trained versions of NLP transform-\n",
            "ers are retrieved from the Hugging Face repository [72].\n",
            "The sentiment analysis modules are published at the GitHub\n",
            "repository.4\n",
            "\n",
            "4https://github.com/f-data/ﬁnSENT\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "A. MACHINE-LEARNING CLASSIFIERS\n",
            "In our evaluation analysis, we use two machine-learning\n",
            "classiﬁers: Support Vector Classiﬁer (SVC), as a represen-\n",
            "tative of Support Vector Machines (SVM), and an Extreme\n",
            "Gradient Boosting (XGB) [73], [74], as a representative of\n",
            "gradient-boosted decision trees. We chose the XGB model\n",
            "\n",
            "131673\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "TABLE 4. Statistical methods results.\n",
            "\n",
            "because it has achieved impressive results in many Kaggle\n",
            "competitions, in the structured data category. When using\n",
            "the ML classiﬁers, we perform a GridSearch approach for\n",
            "retrieving the best hyper-parameters.\n",
            "\n",
            "B. DEEP-NEURAL NETWORKS (DNN)\n",
            "Deep-learning methods [75] are achieving outstanding results\n",
            "in many ﬁelds, including: signal processing [76], computer\n",
            "vision [77], speech processing [78]–[80] and text classiﬁca-\n",
            "tion [81].\n",
            "\n",
            "The text representations and the features extracted from\n",
            "the evaluation methods are fed as input into Convolutional\n",
            "Neural Networks (CNN) [23] and Recurrent Neural Net-\n",
            "works (RNN) [82] in order to proceed with the classiﬁcation.\n",
            "While RNN networks work well in sequence modeling and\n",
            "capturing long-term dependencies, CNN networks are more\n",
            "efﬁcient in capturing spatial or temporal correlations and in\n",
            "reducing data dimensionality.\n",
            "\n",
            "In order to improve the architecture of previous DNN net-\n",
            "works, novel mechanisms have been introduced. One of them\n",
            "is the Attention mechanism [83], which helps RNN networks\n",
            "focus on speciﬁc parts of the input sequence, facilitating\n",
            "the learning and improving the prediction. The Attention\n",
            "mechanism is widely used in encoder-decoder architectures\n",
            "due to its ability to highlight important parts of the contextual\n",
            "information.\n",
            "\n",
            "Bidirectional RNN networks are often used to collect fea-\n",
            "−→\n",
            "h gathers token\n",
            "tures from both directions. A forward RNN\n",
            "features from the start (x1) to the end (xn), while the backward\n",
            "←−\n",
            "h processes the tokens in reverse direction, from (xn)\n",
            "RNN\n",
            "to (x1). The resulting hidden state h uses both sets of features\n",
            "concatenating\n",
            "\n",
            "←−\n",
            "h as shown in Eq.15:\n",
            "\n",
            "−→\n",
            "h and\n",
            "\n",
            "hi =\n",
            "\n",
            "−→\n",
            "hi ⊕\n",
            "\n",
            "←−\n",
            "hi\n",
            "\n",
            "(15)\n",
            "\n",
            "where ⊕ denotes the concatenation function.\n",
            "\n",
            "In our analysis, we used shallow RNN and CNN networks\n",
            "in order to evaluate the features from text representations.\n",
            "These shallow neural networks consist of three main layers:\n",
            "the input (embedding) layer, the hidden layer, and the output\n",
            "\n",
            "layer. The input layer uses text representation methods (lexi-\n",
            "cons/word or sentence encoders) to extract the feature vectors\n",
            "from the headlines. It then gives the vector as an input to the\n",
            "recurrent or convolutional hidden layer to extract complex\n",
            "features from the text representation methods. The output\n",
            "layer uses a softmax activation function to make the ﬁnal\n",
            "classiﬁcation. We then add an attention layer after the hidden\n",
            "layer to evaluate its effectiveness. Furthermore, we build an\n",
            "additional group of GRU and LSTM networks, which support\n",
            "bidirectional feature extraction, to assess their performance\n",
            "in ﬁnance-based sentiment analysis as described in [84]. and\n",
            "we use binary cross-entropy loss function when training the\n",
            "models. The ADAM (Adaptive Learning Rate) optimization\n",
            "algorithm [85] is used to ﬁnd optimal weights in the networks.\n",
            "We use a maximum of one hundred training epochs for all\n",
            "DL models. We impose early stopping when the validation\n",
            "loss does not diminish after ten epochs to prevent over-ﬁtting.\n",
            "Finally, we use dropout layer as regularization in the CNN\n",
            "network [86].\n",
            "\n",
            "C. MODEL FINE-TUNING\n",
            "To evaluate NLP transformers, we use pre-trained mod-\n",
            "els from the Hugging Face’s repository [72]. For ﬁnBERT,\n",
            "we use the language model trained on TRC2 dataset, pub-\n",
            "lished on the GitHub repository.5 We ﬁne-tune the trans-\n",
            "formers with the training dataset by adding only one dense\n",
            "layer after the last hidden state. The dense layer outputs\n",
            "the probabilities of sentence classiﬁcation. Transformer’s\n",
            "hyper-parameter settings during the ﬁne-tuning phase are not\n",
            "model agnostic and they are directly related to the quality of\n",
            "the model.\n",
            "\n",
            "D. EVALUATION METRICS\n",
            "We evaluate the models for sentiment analysis of ﬁnancial\n",
            "headlines, and present the results chronologically, based on\n",
            "the models’ publication date. We ﬁrst evaluate lexicon-based\n",
            "methods, using Harvard IV-4 and Loughran-McDonald dic-\n",
            "tionaries. Next, we evaluate word encoders as pioneers in\n",
            "\n",
            "5https://github.com/ProsusAI/ﬁnBERT\n",
            "\n",
            "131674\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "TABLE 5. Fixed word embedding encoders results.\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "131675\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "TABLE 6. Sentence encoders.\n",
            "\n",
            "modern NLP feature engineering approaches. Here, we use\n",
            "word encoders with shallow RNN architectures, described\n",
            "in Section V. Subsequently, we examine the performance\n",
            "of sentence encoders with a shallow dense layer and CNN\n",
            "architectures. Finally, we measure the efﬁciency of the latest\n",
            "NLP transformers, described in Section III.\n",
            "\n",
            "As a main evaluation metric, we chose Matthews Corre-\n",
            "lation Coefﬁcient (MCC) (16), where TP and TN are True\n",
            "Positive and True Negative samples accordingly, and FP and\n",
            "\n",
            "FN are the False Positive and False Negative number of\n",
            "samples which are misclassiﬁed.\n",
            "\n",
            "MCC =\n",
            "\n",
            "√\n",
            "\n",
            "tp ∗ tn−fp ∗ fn\n",
            "(tp + fp)(fn + tn)(fp + tn)(tp + fn)\n",
            "\n",
            "(16)\n",
            "\n",
            "MCC is widely used in assessing binary classiﬁcation\n",
            "performance with a range between -1 (completely wrong\n",
            "binary classiﬁer) and 1 (completely accurate binary classi-\n",
            "ﬁer). It takes into consideration true and false positives and\n",
            "\n",
            "131676\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "TABLE 7. Contextual word embedding encoders results.\n",
            "\n",
            "negatives, thus providing a balanced measure, which can be\n",
            "used even if the classes have different sample sizes.\n",
            "\n",
            "VI. RESULTS AND DISCUSSION\n",
            "In this section, we present the model evaluation results.\n",
            "In Table 3, we report on the performance of\n",
            "the\n",
            "lexicon-based models by using hand-crafted feature engi-\n",
            "neering, based on the Loughran-McDonald (LM) ﬁnan-\n",
            "cial and general Harvard IV-4 dictionaries. We perform\n",
            "the evaluations by using the Lydia system polarity detec-\n",
            "tion, machine-learning classiﬁers, and deep-learning mod-\n",
            "els, as described in previous sections. As expected, the\n",
            "Loughran-McDonald features outperform the Harvard IV-4\n",
            "general-purpose sentiment analysis dictionary. Hence, fea-\n",
            "ture extraction with a domain-speciﬁc dictionary is a better\n",
            "approach for sentiment analysis tasks. The best perform-\n",
            "ing model is the XGB classiﬁer using LM features, achiev-\n",
            "ing MCC=0.327. Additionally, we ﬁnd that RNN networks\n",
            "outperform CNN and fully-connected dense networks. The\n",
            "improved results are due to the RNN networks’ ability to\n",
            "remember sequential data, which is crucial for classiﬁca-\n",
            "tion of sentences. Furthermore, the bidirectional context and\n",
            "attention layer improve the results when used in combination\n",
            "with RNN networks.\n",
            "\n",
            "In Table 4, we present the results of the experiments per-\n",
            "formed on features extracted from statistical methods. We use\n",
            "ML classiﬁers and a deep neural network classiﬁer based\n",
            "on fully connected dense layers. These methods show good\n",
            "results, achieving an MCC score of 0.667, almost twice as\n",
            "good as the lexicon-based methods.\n",
            "\n",
            "In Table 5, we present the evaluation results of the word\n",
            "encoders. Generally, the best score is achieved when using\n",
            "Stanford’s GloVe with Bidirectional GRU and attention layer\n",
            "(MCC=0.704). Here, the attention layer increases the MCC\n",
            "score by 0.04 compared to the BiGRU method without the\n",
            "attention layer (MCC=0.666). Additionally, the evaluated\n",
            "word encoders achieve better results when used with RNN\n",
            "networks, which further learn the context from the attention\n",
            "layer. In all tests, the GRU units outperform the LSTM units.\n",
            "The features extracted from word encoders are signiﬁ-\n",
            "cantly better compared to the features extracted by using\n",
            "lexicons and dictionaries. Furthermore, the word encoders\n",
            "perform better than statistical methods for feature extraction,\n",
            "which implies that incorporating semantic meaning into the\n",
            "word representation is useful for classiﬁcation.\n",
            "\n",
            "The results obtained from the evaluation of sentence\n",
            "encoders are presented in Table 6. InferSent, developed by\n",
            "Facebook, is the best performing sentence-based encoder.\n",
            "Its version 2 uses a simple architecture composed of\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "131677\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "TABLE 8. Transformers results.\n",
            "\n",
            "fully connected dense layers which averages FastText word\n",
            "embeddings, thus outperforming Doc2Vec, Universal Sen-\n",
            "tence Encoder (USE), Skip-Thought-Vectors, and LASER.\n",
            "Additionally, InferSent outperforms the word encoder Fast-\n",
            "Text, which implies that the InferSent’s algorithm for averag-\n",
            "ing the word embeddings has superior efﬁciency for sentence\n",
            "context representation. Furthermore, we ﬁnd that the FastText\n",
            "version of InferSent outperforms the GloVe version of Inter-\n",
            "Sent. When using sentence vector representation, ML classi-\n",
            "\n",
            "ﬁers are more effective than CNN and a fully connected dense\n",
            "network.\n",
            "\n",
            "In Table 7, we present the results of the ﬁrst contextual\n",
            "word encoder, ELMo, which we evaluate in combination with\n",
            "ML classiﬁers (SVC, XGB) and DL classiﬁer models (Dense,\n",
            "CNN and RNN). ELMo embeddings outperform the evalu-\n",
            "ated word encoders with ﬁxed embeddings. This conﬁrms the\n",
            "hypothesis that contextual word vectors extract better features\n",
            "than the ﬁxed ones. Additionally, concatenated vectors of\n",
            "\n",
            "131678\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "FIGURE 9. Sentiment analysis models’ performances grouped by text representation method. On the chart, the top of the line for each model is the\n",
            "maximum, the bottom is the minimum and the white rectangle is the average performance per group. The numbers in the brackets represent the\n",
            "group of the text representation method, where (1) - lexicon-based methods, (2) - statistical methods, (3) - word encoders, (4) - sentence encoder,\n",
            "(5) NLP Transformer.\n",
            "\n",
            "words embeddings in combination with BiGRU network and\n",
            "an attention layer outperform the other ML and DL networks.\n",
            "We also evaluate the popular NLP transformers which sup-\n",
            "port text classiﬁcation. We ﬁne-tune them with training data\n",
            "in order to bias the embeddings towards ﬁnancial sentiment\n",
            "analysis. All transformer architectures outperform word and\n",
            "sentence encoders, as shown in Table 8. Hence, contextu-\n",
            "alized embeddings perform semantic tasks better than their\n",
            "non-contextualized counterparts. Among the family of BERT\n",
            "transformers, BERT-Large-uncased achieves the best score\n",
            "in classiﬁcation, with MCC=0.859. Although FinBERT was\n",
            "pre-trained on Reuters ﬁnancial texts, it does not perform as\n",
            "well as the other pre-trained versions of BERT, which use\n",
            "Wikipedia and BookCorpus as text corpora for pre-training.\n",
            "RoBERTa’s dynamic masking increases the efﬁciency of\n",
            "the BERT algorithm by 0.023. DistilBERT retains more than\n",
            "95% of the accuracy while having 40% fewer parameters.\n",
            "A distilled version of RoBERTa achieves as good results as\n",
            "BERT-large while using half the parameters of the teacher\n",
            "RoBERTa-base model. Among the ALBERT family of trans-\n",
            "formers, ALBERT-xxlarge pre-trained model outperforms\n",
            "\n",
            "the other ALBERT versions, obtaining MCC=0.881. Addi-\n",
            "tionally, ALBERT outperforms the BERT model. The\n",
            "cross-language model (XLM) also outperforms BERT and\n",
            "XLNet. XLM-MLM-en-2048 achieves the best result, with\n",
            "MCC=0.863, among all XLM versions. Finally, the latest\n",
            "NLP transformer, Facebook’s BART, outperforms all the\n",
            "other NLP transformers when applied to ﬁnance data, achiev-\n",
            "ing the best MCC score of 0.895.\n",
            "\n",
            "We show the performances of\n",
            "\n",
            "representation\n",
            "approaches in Table 2, while the performance of each method\n",
            "chronologically is shown in Fig. 9.\n",
            "\n",
            "text\n",
            "\n",
            "VII. CONCLUSION\n",
            "This paper presents a comprehensive chronological study\n",
            "of NLP-based methods for sentiment analysis in ﬁnance.\n",
            "The study begins with the lexicon-based approach, includes\n",
            "word and sentence encoders and concludes with recent NLP\n",
            "transformers. The NLP transformers show superior perfor-\n",
            "mances compared to the other evaluated approaches. The\n",
            "main progress in sentiment analysis accuracy is driven by\n",
            "the text representation methods, which feed the semantic\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "131679\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "meaning of the words and sentences into the models. The\n",
            "results achieved by the best models are comparable to expert’s\n",
            "opinion. The evaluations were performed on a relatively small\n",
            "dataset of approximately 2000 sentences. Even though the\n",
            "dataset is not large, we obtained good results, suggesting\n",
            "that this approach is appropriate for domains where large\n",
            "annotated data is not available.\n",
            "\n",
            "versions\n",
            "\n",
            "Distilled\n",
            "\n",
            "(Distilled-BERT and Distilled-\n",
            "RoBERTa) of NLP transformers achieve text classiﬁca-\n",
            "tion performances comparable to their large, uncompressed\n",
            "teacher models. Hence, they can be effectively used in text\n",
            "classiﬁcation production environments, where the need for\n",
            "light-weight, responsive, energy-efﬁcient and cost-saving\n",
            "models is essential.\n",
            "\n",
            "The results of this study can be applied in areas such\n",
            "as ﬁnance, where decision-making is based on senti-\n",
            "ment extraction from massive textual datasets. The ﬁnd-\n",
            "ings imply that selected models can be successfully used\n",
            "for forecasting stock market trends and corporate earnings,\n",
            "decision-making in securities trading and portfolio manage-\n",
            "ment, brand reputation management as well as fraud detection\n",
            "and regulation [87]–[89].\n",
            "\n",
            "Although this approach was constructed for sentiment\n",
            "analysis in the ﬁnance domain, it can be extended to other\n",
            "areas such as healthcare, legal and business analytics.\n",
            "\n",
            "APPENDIX A\n",
            "RESULTS\n",
            "See Tables 3–8.\n",
            "\n",
            "REFERENCES\n",
            "\n",
            "[1] A. Yenter and A. Verma, ‘‘Deep CNN-LSTM with combined kernels from\n",
            "multiple branches for IMDb review sentiment analysis,’’ in Proc. IEEE 8th\n",
            "Annu. Ubiquitous Comput., Electron. Mobile Commun. Conf. (UEMCON),\n",
            "Oct. 2017, pp. 540–546.\n",
            "\n",
            "[2] D. Cer, Y. Yang, S.-Y. Kong, N. Hua, N. Limtiaco, R. St. John, N. Constant,\n",
            "M. Guajardo-Cespedes, S. Yuan, C. Tar, Y.-H. Sung, B. Strope, and\n",
            "R. Kurzweil, ‘‘Universal sentence encoder,’’ 2018, arXiv:1803.11175.\n",
            "[Online]. Available: http://arxiv.org/abs/1803.11175\n",
            "\n",
            "[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\n",
            "of deep bidirectional transformers for language understanding,’’ 2018,\n",
            "arXiv:1810.04805. [Online]. Available: http://arxiv.org/abs/1810.04805\n",
            "[4] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\n",
            "L. Zettlemoyer, and V. Stoyanov, ‘‘RoBERTa: A robustly optimized\n",
            "BERT pretraining approach,’’ 2019, arXiv:1907.11692. [Online]. Avail-\n",
            "able: http://arxiv.org/abs/1907.11692\n",
            "\n",
            "[5] B. G. Malkiel, ‘‘The efﬁcient market hypothesis and its critics,’’ J. Econ.\n",
            "\n",
            "Perspect., vol. 17, no. 1, pp. 59–82, Feb. 2003.\n",
            "\n",
            "[6] M.-Y. Day and C.-C. Lee, ‘‘Deep learning for ﬁnancial sentiment analysis\n",
            "on ﬁnance news providers,’’ in Proc. IEEE/ACM Int. Conf. Adv. Social\n",
            "Netw. Anal. Mining (ASONAM), Aug. 2016, pp. 1127–1134.\n",
            "\n",
            "[7] L. Dodevska, V. Petreski, K. Mishev, A. Gjorgjevikj, I. Vodenska,\n",
            "L. Chitkushev, and D. Trajanov, ‘‘Predicting companies stock price direc-\n",
            "tion by using sentiment analysis of news articles,’’ in Proc. 15th Annu.\n",
            "Int. Conf. Comput. Sci. Educ. Comput. Sci., Fulda, Germany, Jul. 2019,\n",
            "pp. 37–42.\n",
            "\n",
            "[8] W. Souma, I. Vodenska, and H. Aoyama, ‘‘Enhanced news sentiment\n",
            "analysis using deep learning methods,’’ J. Comput. Social Sci., vol. 2, no. 1,\n",
            "pp. 33–46, Jan. 2019.\n",
            "\n",
            "[9] S. F. Crone and C. Koeppel, ‘‘Predicting exchange rates with sentiment\n",
            "indicators: An empirical evaluation using text mining and multilayer\n",
            "perceptrons,’’ in Proc. IEEE Conf. Comput. Intell. Financial Eng. Econ.\n",
            "(CIFEr), Mar. 2014, pp. 114–121.\n",
            "\n",
            "[10] C. Curme, H. E. Stanley, and I. Vodenska, ‘‘Coupled network approach\n",
            "to predictability of ﬁnancial market returns and news sentiments,’’ Int. J.\n",
            "Theor. Appl. Finance, vol. 18, no. 7, Nov. 2015, Art. no. 1550043.\n",
            "[11] K. Mishev, A. Gjorgjevikj, I. Vodenska, L. Chitkushev, W. Souma, and\n",
            "D. Trajanov, ‘‘Forecasting corporate revenue by using deep-learning\n",
            "methodologies,’’ in Proc. Int. Conf. Control, Artif. Intell., Robot. Optim.\n",
            "(ICCAIRO), May 2019, pp. 115–120.\n",
            "\n",
            "[12] T. Loughran and B. Mcdonald, ‘‘When is a liability not a liability? Textual\n",
            "analysis, dictionaries, and 10-ks,’’ J. Finance, vol. 66, no. 1, pp. 35–65,\n",
            "Feb. 2011.\n",
            "\n",
            "[13] M. Ghiassi, J. Skinner, and D. Zimbra, ‘‘Twitter brand sentiment analysis:\n",
            "A hybrid system using n-gram analysis and dynamic artiﬁcial neural\n",
            "network,’’ Expert Syst. Appl., vol. 40, no. 16, pp. 6266–6282, Nov. 2013.\n",
            "[14] N. Li, X. Liang, X. Li, C. Wang, and D. D. Wu, ‘‘Network environment\n",
            "and ﬁnancial risk using machine learning and sentiment analysis,’’ Hum.\n",
            "Ecological Risk Assessment, Int. J., vol. 15, no. 2, pp. 227–252, Apr. 2009.\n",
            "[15] G. Wang, T. Wang, B. Wang, D. Sambasivan, Z. Zhang, H. Zheng, and\n",
            "B. Y. Zhao, ‘‘Crowds on wall street: Extracting value from collaborative\n",
            "investing platforms,’’ in Proc. 18th ACM Conf. Comput. Supported Coop-\n",
            "erat. Work Social Comput. CSCW, 2015, pp. 17–30.\n",
            "\n",
            "[16] M. Atzeni, A. Dridi, and D. R. Recupero, ‘‘Fine-grained sentiment analysis\n",
            "on ﬁnancial microblogs and news headlines,’’ in Semantic Web Challenges.\n",
            "Cham, Switzerland: Springer, 2017, pp. 124–128.\n",
            "\n",
            "[17] S. Agaian and P. Kolm, ‘‘Financial sentiment analysis using machine\n",
            "learning techniques,’’ Int. J. Investment Manage. Financial Innov., vol. 3,\n",
            "pp. 1–9, 2017.\n",
            "\n",
            "[18] S. Sohangir, N. Petty, and D. Wang, ‘‘Financial sentiment lexicon analy-\n",
            "sis,’’ in Proc. IEEE 12th Int. Conf. Semantic Comput. (ICSC), Jan. 2018,\n",
            "pp. 286–289.\n",
            "\n",
            "[19] S. Sohangir, D. Wang, A. Pomeranets, and T. M. Khoshgoftaar, ‘‘Big data:\n",
            "Deep learning for ﬁnancial sentiment analysis,’’ J. Big Data, vol. 5, no. 1,\n",
            "p. 3, Dec. 2018.\n",
            "\n",
            "[20] L. Zhang, S. Wang, and B. Liu, ‘‘Deep learning for sentiment analysis: A\n",
            "survey,’’ Wiley Interdiscipl. Rev., Data Mining Knowl. Discovery, vol. 8,\n",
            "no. 4, 2018, Art. no. e1253.\n",
            "\n",
            "[21] D. Tang, B. Qin, and T. Liu, ‘‘Document modeling with gated recurrent\n",
            "neural network for sentiment classiﬁcation,’’ in Proc. Conf. Empirical\n",
            "Methods Natural Lang. Process., 2015, pp. 1422–1432.\n",
            "\n",
            "[22] K. Sheng Tai, R. Socher, and C. D. Manning, ‘‘Improved semantic repre-\n",
            "sentations from tree-structured long short-term memory networks,’’ 2015,\n",
            "arXiv:1503.00075. [Online]. Available: http://arxiv.org/abs/1503.00075\n",
            "\n",
            "[23] Y. Kim, ‘‘Convolutional neural networks for sentence classiﬁcation,’’\n",
            "2014, arXiv:1408.5882. [Online]. Available: http://arxiv.org/abs/1408.\n",
            "5882\n",
            "\n",
            "[24] X. Zhang, J. Zhao, and Y. LeCun, ‘‘Character-level convolutional networks\n",
            "for text classiﬁcation,’’ in Proc. Adv. Neural Inf. Process. Syst., 2015,\n",
            "pp. 649–657.\n",
            "\n",
            "[25] R. Johnson and T. Zhang, ‘‘Deep pyramid convolutional neural networks\n",
            "for text categorization,’’ in Proc. 55th Annu. Meeting Assoc. Comput.\n",
            "Linguistics (Long Papers), vol. 1, 2017, pp. 562–570.\n",
            "\n",
            "[26] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, ‘‘Hierarchical\n",
            "attention networks for document classiﬁcation,’’ in Proc. Conf. North\n",
            "Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol., 2016,\n",
            "pp. 1480–1489.\n",
            "\n",
            "[27] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, ‘‘Distributed\n",
            "representations of words and phrases and their compositionality,’’ in Proc.\n",
            "Adv. Neural Inf. Process. Syst., 2013, pp. 3111–3119.\n",
            "\n",
            "[28] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\n",
            "word representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\n",
            "Process. (EMNLP), 2014, pp. 1532–1543.\n",
            "\n",
            "[29] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, ‘‘Enriching word\n",
            "vectors with subword information,’’ Trans. Assoc. Comput. Linguistics,\n",
            "vol. 5, pp. 135–146, Dec. 2017.\n",
            "\n",
            "[30] Q. Le and T. Mikolov, ‘‘Distributed representations of sentences and\n",
            "documents,’’ in Proc. Int. Conf. Mach. Learn., 2014, pp. 1188–1196.\n",
            "[31] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba,\n",
            "and S. Fidler, ‘‘Skip-thought vectors,’’ in Proc. Adv. Neural Inf. Process.\n",
            "Syst., 2015, pp. 3294–3302.\n",
            "\n",
            "[32] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes, ‘‘Super-\n",
            "vised learning of universal sentence representations from natural lan-\n",
            "guage inference data,’’ 2017, arXiv:1705.02364. [Online]. Available:\n",
            "http://arxiv.org/abs/1705.02364\n",
            "\n",
            "131680\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "[33] M. Artetxe and H. Schwenk, ‘‘Massively multilingual sentence embed-\n",
            "dings for zero-shot cross-lingual transfer and beyond,’’ Trans. Assoc. Com-\n",
            "put. Linguistics, vol. 7, pp. 597–610, Mar. 2019.\n",
            "\n",
            "[34] X. Man, T. Luo, and J. Lin, ‘‘Financial sentiment analysis(FSA): A sur-\n",
            "vey,’’ in Proc. IEEE Int. Conf. Ind. Cyber Phys. Syst. (ICPS), May 2019,\n",
            "pp. 617–622.\n",
            "\n",
            "[35] S. Yang, J. Rosenfeld, and J. Makutonin, ‘‘Financial aspect-based sen-\n",
            "timent analysis using deep representations,’’ 2018, arXiv:1808.07931.\n",
            "[Online]. Available: http://arxiv.org/abs/1808.07931\n",
            "\n",
            "[36] C.-H. Du, M.-F. Tsai, and C.-J. Wang,\n",
            "\n",
            "to\n",
            "sentence-level sentiment analysis for ﬁnancial reports,’’ in Proc. IEEE\n",
            "(ICASSP), May 2019,\n",
            "Int. Conf. Acoust., Speech Signal Process.\n",
            "pp. 1562–1566.\n",
            "\n",
            "‘‘Beyond word-level\n",
            "\n",
            "[37] L. Zhao, L. Li, and X. Zheng, ‘‘A BERT based sentiment analysis\n",
            "and key entity detection approach for online ﬁnancial texts,’’ 2020,\n",
            "arXiv:2001.05326. [Online]. Available: http://arxiv.org/abs/2001.05326\n",
            "\n",
            "[38] J. Howard and S. Ruder, ‘‘Universal language model ﬁne-tuning for\n",
            "text classiﬁcation,’’ 2018, arXiv:1801.06146. [Online]. Available: http://\n",
            "arxiv.org/abs/1801.06146\n",
            "\n",
            "[39] P. J. Stone, D. C. Dunphy, and M. S. Smith, The General Inquirer: A\n",
            "Computer Approach to Content Analysis. Oxford, U.K.: MIT Press, 1966.\n",
            "[40] J. L. Rogers, A. Van Buskirk, and S. L. C. Zechman, ‘‘Disclosure tone and\n",
            "shareholder litigation,’’ Accounting Rev., vol. 86, no. 6, pp. 2155–2183,\n",
            "Nov. 2011.\n",
            "\n",
            "[41] A. K. Davis, W. Ge, D. Matsumoto, and J. L. Zhang, ‘‘The effect of\n",
            "manager-speciﬁc optimism on the tone of earnings conference calls,’’ Rev.\n",
            "Accounting Stud., vol. 20, no. 2, pp. 639–673, Jun. 2015.\n",
            "\n",
            "[42] W. Zhang and S. Skiena, ‘‘Trading strategies to exploit blog and news\n",
            "sentiment,’’ in Proc. 4th Int. AAAI Conf. Weblogs Social Media, May 2010,\n",
            "pp. 1–4.\n",
            "\n",
            "[43] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‘‘Efﬁcient estimation of\n",
            "word representations in vector space,’’ 2013, arXiv:1301.3781. [Online].\n",
            "Available: http://arxiv.org/abs/1301.3781\n",
            "\n",
            "[44] Z. S. Harris,\n",
            "\n",
            "‘‘Distributional structure,’’ Word, vol. 10, nos. 2–3,\n",
            "\n",
            "pp. 146–162, Aug. 1954.\n",
            "\n",
            "[45] T. K. Landauer and S. T. Dumais, ‘‘A solution to Plato’s problem: The latent\n",
            "semantic analysis theory of acquisition, induction, and representation of\n",
            "knowledge.,’’ Psychol. Rev., vol. 104, no. 2, p. 211, 1997.\n",
            "\n",
            "[46] M. Sahlgren, ‘‘The word-space model: Using distributional analysis\n",
            "to represent syntagmatic and paradigmatic relations between words in\n",
            "high-dimensional vector spaces,’’ Ph.D. dissertation, Dept. Comput. Lin-\n",
            "guistics, Stockholm Univ., Stockholm, Sweden, 2006.\n",
            "\n",
            "[47] P. D. Turney and P. Pantel, ‘‘From frequency to meaning: Vector space\n",
            "models of semantics,’’ J. Artif. Intell. Res., vol. 37, pp. 141–188, Feb. 2010.\n",
            "[48] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, ‘‘Bag of tricks for\n",
            "efﬁcient text classiﬁcation,’’ in Proc. 15th Conf. Eur. Chapter Assoc.\n",
            "Comput. Linguistics, Short Papers, vol. 2, 2017, pp. 427–431.\n",
            "\n",
            "[49] Y. Sharma, G. Agrawal, P. Jain, and T. Kumar, ‘‘Vector representation\n",
            "of words for sentiment analysis using GloVe,’’ in Proc. Int. Conf. Intell.\n",
            "Commun. Comput. Techn. (ICCT), Dec. 2017, pp. 279–284.\n",
            "\n",
            "[50] P. Lauren, G. Qu, J. Yang, P. Watta, G.-B. Huang, and A. Lendasse,\n",
            "‘‘Generating word embeddings from an extreme learning machine for\n",
            "sentiment analysis and sequence labeling tasks,’’ Cognit. Comput., vol. 10,\n",
            "no. 4, pp. 625–638, Aug. 2018.\n",
            "\n",
            "[51] S. M. Rezaeinia, R. Rahmani, A. Ghodsi, and H. Veisi, ‘‘Sentiment analysis\n",
            "based on improved pre-trained word embeddings,’’ Expert Syst. Appl.,\n",
            "vol. 117, pp. 139–147, Mar. 2019.\n",
            "\n",
            "[52] T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin, ‘‘Advances\n",
            "in pre-training distributed word representations,’’ 2017, arXiv:1712.09405.\n",
            "[Online]. Available: http://arxiv.org/abs/1712.09405\n",
            "\n",
            "[53] R. Velioglu, T. Yildiz, and S. Yildirim, ‘‘Sentiment analysis using learning\n",
            "approaches over emojis for turkish tweets,’’ in Proc. 3rd Int. Conf. Comput.\n",
            "Sci. Eng. (UBMK), Sep. 2018, pp. 303–307.\n",
            "\n",
            "[54] A. A. Altowayan and A. Elnagar, ‘‘Improving arabic sentiment analysis\n",
            "with sentiment-speciﬁc embeddings,’’ in Proc. IEEE Int. Conf. Big Data\n",
            "(Big Data), Dec. 2017, pp. 4314–4320.\n",
            "\n",
            "[55] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\n",
            "and L. Zettlemoyer, ‘‘Deep contextualized word representations,’’ 2018,\n",
            "arXiv:1802.05365. [Online]. Available: http://arxiv.org/abs/1802.05365\n",
            "\n",
            "[56] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, ‘‘Empirical evalua-\n",
            "tion of gated recurrent neural networks on sequence modeling,’’ 2014,\n",
            "arXiv:1412.3555. [Online]. Available: http://arxiv.org/abs/1412.3555\n",
            "\n",
            "[57] L. Logeswaran and H. Lee, ‘‘An efﬁcient framework for learning sen-\n",
            "tence representations,’’ 2018, arXiv:1803.02893. [Online]. Available:\n",
            "http://arxiv.org/abs/1803.02893\n",
            "\n",
            "[58] A. Akbik, D. Blythe, and R. Vollgraf, ‘‘Contextual string embeddings for\n",
            "sequence labeling,’’ in Proc. 27th Int. Conf. Comput. Linguistics, 2018,\n",
            "pp. 1638–1649.\n",
            "\n",
            "[59] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n",
            "L. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\n",
            "Neural Inf. Process. Syst., 2017, pp. 5998–6008.\n",
            "\n",
            "[60] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,\n",
            "and S. Fidler, ‘‘Aligning books and movies: Towards story-like visual\n",
            "explanations by watching movies and reading books,’’ in Proc. IEEE Int.\n",
            "Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 19–27.\n",
            "\n",
            "[61] D. Araci, ‘‘FinBERT: Financial sentiment analysis with pre-trained lan-\n",
            "guage models,’’ 2019, arXiv:1908.10063. [Online]. Available: http://arxiv.\n",
            "org/abs/1908.10063\n",
            "\n",
            "[62] G. Lample and A. Conneau, ‘‘Cross-lingual language model pretrain-\n",
            "ing,’’ 2019, arXiv:1901.07291. [Online]. Available: http://arxiv.org/abs/\n",
            "1901.07291\n",
            "\n",
            "[63] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n",
            "‘‘ALBERT: A lite BERT for self-supervised learning of language rep-\n",
            "resentations,’’ 2019, arXiv:1909.11942. [Online]. Available: http://arxiv.\n",
            "org/abs/1909.11942\n",
            "\n",
            "for\n",
            "\n",
            "[64] M. A. Al-Garadi, Y.-C. Yang, H. Cai, Y. Ruan, K. O’Connor,\n",
            "G. Gonzalez-Hernandez, J. Perrone, and A. Sarker, Text Classiﬁcation\n",
            "Models\n",
            "the Automatic Detection of Nonmedical Prescription\n",
            "Medication Use from Social Media. Cold Spring Harbor Laboratory\n",
            "Press, 2020. [Online]. Available: https://www.medrxiv.org/content/early/\n",
            "2020/04/17/2020.04.13.20064089.full.pdf,\n",
            "10.1101/2020.04.13.\n",
            "20064089.\n",
            "\n",
            "doi:\n",
            "\n",
            "[65] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, ‘‘DistilBERT, a dis-\n",
            "tilled version of BERT: Smaller, faster, cheaper and lighter,’’ 2019,\n",
            "arXiv:1910.01108. [Online]. Available: http://arxiv.org/abs/1910.01108\n",
            "\n",
            "[66] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek,\n",
            "F. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov,\n",
            "‘‘Unsupervised cross-lingual representation learning at scale,’’ 2019,\n",
            "arXiv:1911.02116. [Online]. Available: http://arxiv.org/abs/1911.02116\n",
            "\n",
            "[67] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\n",
            "V. Stoyanov, and L. Zettlemoyer,\n",
            "‘‘BART: Denoising sequence-to-\n",
            "sequence pre-training for natural language generation, translation, and\n",
            "comprehension,’’ 2019, arXiv:1910.13461. [Online]. Available: http://\n",
            "arxiv.org/abs/1910.13461\n",
            "\n",
            "[68] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n",
            "‘‘Language models are unsupervised multitask learners,’’ OpenAI Blog,\n",
            "vol. 1, no. 8, p. 9, 2019.\n",
            "\n",
            "[69] P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala, ‘‘Good debt\n",
            "or bad debt: Detecting semantic orientations in economic texts,’’ J. Assoc.\n",
            "Inf. Sci. Technol., vol. 65, no. 4, pp. 782–796, Apr. 2014.\n",
            "\n",
            "[70] K. Cortis, A. Freitas, T. Daudert, M. Huerlimann, M. Zarrouk, S. Hand-\n",
            "schuh, and B. Davis, ‘‘SemEval-2017 task 5: Fine-grained sentiment\n",
            "analysis on ﬁnancial microblogs and news,’’ in Proc. 11th Int. Workshop\n",
            "Semantic Eval. (SemEval-), 2017, pp. 519–535.\n",
            "\n",
            "[71] F. Chollet. (2015). Keras. [Online]. Available: https://keras.io\n",
            "[72] T. Wolf et al., ‘‘HuggingFace’s transformers: State-of-the-art natural lan-\n",
            "guage processing,’’ 2019, arXiv:1910.03771. [Online]. Available: http://\n",
            "arxiv.org/abs/1910.03771\n",
            "\n",
            "[73] J. H. Friedman, ‘‘Greedy function approximation: A gradient boosting\n",
            "\n",
            "machine,’’ Ann. Statist., vol. 29, no. 5, pp. 1189–1232, 2001.\n",
            "\n",
            "[74] T. Chen and C. Guestrin, ‘‘XGBoost: A scalable tree boosting sys-\n",
            "in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery\n",
            "tem,’’\n",
            "Data Mining (KDD), San Francisco, CA, USA. New York, NY, USA:\n",
            "ACM, 2016, pp. 785–794. [Online]. Available: http://doi.acm.org/10.1145/\n",
            "2939672.2939785, doi: 10.1145/2939672.2939785.\n",
            "\n",
            "[75] L. Deng and D. Yu, ‘‘Deep learning: Methods and applications,’’\n",
            "Found. Trends Signal Process., vol. 7, nos. 3–4, pp. 197–387,\n",
            "Jun. 2014.\n",
            "\n",
            "[76] D. Yu and L. Deng, ‘‘Deep learning and its applications to signal and\n",
            "information processing [exploratory DSP],’’ IEEE Signal Process. Mag.,\n",
            "vol. 28, no. 1, pp. 145–154, Jan. 2011.\n",
            "\n",
            "[77] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, ‘‘Deep\n",
            "learning for computer vision: A brief review,’’ Comput. Intell. Neurosci.,\n",
            "vol. 2018, pp. 1–13, Feb. 2018.\n",
            "\n",
            "[78] L. Deng, G. Hinton, and B. Kingsbury, ‘‘New types of deep neural network\n",
            "learning for speech recognition and related applications: An overview,’’\n",
            "in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., May 2013,\n",
            "pp. 8599–8603.\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "131681\n",
            "\n",
            "\fK. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\n",
            "\n",
            "[79] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen,\n",
            "Y. Zhang, Y. Wang, R. Skerrv-Ryan, R. A. Saurous, Y. Agiomvrgiannakis,\n",
            "and Y. Wu, ‘‘Natural TTS synthesis by conditioning wavenet on MEL\n",
            "spectrogram predictions,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal\n",
            "Process. (ICASSP), Apr. 2018, pp. 4779–4783.\n",
            "\n",
            "[80] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang,\n",
            "J. Raiman, and J. Miller, ‘‘Deep voice 3: Scaling text-to-speech with convo-\n",
            "lutional sequence learning,’’ 2017, arXiv:1710.07654. [Online]. Available:\n",
            "http://arxiv.org/abs/1710.07654\n",
            "\n",
            "[81] G. Liu and J. Guo, ‘‘Bidirectional LSTM with attention mechanism and\n",
            "convolutional layer for text classiﬁcation,’’ Neurocomputing, vol. 337,\n",
            "pp. 325–338, Apr. 2019.\n",
            "\n",
            "[82] P. Liu, X. Qiu, and X. Huang, ‘‘Recurrent neural network for text clas-\n",
            "siﬁcation with multi-task learning,’’ 2016, arXiv:1605.05101. [Online].\n",
            "Available: http://arxiv.org/abs/1605.05101\n",
            "\n",
            "[83] D. Bahdanau, K. Cho, and Y. Bengio, ‘‘Neural machine translation by\n",
            "jointly learning to align and translate,’’ 2014, arXiv:1409.0473. [Online].\n",
            "Available: http://arxiv.org/abs/1409.0473\n",
            "\n",
            "[84] K. Mishev et al., ‘‘Performance evaluation of word and sentence embed-\n",
            "dings for ﬁnance headlines sentiment analysis,’’ in ICT Innovations 2019.\n",
            "Big Data Processing and Mining (Communications in Computer and\n",
            "Information Science), vol. 1110, S. Gievska and G. Madjarov, Eds. Cham,\n",
            "Switzerland: Springer, 2019, pp. 161–172.\n",
            "\n",
            "[85] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimiza-\n",
            "tion,’’ 2014, arXiv:1412.6980. [Online]. Available: http://arxiv.org/abs/\n",
            "1412.6980\n",
            "\n",
            "[86] N. Srivastava, G. Hinton, A. Krizhevsky,\n",
            "\n",
            "and\n",
            "R. Salakhutdinov, ‘‘Dropout: A simple way to prevent neural networks\n",
            "from overﬁtting,’’ J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958,\n",
            "2014.\n",
            "\n",
            "I. Sutskever,\n",
            "\n",
            "[87] T. Rao and S. Srivastava, ‘‘Analyzing stock market movements using\n",
            "\n",
            "twitter sentiment analysis,’’ Tech. Rep., 2012.\n",
            "\n",
            "[88] J. Smailović, M. Grčar, N. Lavrač, and M. Žnidaršič, ‘‘Predictive sentiment\n",
            "analysis of tweets: A stock market application,’’ in Proc. Int. Workshop\n",
            "Hum.-Comput. Interact. Knowl. Discovery Complex, Unstructured, Big\n",
            "Data. Berlin, Germany: Springer, 2013, pp. 77–88.\n",
            "\n",
            "[89] X. Li, H. Xie, L. Chen, J. Wang, and X. Deng, ‘‘News impact on stock price\n",
            "return via sentiment analysis,’’ Knowl.-Based Syst., vol. 69, pp. 14–23,\n",
            "Oct. 2014.\n",
            "\n",
            "KOSTADIN MISHEV received the bachelor’s\n",
            "degree in informatics and computer engineering\n",
            "and the master’s degree in computer networks\n",
            "and e-technologies degree from Saints Cyril and\n",
            "Methodius University, Skopje, in 2013 and 2016,\n",
            "respectively, where he is currently pursuing the\n",
            "Ph.D. degree. He is also a Teaching and a Research\n",
            "Assistant with the Faculty of Computer Science\n",
            "and Engineering, Saints Cyril and Methodius Uni-\n",
            "versity. His research interests include data science,\n",
            "natural language processing, semantic Web, enterprise application architec-\n",
            "tures, Web technologies, and computer networks.\n",
            "\n",
            "received the bachelor’s\n",
            "ANA GJORGJEVIKJ\n",
            "degree in computer science and engineering and\n",
            "the master’s degree in computer networks and\n",
            "e-technologies from Saints Cyril and Methodius\n",
            "University, Skopje, in 2010 and 2014, respectively,\n",
            "where she is currently pursuing the Ph.D. degree in\n",
            "the domain of data science, with particular focus\n",
            "on deep learning and natural language processing.\n",
            "She has been working as a Software Engineer,\n",
            "since 2010. Her research interests include data\n",
            "science, machine learning, natural language processing, and knowledge\n",
            "representation.\n",
            "\n",
            "IRENA VODENSKA received the B.S. degree in\n",
            "computer information systems from the University\n",
            "of Belgrade, the Ph.D. degree in econophysics (sta-\n",
            "tistical ﬁnance) from Boston University, and the\n",
            "M.B.A. degree from the Owen Graduate School\n",
            "of Management, Vanderbilt University. She is cur-\n",
            "rently an Associate Professor of ﬁnance and the\n",
            "Director of ﬁnance programs with the Boston\n",
            "University’s Metropolitan College. Her research\n",
            "focuses on network theory and complexity science\n",
            "in macroeconomics. She conducts a theoretical and applied interdisciplinary\n",
            "research using quantitative approaches for modeling interdependences of\n",
            "ﬁnancial networks, banking system dynamics, and global ﬁnancial crises.\n",
            "More speciﬁcally, her research focuses on modeling of early warning indi-\n",
            "cators and systemic risk propagation throughout interconnected ﬁnancial\n",
            "and economic networks. She also studies the effects of news announcement\n",
            "on ﬁnancial markets, corporations, ﬁnancial institutions, and related global\n",
            "economic systems. She uses neural networks and deep learning methodolo-\n",
            "gies for natural language processing to text mine important factors affecting\n",
            "corporate performance and global economic trends. She teaches Invest-\n",
            "ment Analysis and Portfolio Management, International Finance and Trade,\n",
            "Financial Regulation and Ethics, and Derivatives Securities and Markets at\n",
            "Boston University. She is also a Chartered Financial Analyst (CFA) charter\n",
            "holder. As a Principal Investigator (PI) for Boston University, she has won\n",
            "interdisciplinary research grants awarded by the European Commission, EU,\n",
            "U.S. Army Research Ofﬁce, and the National Science Foundation, USA.\n",
            "\n",
            "LUBOMIR T. CHITKUSHEV received the\n",
            "Dipl.Ing. degree in electrical engineering from\n",
            "the University of Belgrade, the M.Sc. degree in\n",
            "biomedical engineering from the Medical Col-\n",
            "lege of Virginia, VCU, and the Ph.D. degree in\n",
            "biomedical engineering from Boston University.\n",
            "He is currently an Associate Professor of computer\n",
            "science with the Boston University’s Metropolitan\n",
            "College, where he serves as the Director of health\n",
            "informatics and health sciences and the Associate\n",
            "Dean of academic affairs. His research activity is focused on modeling of\n",
            "complex systems, computer network security and architecture, and biomed-\n",
            "ical and health informatics. He is also the founder of the Health Informatics\n",
            "Program, Boston University, and a founding member of Boston University’s\n",
            "RINA Laboratory, where Recursive Inter-Network Architecture (RINA) was\n",
            "introduced as efﬁcient, scalable, and secure approach to Internet architecture.\n",
            "He is also a Co-Founder and the Associate Director of the Center for Reliable\n",
            "Information Systems and Cyber Security (RISCS), Boston University, which\n",
            "coordinates research on reliable and secure computational systems and\n",
            "infrastructure and information assurance education. He has served as the\n",
            "Principal Investigator for Boston University on research grants awarded by\n",
            "the European Commission, EU, the National Security Agency, USA, and the\n",
            "U.S. Department of Justice. He has also served as a Reviewer with the U.S.\n",
            "National Science Foundation.\n",
            "\n",
            "DIMITAR TRAJANOV (Member, IEEE) received\n",
            "the Ph.D. degree. He is currently a Professor\n",
            "and the Head of the Department of Informa-\n",
            "tion Systems and Network Technologies, Faculty\n",
            "of Computer Science and Engineering, Ss. Cyril\n",
            "and Methodius University, Skopje. He is also the\n",
            "Leader of Regional Social Innovation Hub estab-\n",
            "lished, in 2013, as a co-operation between UNDP\n",
            "and the Faculty of Computer Science and Engi-\n",
            "neering. He is the author of more than 150 journal\n",
            "and conference papers and seven books. He has been involved in more\n",
            "than 60 research and industrial projects, mostly as the Project Leader. His\n",
            "research interests include data science, machine learning, NLP, FinTech,\n",
            "semantic Web, open data, sharing economy, social innovation, e-commerce,\n",
            "entrepreneurship, technology for development, mobile development, and\n",
            "climate change.\n",
            "\n",
            "131682\n",
            "\n",
            "VOLUME 8, 2020\n",
            "\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type of texts formates[LTTextBox, LTFigure, LTLine, LTRect or an LTImage]"
      ],
      "metadata": {
        "id": "0qY7IjzFAU1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layout Analysis\n",
        "from pdfminer.high_level import extract_pages\n",
        "path = \"/content/drive/MyDrive/297 NLP/Evaluation_of_Sentiment_Analysis_in_Finance_From_Lexicons_to_Transformers.pdf\"\n",
        "for page_layout in extract_pages(path):\n",
        "    for element in page_layout:\n",
        "        print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tUeewLt9Y6K",
        "outputId": "bc1cb43d-257f-4d82-f14c-e3960dcf7c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<LTTextBoxHorizontal(0) 36.170,711.591,405.297,718.565 'Received June 13, 2020, accepted July 1, 2020, date of publication July 16, 2020, date of current version July 29, 2020.\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,699.066,175.571,705.044 'Digital Object Identifier 10.1109/ACCESS.2020.3009626\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,624.238,416.009,671.461 'Evaluation of Sentiment Analysis in Finance:\\nFrom Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,570.476,458.857,609.124 'KOSTADIN MISHEV 1, ANA GJORGJEVIKJ 1, IRENA VODENSKA2, LUBOMIR T. CHITKUSHEV2,\\nAND DIMITAR TRAJANOV 1, (Member, IEEE)\\n1Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, 1000 Skopje, North Macedonia\\n2Financial Informatics Lab, Metropolitan College, Boston University, Boston, MA 02215, USA\\n'>\n",
            "<LTTextBoxHorizontal(4) 36.170,558.198,267.305,565.869 'Corresponding author: Kostadin Mishev (kostadin.mishev@ﬁnki.ukim.mk)\\n'>\n",
            "<LTTextBoxHorizontal(5) 36.170,543.323,438.782,550.994 'This work was supported in part by the Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, Skopje.\\n'>\n",
            "<LTTextBoxHorizontal(6) 43.422,312.221,469.881,513.467 'ABSTRACT Financial and economic news is continuously monitored by ﬁnancial market participants.\\nAccording to the efﬁcient market hypothesis, all past information is reﬂected in stock prices and new infor-\\nmation is instantaneously absorbed in determining future stock prices. Hence, prompt extraction of positive\\nor negative sentiments from news is very important for investment decision-making by traders, portfolio\\nmanagers and investors. Sentiment analysis models can provide an efﬁcient method for extracting actionable\\nsignals from the news. However, ﬁnancial sentiment analysis is challenging due to domain-speciﬁc language\\nand unavailability of large labeled datasets. General sentiment analysis models are ineffective when applied\\nto speciﬁc domains such as ﬁnance. To overcome these challenges, we design an evaluation platform\\nwhich we use to assess the effectiveness and performance of various sentiment analysis approaches, based\\non combinations of text representation methods and machine-learning classiﬁers. We perform more than\\none hundred experiments using publicly available datasets, labeled by ﬁnancial experts. We start the\\nevaluation with speciﬁc lexicons for sentiment analysis in ﬁnance and gradually build the study to include\\nword and sentence encoders, up to the latest available NLP transformers. The results show improved\\nefﬁciency of contextual embeddings in sentiment analysis compared to lexicons and ﬁxed word and sentence\\nencoders, even when large datasets are not available. Furthermore, distilled versions of NLP transformers\\nproduce comparable results to their larger teacher models, which makes them suitable for use in production\\nenvironments.\\n'>\n",
            "<LTTextBoxHorizontal(7) 43.422,272.183,469.879,294.101 'INDEX TERMS Sentiment analysis, ﬁnance, natural language processing, text representations, deep-\\nlearning, encoders, word embedding, sentence embedding, transfer-learning, transformers, survey.\\n'>\n",
            "<LTTextBoxHorizontal(8) 36.170,95.780,277.384,248.112 'I. INTRODUCTION\\nThe latest advances in Natural Language Processing (NLP)\\nhave received signiﬁcant attention due to their efﬁciency\\nin language modeling. These language models are ﬁnding\\napplications in various industries as they provide powerful\\nmechanisms for real-time, reliable, and semantic-oriented\\ntext analysis. Sentiment analysis is one of the NLP tasks that\\nleverages language modeling advancements and is achiev-\\ning improved results. According to the Oxford University\\nPress dictionary,1 sentiment analysis is deﬁned as the pro-\\ncess of computationally identifying and categorizing opin-\\nions expressed in a text, primarily to determine whether\\nthe writer’s attitude towards a particular topic or product is\\n'>\n",
            "<LTTextBoxHorizontal(9) 44.140,60.283,277.374,80.074 '1https://lexico.com\\nThe associate editor coordinating the review of this manuscript and\\n'>\n",
            "<LTTextBoxHorizontal(10) 36.170,47.647,184.516,55.617 'approving it for publication was K. C. Santosh\\n'>\n",
            "<LTTextBoxHorizontal(11) 192.618,47.647,194.611,55.617 '.\\n'>\n",
            "<LTTextBoxHorizontal(12) 297.079,215.333,538.293,249.206 'positive, negative, or neutral. Sentiment analysis is becoming\\nan essential tool for transforming emotions and attitudes into\\nactionable information.\\n'>\n",
            "<LTTextBoxHorizontal(13) 297.079,107.736,538.293,213.341 'Designing and building deep-learning-based sentiment\\nanalysis models require substantial datasets for training and\\ntesting. While there are several large, publicly available\\nsentiment-annotated datasets, they are mostly related to prod-\\nucts and movies. Many sentiment analysis models [1]–[4]\\nuse these datasets and achieve good performance in related\\ndomains. However, the application of these models in differ-\\nent domains is challenging because each domain has a unique\\nset of words for emotion expression.\\n'>\n",
            "<LTTextBoxHorizontal(14) 297.079,47.960,538.293,105.744 'The ﬁnancial domain is characterized by a unique vocab-\\nulary, which calls for domain-speciﬁc sentiment analysis.\\nPrices observed in ﬁnancial markets reﬂect all available\\ninformation related to traded assets [5], hence new informa-\\ntion allows stakeholders to make well-informed and timely\\n'>\n",
            "<LTTextBoxHorizontal(15) 36.170,24.831,60.565,31.805 '131662\\n'>\n",
            "<LTTextBoxHorizontal(16) 92.568,28.663,481.915,34.641 'This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\\n'>\n",
            "<LTTextBoxHorizontal(17) 492.714,25.077,538.290,31.055 'VOLUME 8, 2020\\n'>\n",
            "<LTFigure(Fm0) 36.170,742.649,128.170,765.649 matrix=[1.00,0.00,0.00,1.00, (36.17,742.65)]>\n",
            "<LTLine 36.170,737.138,538.287,737.138>\n",
            "<LTFigure(Fm1) 122.915,602.911,131.015,610.827 matrix=[1.00,0.00,0.00,1.00, (122.92,602.91)]>\n",
            "<LTFigure(Fm1) 224.038,602.911,232.138,610.827 matrix=[1.00,0.00,0.00,1.00, (224.04,602.91)]>\n",
            "<LTFigure(Fm1) 150.886,590.832,158.986,598.748 matrix=[1.00,0.00,0.00,1.00, (150.89,590.83)]>\n",
            "<LTFigure(Fm2) 36.170,503.626,39.170,516.626 matrix=[1.00,0.00,0.00,1.00, (36.17,503.63)]>\n",
            "<LTFigure(Fm2) 36.170,284.261,39.170,297.261 matrix=[1.00,0.00,0.00,1.00, (36.17,284.26)]>\n",
            "<LTFigure(Fm1) 184.518,52.364,192.618,60.280 matrix=[1.00,0.00,0.00,1.00, (184.52,52.36)]>\n",
            "<LTTextBoxHorizontal(0) 36.170,749.978,321.998,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,492.544,277.384,717.700 'decisions. The sentiments expressed in news and tweets inﬂu-\\nence stock prices and brand reputation, hence, constant mea-\\nsurement and tracking of these sentiments is becoming one of\\nthe most important activities for investors. Studies have used\\nsentiment analysis based on ﬁnancial news to forecast stock\\nprices [6]–[8], foreign exchange and global ﬁnancial market\\ntrends [9], [10] as well as to predict corporate earnings [11].\\nGiven that the ﬁnancial sector uses its own jargon, it is\\nnot suitable to apply generic sentiment analysis in ﬁnance\\nbecause many of the words differ from their general meaning.\\nFor example, ‘‘liability’’ is generally a negative word, but\\nin the ﬁnancial domain it has a neutral meaning. The term\\n‘‘share’’ usually has a positive meaning, but in the ﬁnancial\\ndomain, share represents a ﬁnancial asset or a stock, which\\nis a neutral word. Furthermore, ‘‘bull’’ is neutral in general,\\nbut in ﬁnance, it is strictly positive, while ‘‘bear’’ is neutral in\\ngeneral, but negative in ﬁnance. These examples emphasize\\nthe need for development of dedicated models, which will\\nextract sentiments from ﬁnancial texts.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,337.126,277.384,490.551 'Sentiment analysis in ﬁnance has become an important\\nresearch topic, connecting quantitative and qualitative mea-\\nsures of ﬁnancial performance. A seminal study by Loughran\\nand McDonald [12] shows that word lists developed for\\nother disciplines misclassify common words in ﬁnancial\\ntexts. Hence, Loughran and McDonald created an expert\\nannotated lexicon of positive, negative, and neutral words\\nin ﬁnance, which better reﬂect sentiments in ﬁnancial texts.\\nIn [13], the authors introduce a Twitter-speciﬁc lexicon,\\nwhich, in combination with the DAN2 machine learning\\napproach, produces more accurate sentiment classiﬁcation\\nresults than support vector machine (SVM) approach while\\nusing the same Twitter-speciﬁc lexicon.\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,205.619,277.384,335.134 'Machine learning methods for sentiment extraction have\\nbeen applied on datasets of tweets or news [14]–[18]. In [15],\\nthe authors use various machine-learning binary classiﬁers\\nto obtain StockTwits tweets sentiments. They show that the\\nSVM classiﬁer is more accurate compared to Decision Trees\\nand Naïve Bayes classiﬁer. In [16], Atzeni et al. test the\\nperformance of various regression models in combination\\nwith statistical and semantic methods for feature extraction\\nto predict a real-valued sentiment score in micro-blogs and\\nnews headlines, and show that semantic methods improve\\nclassiﬁcation accuracy.\\n'>\n",
            "<LTTextBoxHorizontal(4) 36.170,98.023,277.384,203.627 'Researchers have used lexicon-based approaches in com-\\nbination with machine-learning models. The authors in [18]\\nshow that such combinations are more efﬁcient for senti-\\nment extraction than using single models. However, regular\\nmachine-learning methods are unable to extract complex fea-\\ntures and to keep the order of words in a sentence. These tasks\\nrequire the use of deep-learning approaches, which allow for\\ncomplex feature extraction, location identiﬁcation, and order\\ninformation [19].\\n'>\n",
            "<LTTextBoxHorizontal(5) 36.170,50.202,277.384,96.031 'Deep-learning methods [20] use a cascade of multiple lay-\\ners of non-linear processing units for complex feature extrac-\\ntion and transformation. Each successive layer uses the output\\nfrom the previous layer as input, thus extracting complex\\n'>\n",
            "<LTTextBoxHorizontal(6) 297.079,552.319,538.293,717.699 'features which in many cases can be useful for generating\\nlearning patterns and relationships beyond immediate neigh-\\nbors in the sequence. Many studies conﬁrm the efﬁciency\\nof deep-learning models, including recurrent neural network\\n(RNN) [21], [22], convolutional neural networks [23]–[25]\\nand attention mechanism [19], [26] in sentiment extraction\\nin ﬁnance. The great success of deep-learning approaches\\nin NLP is mainly due to the introduction and improvement\\nof text representation methods, such as word [27]–[29] and\\nsentence encoders [30]–[33]. These convert words/sentences\\ninto vector representation, making them suitable as input for\\nneural networks. These representations keep the semantic\\ninformation coded into words and sentences, which is crucial\\nfor sentiment extraction.\\n'>\n",
            "<LTTextBoxHorizontal(7) 297.079,408.857,538.294,550.327 'Recent developments in NLP, deep-learning, and transfer-\\nlearning have signiﬁcantly improved the sentiment extrac-\\ntion from ﬁnancial news and texts [17], [34]–[37]. In [35],\\nYang et al. incorporate inductive transfer-learning meth-\\nods such as ULMFiT [38]\\nfor sentiment analysis in\\nﬁnance, and the results show improvements in senti-\\nment classiﬁcation compared to traditional transfer-learning\\nrecent NLP\\napproaches. The superior performance of\\ntransformers, BERT and RoBERTA, in sentiment analy-\\nsis is evaluated in [37], where the effectiveness of using\\nthe RoBERTa model\\nis compared to dictionary-based\\nmodels.\\n'>\n",
            "<LTTextBoxHorizontal(8) 297.079,361.036,538.293,406.865 'Studies have used sentiment analysis based on ﬁnancial\\nnews to forecast stock prices [6]–[8], foreign exchange and\\nglobal ﬁnancial market trends [9], [10] as well as to predict\\ncorporate earnings [11].\\n'>\n",
            "<LTTextBoxHorizontal(9) 297.079,229.529,538.293,359.044 'This paper aims to survey approaches to sentiment\\nanalysis, including combinations of machine-learning and\\ndeep-learning models with lexicon-based feature extrac-\\ntion methods and word and sentence encoders, up to the\\nmost recent NLP transformers. The goal is to apply these\\napproaches to ﬁnance. We evaluate and compare model effec-\\ntiveness when trained under same conditions and on the same\\ndataset. The main contribution of this paper is the develop-\\nment of an evaluation platform, which we use to assess the\\nperformance of NLP methodologies for text feature extrac-\\ntion in ﬁnance.\\n'>\n",
            "<LTTextBoxHorizontal(10) 297.079,169.754,538.293,227.537 'We show that recent advances in deep-learning and\\ntransfer-learning methods in NLP increase the accuracy of\\nsentiment analysis based on ﬁnancial headlines. Moreover,\\nour results indicate that lexicon-based approaches can be\\nefﬁciently replaced by modern NLP transformers.\\n'>\n",
            "<LTTextBoxHorizontal(11) 297.079,50.202,538.293,167.761 'The rest of the paper is organized as follows. Section II\\nprovides an overview of NLP methods for text representation:\\nlexicon-based and statistical, as well as word and sentence\\nencoders. Section III presents NLP transformers, their archi-\\ntectures and objectives, as a separate group of deep-learning\\nmodels for text classiﬁcation, which we evaluate in extrac-\\ntion of ﬁnance text sentiments. Section IV describes the\\ndataset that we created to evaluate text representation meth-\\nods. Section V presents the evaluation platform that we build\\nfor measuring model performances. Section VI reports the\\n'>\n",
            "<LTTextBoxHorizontal(12) 36.170,30.057,81.746,36.035 'VOLUME 8, 2020\\n'>\n",
            "<LTTextBoxHorizontal(13) 513.893,29.811,538.288,36.785 '131663\\n'>\n",
            "<LTFigure(Fm0) 461.287,751.701,538.287,764.701 matrix=[1.00,0.00,0.00,1.00, (461.29,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTTextBoxHorizontal(0) 252.467,749.978,538.295,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,695.782,277.381,717.700 'results, and section VII concludes the paper and considers\\nfuture applications.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,599.184,277.384,680.781 'II. TEXT REPRESENTATION METHODS\\nA. LEXICON-BASED KNOWLEDGE EXTRACTION\\nLexicon-based sentiment analysis methods rely on domain-\\nspeciﬁc knowledge represented as a lexicon or dictionary.\\nThe process of sentiment calculation is based on identifying\\nand keeping words that hold useful information while remov-\\ning words that are not related to sentiments in ﬁnance.\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,551.363,277.384,597.191 'Commonly used lexicons and dictionaries in ﬁnance are\\nGeneral Inquirer (GI), Harvard IV-4 (HIV4) [39], Diction\\n[40], [41], and Loughran and McDonald’s (LM) [12] word\\nlists.\\n'>\n",
            "<LTTextBoxHorizontal(4) 46.133,539.408,82.388,549.371 'To infer\\n'>\n",
            "<LTTextBoxHorizontal(5) 36.170,443.766,277.384,549.371 'the sentiment, we evaluate the Loughran-\\nMcDonald lexicon (a ﬁnancial lexical rule-based tool) and the\\ngeneral-purpose Harvard IV-4 dictionary (general sentiment\\ndictionary). We calculate the sentiment polarity using the\\nLydia system [42]. Each of the words in the sentences is\\ncategorized into either a positive or a negative group based\\non its sentiment in the lexicon (Eq. 1). If polarity>0, then the\\nsentence is classiﬁed as positive, and if polarity<0, then the\\nsentence is classiﬁed as negative.\\n'>\n",
            "<LTTextBoxHorizontal(6) 108.596,421.474,150.994,432.602 'Polarity =\\n'>\n",
            "<LTTextBoxHorizontal(7) 154.957,414.400,196.955,439.586 'Pos − Neg\\nPos + Neg\\n'>\n",
            "<LTTextBoxHorizontal(8) 265.762,421.374,277.379,431.337 '(1)\\n'>\n",
            "<LTTextBoxHorizontal(9) 36.170,315.168,277.384,408.817 'When using machine-learning (ML) and deep-learning\\n(DL) classiﬁers, we extract the headline features by replacing\\nthe words in the sentence with the sentiment value, speci-\\nﬁed in the dictionary. Next, we input the newly generated\\nsequence into the neural network to classify the text. The\\nDL’s output soft-max layer calculates the probability that the\\nsequence belongs to either the positive or negative sentiment\\nlabels.\\n'>\n",
            "<LTTextBoxHorizontal(10) 36.170,157.799,277.384,300.193 'B. STATISTICAL METHODS\\n1) COUNT VECTORS\\nCount Vectorizer (CV) is a simple statistical approach to\\ntext representation which converts a collection of text doc-\\numents into a matrix of token counts, thus reducing the entire\\nsentence into a single vector. The positions in the vector\\nrepresent the number of appearances of each word in the\\nsentence. The CV algorithm performs feature extraction by\\nusing a vocabulary of words (tokens) which can be built\\nfrom the same text corpus, or input manually (a-priori) from\\nan external resource. The vocabulary limits the number of\\nfeatures which can be extracted from the text.\\n'>\n",
            "<LTTextBoxHorizontal(11) 36.170,50.202,277.384,155.807 'The CV approach for text representation has some draw-\\nbacks. First, the ordering information gets lost due to the\\nmethodology for term ‘‘squeezing.’’ Second, the contextual\\ninformation of the sentence is hidden, although it is crucial\\nfor sentiment extraction. These issues can be partially solved\\nby using n-gram vectorizers where two, three or more consec-\\nutive words are put together in order to form tokens. Another\\nissue with CV is that it shadows the important words that hold\\ndecision-making features for classiﬁers, because it pays more\\n'>\n",
            "<LTTextBoxHorizontal(12) 297.079,671.871,538.293,717.700 'attention to general, frequent words such as ‘‘like,’’ ‘‘but,’’\\nand, ‘‘or,’’ which do not add meaningful information. As a\\nresult, important text features may vanish, which calls for\\nmore sophisticated algorithms.\\n'>\n",
            "<LTTextBoxHorizontal(13) 297.079,502.791,538.293,656.162 '2) TF-IDF TERM WEIGHTING\\nTF-IDF (Term frequency - inverse document frequency) is\\nan algorithm for statistical measurement, which evaluates\\nthe relevance of a word in a document within a corpus\\nof documents. It addresses the feature-vanishing issue of\\nCV algorithms by re-weighting the count frequencies of the\\nwords (tokens) in the sentence according to the number of\\nappearances of each token. The algorithm works by multi-\\nplying two metrics: term frequency (TF), which calculates\\nthe number of occurrences of a term in the sequence (Eq.3),\\nand inverse document frequency (IDF), which penalizes the\\nfeature count of the term if it appears in more sentences within\\nthe corpus (Eq.4),\\n'>\n",
            "<LTTextBoxHorizontal(14) 331.920,466.321,470.990,493.390 'tﬁdf (t, d, D) = tf (t, d) · idf (t, D)\\ntf (t, d) = log(1 + freq(t, d))\\n'>\n",
            "<LTTextBoxHorizontal(15) 348.292,447.688,413.773,459.813 'idf (t, D) = log(\\n'>\n",
            "<LTTextBoxHorizontal(16) 414.968,440.615,498.933,464.735 'N\\ncount(d ∈ D : t ∈ d)\\n'>\n",
            "<LTTextBoxHorizontal(17) 500.129,447.689,503.447,457.652 ')\\n'>\n",
            "<LTTextBoxHorizontal(18) 526.671,466.321,538.290,491.228 '(2)\\n(3)\\n'>\n",
            "<LTTextBoxHorizontal(19) 526.670,447.689,538.287,457.652 '(4)\\n'>\n",
            "<LTTextBoxHorizontal(20) 297.079,398.610,538.288,432.583 'where t denotes the term, d denotes the document, D denotes\\nthe corpus of documents and N is the total number of\\ndocuments.\\n'>\n",
            "<LTTextBoxHorizontal(21) 297.079,350.789,538.293,396.618 'In this study, we assess the feature extraction perfor-\\nmance of the uni-gram and 2-gram count vectorizers as\\nwell as the TF-IDF term weighting in combination with\\nmachine-learning classiﬁers and deep-neural networks.\\n'>\n",
            "<LTTextBoxHorizontal(22) 297.079,98.023,538.293,335.063 'C. WORD ENCODERS\\nStatistical features do not provide semantics of the contex-\\ntually close words, which means that words with similar\\nmeaning will not have similar codes. Many NLP tasks such\\nas sentiment analysis, question-answering and text generation\\nrequire detailed semantic knowledge that is not provided by\\nCV and TF-IDF. To overcome these challenges, researchers\\nhave introduced word encoders [43] to convert discrete words\\ninto high-dimensional vectors composed of real numbers,\\nusing a procedure called word embedding. Word encoders\\nhelp with understanding the context of the sentences, which\\nimproves the extracted features. These models are based on\\nthe principle of distributional hypothesis [44], in which the\\nmeaning of words is evidenced by the context. This approach\\nestablishes a new area of research in NLP called distribu-\\ntional semantics, which is the core of many contemporary\\nNLP techniques, including word encoders. These methods\\nare called distributional semantic models (DSM), also known\\nin the literature as vector space or semantic space models of\\nmeaning [45]–[47].\\n'>\n",
            "<LTTextBoxHorizontal(23) 297.079,50.202,538.293,96.031 'The word encoders classify the words that appear in the\\nsame context as semantically similar to one another, hence\\nassigning similar vectors to them. This retained semantic\\ninformation is very useful for classiﬁers or neural networks.\\n'>\n",
            "<LTTextBoxHorizontal(24) 36.170,29.812,60.565,36.786 '131664\\n'>\n",
            "<LTTextBoxHorizontal(25) 492.714,30.058,538.290,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTFigure(Fm0) 36.170,751.701,113.170,764.701 matrix=[1.00,0.00,0.00,1.00, (36.17,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTLine 154.957,426.026,196.954,426.026>\n",
            "<LTLine 414.968,452.341,498.933,452.341>\n",
            "<LTTextBoxHorizontal(0) 36.170,749.978,321.998,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 297.079,588.185,538.293,717.700 'methods for distributed word representations: global matrix\\nfactorization and Skip-grams are used to extract better fea-\\ntures by examining the relationships between words. The\\nglobal matrix factorization method can capture the overall\\nstatistics and relationships between words. On the other hand,\\nWord2Vec Skip-gram’s method is efﬁcient in extracting the\\nlocal context and capturing the word analogy. Both meth-\\nods are successfully incorporated into the GloVe encoder,\\nthus outperforming Word2Vec in many NLP tasks. GloVe\\nis widely used as a word encoder for NLP-based sentiment\\nanalysis [49]–[51].\\n'>\n",
            "<LTTextBoxHorizontal(2) 297.079,347.862,538.295,572.965 '3) FastText\\nIn 2016, the Facebook research laboratory introduced a\\nnovel method for word encoding called FastText, which\\ntackles the generalization problem of unknown words [29],\\n[48]. FastText differs from previous models in its ability\\nto build word embeddings at a deeper level by harnessing\\nsub-words and characters. In this method, words become a\\ncontext and word embedding is calculated based on com-\\nbinations of lower-level embeddings. Each word is repre-\\nsented as a bag of character n-grams. For example, the word\\n‘‘ﬁnance,’’ given n = 3, will be represented by the fol-\\nlowing character n-grams: < ﬁ, ﬁn, ina, nan, anc, nce, ce >.\\nThe main algorithm behind FastText is Word2Vec. Learning\\nthe sub-word information enables training of embeddings\\non smaller datasets and generalization to unknown words.\\nFastText shows improved results in text classiﬁcation [52],\\neven in structurally rich languages such as Turkish [53] and\\nArabic [54], which require morphological analysis instead of\\nassigning a distinct vector to each word.\\n'>\n",
            "<LTTextBoxHorizontal(3) 297.079,276.131,538.295,345.870 'We evaluate pre-trained FastText vectors in order to assess\\ntheir performance on ﬁnancial\\ntexts. We use the wiki-\\nnews-300d-1M pre-trained model, which wraps 1 million\\nword vectors trained on Wikipedia’s 2017 corpus and the\\nstatmt.org2 news dataset, where each embedding consists\\nof 300 dimensions.\\n'>\n",
            "<LTTextBoxHorizontal(4) 297.079,71.674,538.293,260.911 '4) ELMo\\nIn 2018, a team of researchers at Allen Institute for Artiﬁ-\\ncial Intelligence developed an advanced word encoder called\\nELMo (Embeddings from Language Models) [55], whose\\nword embeddings are learned from a deep bidirectional lan-\\nguage model (biLM), pre-trained on large corpora of textual\\ndata. The essential feature, which makes ELMo different\\nfrom previous word encoders, is that it produces contextual\\nword embeddings considering the whole context in which\\nthe word is used. Hence, we can obtain different embed-\\nding for the same word in a different context, a major\\nimprovement from previous encoders, which always pro-\\nduce a static embedding. To tackle out-of-vocabulary (OOV)\\ntokens, ELMo uses character-derived embedding, leveraging\\nthe morphological clues of words, thus improving the quality\\nof word representations.\\n'>\n",
            "<LTTextBoxHorizontal(5) 305.049,50.635,362.159,60.562 '2http://statmt.org/\\n'>\n",
            "<LTTextBoxHorizontal(6) 36.170,590.770,242.181,597.758 'FIGURE 1. Word2Vec CBOW and Skip-Grams architectures [43].\\n'>\n",
            "<LTTextBoxHorizontal(7) 36.170,518.918,277.384,564.746 'In this section, we provide an overview of the most popular\\nword encoders: Word2Vec [43], GloVe [28] and FastText\\n[29], [48], which exemplify different approaches in modeling\\nword embeddings.\\n'>\n",
            "<LTTextBoxHorizontal(8) 36.170,302.493,277.384,503.685 '1) Word2Vec\\nIn 2013, a team of researchers at Google, led by Tomas\\nMikolov, introduced the breakthrough model for word rep-\\nresentation called Word2Vec [27], [43], which marked the\\nbeginning of a spectacular evolution in NLP. Mikolov and his\\ncollaborators proposed two model architectures for comput-\\ning continuous vector representations of words by using the\\nunsupervised approach: Continuous Bag-of-Words (CBOW)\\nand Continuous Skip-gram Model (Fig. 1). The CBOW archi-\\ntecture predicts the current word based on the context, while\\nin the Skip-Gram architecture, the distributed representation\\nof the input word is used to predict the context [43]. The\\nauthors show the effectiveness of the proposed methodology\\nexperimentally, using several NLP applications, including\\nsentiment analysis. Additionally, they demonstrate that the\\nSkip-gram architecture gives more accurate results for large\\ndatasets because it generates more general contexts.\\n'>\n",
            "<LTTextBoxHorizontal(9) 36.170,206.851,277.384,300.501 'The main drawback of Word2Vec is its inability to handle\\nunknown or out-of-vocabulary (OOV) words. If the model\\nhas not encountered a word before, it will be unable to inter-\\npret it or build a vector for it. Additionally, Word2Vec does\\nnot support shared representations at sub-word level, which\\nmeans that it will create two completely different vector\\nrepresentations for words which are morphologically similar,\\nlike agree/agreement or worth/worthwhile [29].\\n'>\n",
            "<LTTextBoxHorizontal(10) 36.170,170.986,277.384,204.859 'In our analysis, we use a pre-trained version of Word2Vec\\non the Google News corpus, which contains almost 3 million\\nEnglish words represented by 300-dimensional vectors.\\n'>\n",
            "<LTTextBoxHorizontal(11) 36.170,50.202,277.384,155.753 '2) GloVe\\nIn 2014, a team of researchers at Stanford University pro-\\nposed GloVe, an improved methodology for word encoding,\\nbased on a solid mathematical approach [28]. GloVe over-\\ncomes the drawbacks of Word2Vec in the training phase,\\nimproving the generated embeddings. It emphasizes the\\nimportance of considering the co-occurrence probabilities\\nbetween the words rather than single word occurrence prob-\\nabilities themselves. The model combines two classes of\\n'>\n",
            "<LTTextBoxHorizontal(12) 36.170,30.058,81.746,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTTextBoxHorizontal(13) 513.893,29.812,538.288,36.786 '131665\\n'>\n",
            "<LTFigure(Fm0) 461.287,751.701,538.287,764.701 matrix=[1.00,0.00,0.00,1.00, (461.29,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 63.774,606.873,249.774,716.873 matrix=[1.00,0.00,0.00,1.00, (63.77,606.87)]>\n",
            "<LTTextBoxHorizontal(0) 252.467,749.978,538.295,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,599.144,277.384,716.632 'D. SENTENCE ENCODERS\\nIn 2014, the idea of encoding entire sentences surpassed\\nword encoding. The primary purpose of sentence encoders\\nis to learn ﬁxed-length feature vectors that encode the syntax\\nand semantic properties of variable-length sentences. While a\\nsimple sentence embedding model can be built by averaging\\nthe individual word embeddings for every word of the sen-\\ntence, this approach loses the inherent context and sequence\\nof words as valuable information that should be retained in\\nmany tasks.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,539.368,277.384,597.152 'The main weakness of using sentence encoders to han-\\ndle variable-length text input is related to the ﬁxed size of\\nthe produced vectors. Long and short sentences are treated\\nequally, producing the same number of extracted features,\\nthus diluting the embeddings.\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,491.547,277.384,537.376 'In this section, we outline recent and most prevalent sen-\\ntence encoders [2], [30]–[33], to assess their ability to extract\\nimportant features in sentence representation of ﬁnancial\\nheadlines.\\n'>\n",
            "<LTTextBoxHorizontal(4) 36.170,247.463,277.384,472.565 '1) Doc2Vec\\nIn 2014, the ﬁrst successful sentence encoder, Doc2Vec\\n[30] introduced an approach for representing variable-length\\nfragments of texts (sentences, paragraphs, and documents)\\nas ﬁxed-size dense vectors, a.k.a. paragraph vectors. These\\nvectors are trained to predict words in documents. Their\\nprimary goal is to make an appropriate distributed repre-\\nsentation of large texts, overcoming the weaknesses of bag-\\nof-words methods. Paragraph vectors combine word vectors\\nto build phrase-level or sentence-level representations. They\\nepitomize a distributed memory model, holding the context\\nof the paragraph and contributing to the prediction task of the\\nnext word in combination with word vectors. Additionally,\\nparagraph vectors can be used as features for the paragraph,\\nwhich can be fed as input to a classiﬁer or to a neural network,\\nmaking them appropriate for evaluation of sentiment anal-\\nysis in ﬁnancial headlines. To obtain sentence embeddings,\\nwe use a Doc2Vec approach, which is pre-trained on English\\nWikipedia texts.\\n'>\n",
            "<LTTextBoxHorizontal(5) 36.170,63.154,277.384,228.480 '2) SKIP-THOUGHT VECTORS\\nSkip-Thought Vectors [31] are models that use encoder-\\ndecoder architecture for sequence modeling based on unsu-\\npervised learning. These models use continuity of texts,\\nextracted from books, to train an encoder-decoder method.\\nThe model tries to reconstruct the surrounding sentences of an\\nencoded passage in order to remap their syntactic and seman-\\ntic meaning into similar vector representations. The encoder\\ngenerates a sentence vector, and the decoder is used to gen-\\nerate the surrounding sentences. The model uses a Recur-\\nrent Neural Networks (RNN) encoder with Gated Recurrent\\nUnit (GRU) [56] activations, and an RNN decoder uses a\\nconditional GRU. The use of the attention layer provides\\nfor a dynamic change of the source sentence representation.\\n'>\n",
            "<LTTextBoxHorizontal(6) 297.079,541.770,433.929,548.758 'FIGURE 2. InferSent training scheme [32].\\n'>\n",
            "<LTTextBoxHorizontal(7) 297.079,374.439,538.293,515.909 'Depending on the encoder type, two separate models are\\ntrained: uni-skip and bi-skip. Uni-skip passes sentences in the\\ncorrect order and extracts 2400 features. The Bi-skip model\\nuses two encoders. One of them passes the sentence in the cor-\\nrect order and the other passes the sentence in reverse order,\\nextracting a total of 2400 features. Due to their generative\\nnature, Skip-Thought vectors are appropriate and effective for\\nneural machine translation and classiﬁcation tasks. The main\\nshortcoming of this approach is the arduous task assigned\\nto the decoder [57], as the next sentence prediction requires\\nmodeling aspects that are, in most cases, irrelevant to the\\nmeaning of the sentence.\\n'>\n",
            "<LTTextBoxHorizontal(8) 297.079,241.767,538.293,359.273 '3) InferSent\\nInferSent [32] is a supervised approach to learning sentence\\nembeddings using natural language inference (NLI) data.\\nNLI captures universally useful features, thus learning uni-\\nversal sentence embeddings in a supervised manner. The\\ntraining dataset used by this model is the Stanford Natu-\\nral Language Inference (SNLI) dataset that contains 570k\\nhuman-generated English sentence pairs, manually anno-\\ntated with one of the three labels: entailment, contradiction,\\nor neutral.\\n'>\n",
            "<LTTextBoxHorizontal(9) 297.079,110.261,538.295,239.775 'Fig. 2 shows a shared encoder used for encoding the\\npremise u and the hypothesis v. In order to extract relations\\nbetween u and v, three matching methods are applied: con-\\ncatenation (u, v), element-wise product u ∗ v and absolute\\nelement-wise difference |u − v|. Next, the resulting feature\\nvector is applied as input to the 3-class classiﬁer to evaluate\\nthe relationship between u and v based on the extracted\\nfeatures. Experimentally, the best architecture for the encoder\\nis shown to be the BiLSTM network with max pooling. This\\napproach outperforms Skip-Thought vectors in many NLP\\ntasks.\\n'>\n",
            "<LTTextBoxHorizontal(10) 297.079,62.440,538.293,108.268 'In our study, we assess the performances of two publicly\\navailable versions of InferSent. The ﬁrst version is trained\\nwith Stanford’s GloVe as word encoder and the second is\\ntrained with Facebook’s FastText.\\n'>\n",
            "<LTTextBoxHorizontal(11) 36.170,29.812,60.565,36.786 '131666\\n'>\n",
            "<LTTextBoxHorizontal(12) 492.714,30.058,538.290,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTFigure(Fm0) 36.170,751.701,113.170,764.701 matrix=[1.00,0.00,0.00,1.00, (36.17,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 328.683,557.873,506.683,716.873 matrix=[1.00,0.00,0.00,1.00, (328.68,557.87)]>\n",
            "<LTTextBoxHorizontal(0) 36.170,749.978,321.998,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,569.277,186.968,576.264 'FIGURE 3. USE based on DAN architecture [2].\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,434.845,277.384,540.396 '4) UNIVERSAL SENTENCE ENCODER\\nIn March 2018, Google researchers published their ﬁrst ver-\\nsion of a model which converts variable-length sentences into\\n512-dimensional vectors, called Universal Sentence Encoder\\n(USE) [2]. The model is able to embed not only sentences,\\nbut also words and entire paragraphs. USE uses the concept\\nof transfer-learning to leverage the knowledge extracted from\\nlarge datasets to improve the results when limited training\\ndata is available.\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,363.114,277.384,432.853 'We evaluate the USE encoder, which is based on Deep\\nAveraging Network (DAN) architecture as shown in Fig.3.\\nInput embeddings for words and bi-grams are ﬁrst averaged\\nand then passed through a feed-forward deep neural net-\\nwork (DNN) to produce sentence embeddings. The compu-\\ntational time is linear in the length of the input sentence.\\n'>\n",
            "<LTTextBoxHorizontal(4) 36.170,279.428,277.384,361.122 'USE models are trained on a variety of data sources:\\nWikipedia, news, question-answer pages, and discussion\\nforums. These models are based on transfer-learning exper-\\niments with several datasets to evaluate the efﬁciency of the\\nencoder. The results show that sentence encoders outperform\\ntransfer-learning methodologies that use word-level embed-\\ndings alone.\\n'>\n",
            "<LTTextBoxHorizontal(5) 36.170,219.652,277.384,277.436 'The main issues with USE (DAN model) are related to the\\nuse of averaging techniques that cannot recognize negation\\nphrases like ‘‘not good.’’ This refers to using contextualized\\nembeddings, which considers the inﬂuence of other words in\\nproducing sentence embedding.\\n'>\n",
            "<LTTextBoxHorizontal(6) 36.170,183.787,277.384,217.660 'In our analysis, we assess the two latest versions of\\nUSE (4 and 5) that can be found at the TensorFlow Hub\\nrepository.3\\n'>\n",
            "<LTTextBoxHorizontal(7) 36.170,50.202,277.384,167.708 '5) LANGUAGE-AGNOSTIC SENTENCE REPRESENTATIONS\\n(LASER)\\nIn 2019, Facebook researchers [33] introduced an architec-\\nture for universal language-agnostic multilingual sentence\\nrepresentations (LASER) for 93 languages by using a single\\nBiLSTM encoder with a shared Byte Pair Encoding (BPE)\\nvocabulary for different languages. The main contribution\\nof the LASER methodology is that it provides a framework\\nfor zero-shot transfer-learning. LASER leverages one model,\\ntrained on one language, to be used in another language\\n'>\n",
            "<LTTextBoxHorizontal(8) 297.079,633.467,411.131,640.455 'FIGURE 4. LASER architecture [33].\\n'>\n",
            "<LTTextBoxHorizontal(9) 297.079,569.583,538.293,615.411 'without the need for pre-training. This is accomplished by\\nLASER’s ability to bring semantically similar sentences,\\nwritten in different languages, close to each other in the\\nembedding space.\\n'>\n",
            "<LTTextBoxHorizontal(10) 297.079,473.942,538.293,567.591 'Sentence embeddings are obtained by applying a\\nmax-pooling operation to the output of the BiLSTM encoder.\\nThe same encoder is used for all 93 languages. The byte-pair\\nencoding (BPE) vocabulary is learned based on the con-\\ncatenation of all training corpora, hence, it does not require\\nspeciﬁc information about the input language. LASER’s\\nencoder architecture, illustrated in Fig. 4, is shown to be\\nefﬁcient even for low-resource languages.\\n'>\n",
            "<LTTextBoxHorizontal(11) 297.079,426.121,538.293,471.949 'In this study, we evaluate LASER on English texts, though\\nthe same model that we build here can be used for sentiment\\nanalysis in texts written in the other 92 languages supported\\nby LASER.\\n'>\n",
            "<LTTextBoxHorizontal(12) 297.079,131.063,538.293,414.902 'III. NLP TRANSFORMERS\\nThe pre-trained word and sentence embeddings show good\\nperformance for NLP tasks due to their ability to retain the\\nsemantics and the syntax of the words in the sentence. The\\ntransfer-learning task, in this case, allows for the information\\nthat has been learned from unlabeled data to be used in tasks\\nwith relatively small labeled data to achieve higher accu-\\nracy. Although such embeddings have proven to be powerful,\\nthey lack context-based mutability. Word2Vec, GloVe, and\\nFastText use ﬁxed embeddings for each of the words, thus\\nproducing one-to-one mapping, which in many cases is not\\nappropriate and requires additional attention. Recent research\\nstudies have proposed methods that produce different embed-\\ndings for the same word, taking into consideration speciﬁc\\ncontexts [3], [55], [58]. As an illustration of context impor-\\ntance, we analyze the following two sentences that contain\\nthe word ‘‘Apple’’: ‘‘Apple Inc performed well this year.’’\\nand ‘‘Apple fruits are exported to various countries.’’ In the\\nﬁrst sentence, Apple refers to the technology company Apple,\\nheadquartered in the US, while in the second sentence, apple\\nrefers to the fruit, with a completely different meaning. The\\nencoders, however, will produce the same encoding for both\\nwords regardless of the contexts. This problem highlights the\\nneed for contextualized embeddings for the word ‘‘Apple.’’\\n'>\n",
            "<LTTextBoxHorizontal(13) 297.079,50.202,538.293,119.869 'A. NLP TRANSFORMER ARCHITECTURE\\nA transformer represents an architecture that transforms one\\nsequence into another by using two models: encoder and\\ndecoder. Unlike previously described standard sequence-to-\\nsequence models, which are based on LSTM/GRU units,\\nthe paper ‘‘Attention is All You Need’’ [59] introduces a\\n'>\n",
            "<LTTextBoxHorizontal(14) 36.170,30.058,81.746,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTTextBoxHorizontal(15) 513.893,29.812,538.288,36.786 '131667\\n'>\n",
            "<LTFigure(Fm0) 461.287,751.701,538.287,764.701 matrix=[1.00,0.00,0.00,1.00, (461.29,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 47.982,585.380,265.567,716.873 matrix=[1.00,0.00,0.00,1.00, (47.98,585.38)]>\n",
            "<LTFigure(Fm2) 308.650,649.571,526.717,716.873 matrix=[1.00,0.00,0.00,1.00, (308.65,649.57)]>\n",
            "<LTTextBoxHorizontal(0) 252.467,749.978,538.295,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 297.079,659.916,538.293,717.700 'A multi-head attention mechanism calculates the scaled\\ndot-product attention multiple times in parallel. The inde-\\npendent outputs are concatenated and linearly transformed\\ninto expected dimensions. Multi-head attention is obtained by\\nusing Eq. 7:\\n'>\n",
            "<LTTextBoxHorizontal(2) 297.957,635.953,510.182,649.210 'MultiHead(Q, K , V ) = [head1, head2, . . . headh]W O\\n'>\n",
            "<LTTextBoxHorizontal(3) 526.671,636.928,538.288,646.891 '(7)\\n'>\n",
            "<LTTextBoxHorizontal(4) 297.079,613.041,476.751,624.003 'Each of the headi can be calculated by Eq. 8:\\n'>\n",
            "<LTTextBoxHorizontal(5) 337.049,588.114,434.484,603.894 'headi = Attention(QW Q\\ni\\n'>\n",
            "<LTTextBoxHorizontal(6) 434.982,588.562,461.285,602.984 ', KW K\\ni\\n'>\n",
            "<LTTextBoxHorizontal(7) 462.672,588.465,491.514,602.984 ', VW i\\nV )\\n'>\n",
            "<LTTextBoxHorizontal(8) 526.673,590.702,538.290,600.665 '(8)\\n'>\n",
            "<LTTextBoxHorizontal(9) 360.747,566.348,377.560,578.279 ', W i\\n'>\n",
            "<LTTextBoxHorizontal(10) 339.600,563.760,359.360,578.181 ', W K\\ni\\n'>\n",
            "<LTTextBoxHorizontal(11) 297.079,530.482,538.296,579.539 'where W Q\\nV and W O are parameter matrices, which\\ni\\nthe model needs to learn. Multi-head attentions have an\\nimportant role in obtaining the contextual embeddings when\\nusing NLP transformers.\\n'>\n",
            "<LTTextBoxHorizontal(12) 297.079,363.110,538.293,528.490 'A pre-training phase is an unsupervised learning approach\\nwhere an unlabeled text corpus is introduced into the trans-\\nformer architecture to produce text representations based on\\nan objective function used by the transformer. This is a rel-\\natively expensive task, but the learned token or generic sen-\\ntence representations can be used in many other tasks using\\ntransfer-learning. Later, the representation can be ﬁne-tuned\\nin order to recognize the speciﬁcs of the task and to achieve\\nbetter results. Fine-tuning is performed by adding an addi-\\ntional dense layer after the last hidden state, recommended for\\nusing transformers in classiﬁcation and regression tasks [3].\\nThe transformer performs supervised learning (ﬁne-tuning)\\non the labeled sentiment dataset, which is relatively inexpen-\\nsive compared to pre-training.\\n'>\n",
            "<LTTextBoxHorizontal(13) 297.079,327.244,538.293,361.118 'NLP transformers are applicable to many different text\\nclassiﬁcation problems, such as binary sentiment classiﬁca-\\ntion, which we use in our analysis.\\n'>\n",
            "<LTTextBoxHorizontal(14) 297.079,121.933,538.293,311.170 '1) BERT\\nIn 2018, Devlin et al.\\nleveraged the transformer\\n[3]\\narchitecture to introduce a revolutionary language represen-\\ntation model, called BERT (Bidirectional Encoder Repre-\\nsentations from Transformers). This model started the new\\nera in NLP, with state-of-the-art performance achieved on\\nmost NLP tasks. BERT leverages the unsupervised learning\\napproach to pre-train deep bidirectional representations from\\nlarge unlabeled text corpora by using two new pre-training\\nobjectives — masked language model (MLM) and next sen-\\ntence prediction (NSP). BERT overcomes the limitation of\\nprevious language models, which incorporate only unidirec-\\ntional representations of words in sentences. It builds a bidi-\\nrectional masked language model, which predicts randomly\\nmasked words in the sentence, enriching the contextual infor-\\nmation of the words.\\n'>\n",
            "<LTTextBoxHorizontal(15) 297.079,50.202,538.293,119.941 'BERT is based on conventional, auto-regressive (AR) lan-\\nguage modeling. The process of pre-training is performed\\nby maximizing the likelihood between the tokens x in a\\ntext sequence x = [x1, . . . , xT ]. Let ˆx denote the same text\\nsentence with masked tokens and x be an array of masked\\ntokens. The training objective for BERT is to reconstruct x\\n'>\n",
            "<LTTextBoxHorizontal(16) 36.170,430.751,182.623,437.738 'FIGURE 5. The Transformer architecture [59].\\n'>\n",
            "<LTTextBoxHorizontal(17) 36.170,321.103,277.384,414.752 'novel, breakthrough transformer architecture based solely\\non multi-headed self-attention mechanisms. There are three\\nreasons for choosing self-attention instead of recurrent\\nlayer: computational complexity, parallelization, and learning\\nlong-range dependencies between words in the sequence, all\\nof which are crucial for building contextualized embeddings.\\nBy using this approach, transformers have shown improved\\nresults in machine translation and other related tasks.\\n'>\n",
            "<LTTextBoxHorizontal(18) 36.170,261.327,277.384,319.110 'This method uses positional embedding to remember the\\norder of words in the sequence. The main building blocks\\nin the encoder/decoder modules are Multi-Head Attention\\nand Feed Forward layers, as shown in the Attention-based\\ntransformer architecture (Fig.5).\\n'>\n",
            "<LTTextBoxHorizontal(19) 46.133,249.371,277.384,259.334 'The scaled dot-product attention mechanism is described\\n'>\n",
            "<LTTextBoxHorizontal(20) 36.170,237.416,121.125,247.379 'by equations 5 and 6.\\n'>\n",
            "<LTTextBoxHorizontal(21) 113.506,201.910,166.758,213.138 'a = softmax(\\n'>\n",
            "<LTTextBoxHorizontal(22) 167.953,194.796,187.171,220.677 'QK T\\n√\\nn\\n'>\n",
            "<LTTextBoxHorizontal(23) 189.921,201.910,193.239,211.873 ')\\n'>\n",
            "<LTTextBoxHorizontal(24) 265.769,201.910,277.386,211.873 '(5)\\n'>\n",
            "<LTTextBoxHorizontal(25) 36.170,70.092,277.387,187.752 'In Eq.5, the attention weights a represent the inﬂuence of\\neach word in the sequence (Q) by all the other words (K) in\\nthe same sequence. Q is a matrix that contains the query\\n(vector representation of one word in the sequence), K are\\nall keys (all vector representations of all the words in the\\nsequence) and n is dimensionality of the query/key vectors.\\nThe softmax function is used to ensure that weights a have a\\ndistribution between 0 and 1. Considering a, a self-attention is\\ncalculated by using Eq.6, which represents a weighted sum of\\nvalues (V), where V is the vector obtained from the encoder.\\n'>\n",
            "<LTTextBoxHorizontal(26) 102.706,50.202,202.650,62.327 'Attention(Q, K , V ) = aV\\n'>\n",
            "<LTTextBoxHorizontal(27) 265.762,50.202,277.379,60.165 '(6)\\n'>\n",
            "<LTTextBoxHorizontal(28) 36.170,29.812,60.565,36.786 '131668\\n'>\n",
            "<LTTextBoxHorizontal(29) 492.714,30.058,538.290,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTFigure(Fm0) 36.170,751.701,113.170,764.701 matrix=[1.00,0.00,0.00,1.00, (36.17,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 61.274,441.873,252.274,716.873 matrix=[1.00,0.00,0.00,1.00, (61.27,441.87)]>\n",
            "<LTLine 167.953,206.563,188.726,206.563>\n",
            "<LTLine 180.063,204.257,185.044,204.257>\n",
            "<LTLine 437.563,70.490,442.544,70.490>\n",
            "<LTLine 533.211,58.358,538.287,58.358>\n",
            "<LTTextBoxHorizontal(0) 36.170,749.978,321.998,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,707.737,99.360,718.965 'from ˆx by Eq.9:\\n'>\n",
            "<LTTextBoxHorizontal(2) 39.117,675.493,56.273,690.177 'max\\nθ\\n'>\n",
            "<LTTextBoxHorizontal(3) 62.084,680.214,116.664,691.442 'log pθ (x|ˆx) ≈\\n'>\n",
            "<LTTextBoxHorizontal(4) 108.893,646.223,116.664,656.186 '=\\n'>\n",
            "<LTTextBoxHorizontal(5) 120.066,685.862,134.164,700.945 'T\\n(cid:88)\\n'>\n",
            "<LTTextBoxHorizontal(6) 120.066,650.607,134.164,676.892 't=1\\nT\\n(cid:88)\\n'>\n",
            "<LTTextBoxHorizontal(7) 120.911,633.102,133.318,641.636 't=1\\n'>\n",
            "<LTTextBoxHorizontal(8) 36.170,617.965,63.000,627.928 'where,\\n'>\n",
            "<LTTextBoxHorizontal(9) 135.823,679.238,193.995,691.442 'mt log pθ (xt|ˆx)\\n'>\n",
            "<LTTextBoxHorizontal(10) 135.823,644.059,160.622,655.021 'mt log\\n'>\n",
            "<LTTextBoxHorizontal(11) 171.908,634.888,227.271,663.725 'exp (Hθ (ˆx)T\\nx(cid:48) exp(Hθ (ˆx)T\\n'>\n",
            "<LTTextBoxHorizontal(12) 214.613,649.857,242.389,662.005 't e(xt ))\\n'>\n",
            "<LTTextBoxHorizontal(13) 223.061,635.436,250.820,649.094 't e(x(cid:48)))\\n'>\n",
            "<LTTextBoxHorizontal(14) 163.477,640.818,173.888,650.781 '(cid:80)\\n'>\n",
            "<LTTextBoxHorizontal(15) 297.079,695.781,538.292,718.965 'sequence x of length T , there are T ! different orders on which\\nthe algorithm performs auto-regressive factorizations.\\n'>\n",
            "<LTTextBoxHorizontal(16) 297.079,659.016,538.297,694.018 'Let ZT be the set of permutations of the words in a sentence\\nof length T. xz<t denotes the ﬁrst t − 1 elements of the\\npermutation z ∈ ZT . The PLM objective is given in Eq. 10.\\n'>\n",
            "<LTTextBoxHorizontal(17) 265.762,644.958,277.379,654.921 '(9)\\n'>\n",
            "<LTTextBoxHorizontal(18) 349.755,626.917,366.911,641.601 'max\\nθ\\n'>\n",
            "<LTTextBoxHorizontal(19) 371.061,630.377,393.060,641.721 'Ez∼Z\\n'>\n",
            "<LTTextBoxHorizontal(20) 399.646,637.286,413.744,653.753 '|z|\\n(cid:88)\\n'>\n",
            "<LTTextBoxHorizontal(21) 395.857,619.782,417.532,628.316 't=c+1\\n'>\n",
            "<LTTextBoxHorizontal(22) 419.191,629.873,478.809,642.866 'log pθ (xzt |xz<t)\\n'>\n",
            "<LTTextBoxHorizontal(23) 521.689,631.638,538.288,641.601 '(10)\\n'>\n",
            "<LTTextBoxHorizontal(24) 46.133,580.134,277.389,616.714 '• e(x(cid:48)) denotes the embedding of the token x;\\n• mt = 1, if xt token of the text sequence x is masked;\\n• Hθ is a Transformer which transforms each token of text\\n'>\n",
            "<LTTextBoxHorizontal(25) 55.896,568.179,176.110,578.142 'sequence into a hidden vector.\\n'>\n",
            "<LTTextBoxHorizontal(26) 36.170,482.527,277.387,564.321 'BERT assumes that all masked tokens x are mutually inde-\\npendent, which is the main rationale behind the approxi-\\nmation of the joint conditional probability p(x, ˆx) in Eq.9.\\nAnother advantage that differentiates BERT from previous\\nAR methods is the ability to increase the context information\\nHθ (x)t by accessing the tokens placed on the left and the right\\nside of token t.\\n'>\n",
            "<LTTextBoxHorizontal(27) 36.170,410.796,277.384,480.535 'BERT has two versions: BERT-base, with 12 encoder lay-\\ners, hidden size of 768, 12 multi-head attention heads and\\n110M parameters in total; and BERT-large, with 24 encoder\\nlayers, hidden size of 1024, 16 multi-head attention heads and\\n340M parameters. Both of these models have been trained on\\nEnglish Wikipedia and BookCorpus [60].\\n'>\n",
            "<LTTextBoxHorizontal(28) 36.170,302.230,277.384,395.826 '2) FinBERT\\nFinBERT [61] is a version of BERT intended for the ﬁnance\\ndomain. It is pre-trained on a ﬁnancial text corpus which con-\\nsists of 1.8M news articles from Reuters TRC2 dataset, pub-\\nlished between 2008 and 2010. Compared to other pre-trained\\nversions of BERT, FinBERT model has achieved a 15%\\nimprovement in accuracy in text classiﬁcation tasks specif-\\nically applied to ﬁnancial texts.\\n'>\n",
            "<LTTextBoxHorizontal(29) 36.170,133.888,277.384,287.260 '3) XLNet\\nThe XLNet model, developed by Google Brain and Carnegie\\nMellon University, addresses the disadvantages of BERT,\\nimproves its architectural design for pre-training, and pro-\\nduces results that outperform BERT in 20 different tasks.\\nIt utilizes a generalized AR model where the next token is\\ndependent on all previous tokens, thus avoiding corrupted\\ninput caused by masking of the words, performed by BERT.\\nThe limitations of BERT include neglecting the dependency\\nbetween masked tokens as it assumes that they are mutually\\nindependent variables. On the other hand, XLNet considers\\nthese tokens in the process of context building and assumes\\nthat masked words are mutually dependent.\\n'>\n",
            "<LTTextBoxHorizontal(30) 36.170,50.202,277.384,131.896 'Additionally, XLNet uses Permutation Language Model-\\ning (PLM) to capture bidirectional context by maximizing\\nthe expected log-likelihood of a sequence given all possible\\npermutations of words in a sentence. This means that XLNet\\nenriches the contextual information of each position by lever-\\naging the tokens from all the other positions found on the\\nleft and on the right sides of the token. Speciﬁcally, for a\\n'>\n",
            "<LTTextBoxHorizontal(31) 297.079,567.050,538.290,613.954 'The hyperparameter c can be derived from the hyperpa-\\nrameter K, where c = |z|(K − 1)/K , and it represents the\\ncutting-point of the division of vector z into non-target z≤c\\nand target z>c subsequences.\\n'>\n",
            "<LTTextBoxHorizontal(32) 297.079,495.395,538.293,566.033 'As shown in Eq.9 and Eq.10, both BERT and XLNet\\nperform partial prediction, due to optimization. The main dif-\\nference lies in the choice of tokens used for context modeling.\\nBERT predicts the masked tokens, assuming that targets are\\nmutually independent, while XLNet predicts the last token in\\na factorization order z>c.\\n'>\n",
            "<LTTextBoxHorizontal(33) 297.079,424.563,538.808,496.464 'The following example [Wells, Fargo, is, a, bank, in, USA]\\nexplains the difference. Assume that our goal is to predict\\n‘‘Wells Fargo.’’ In order to use [Wells, Fargo] as prediction\\ntargets, BERT masks them, and XLNet samples the factor-\\nization order [is,a,bank,in,USA,Wells,Fargo]. Using Eq. 9,\\nBERT will compute:\\n'>\n",
            "<LTTextBoxHorizontal(34) 298.843,404.738,467.547,417.839 'JBERT = log p(Wells|is, a, bank, in, USA)\\n'>\n",
            "<LTTextBoxHorizontal(35) 357.377,390.770,508.299,402.895 '+ log p(FARGO|is, a, bank, in, USA)\\n'>\n",
            "<LTTextBoxHorizontal(36) 521.689,390.770,538.287,400.733 '(11)\\n'>\n",
            "<LTTextBoxHorizontal(37) 307.042,371.921,449.531,383.149 'Using Eq. 10, XLNet will compute:\\n'>\n",
            "<LTTextBoxHorizontal(38) 307.096,352.096,479.196,365.196 'JXLNet = log p(Wells|is, a, bank, in, USA)\\n'>\n",
            "<LTTextBoxHorizontal(39) 349.947,338.127,528.272,350.252 '+ log p(FARGO|Wells, is, a, bank, in, USA)\\n'>\n",
            "<LTTextBoxHorizontal(40) 521.690,323.183,538.288,333.146 '(12)\\n'>\n",
            "<LTTextBoxHorizontal(41) 297.079,232.603,538.293,314.297 'These examples show that both BERT and XLNet compute\\nthe objective differently. XLNet captures important depen-\\ndencies between prediction targets, such as (Wells, Fargo),\\nwhich BERT omits. Hence, XLNet combines the advantages\\nof AR and auto-encoding methods by using a generalized AR\\npre-training approach with a permutation language modeling\\nobjective, in order to improve the results in NLP.\\n'>\n",
            "<LTTextBoxHorizontal(42) 297.079,159.834,538.293,217.563 '4) XLM\\nThe Cross-lingual Language Model (XLM) [62] has a\\ntransformer architecture that is mainly used for modeling\\ncross-lingual features. XLM is pre-trained using several\\nobjectives:\\n'>\n",
            "<LTTextBoxHorizontal(43) 307.042,145.844,473.055,155.807 '• Causal Language Modeling (CLM)\\n'>\n",
            "<LTTextBoxHorizontal(44) 480.676,145.844,508.633,155.807 '- next\\n'>\n",
            "<LTTextBoxHorizontal(45) 516.254,145.844,538.292,155.807 'token\\n'>\n",
            "<LTTextBoxHorizontal(46) 316.805,133.888,359.696,143.851 'prediction.\\n'>\n",
            "<LTTextBoxHorizontal(47) 307.042,98.023,538.292,131.896 '• Masked Language Modeling (MLM) - approach similar\\nto BERT’s objective for masking random tokens in the\\nsentence.\\n'>\n",
            "<LTTextBoxHorizontal(48) 307.042,50.202,538.292,96.031 '• Translation Language Modeling (TLM) - supervised\\napproach, which harnesses parallel streams of textual\\ndata written in different languages in order to improve\\ncross-lingual pre-training support.\\n'>\n",
            "<LTTextBoxHorizontal(49) 36.170,30.058,81.746,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTTextBoxHorizontal(50) 513.893,29.812,538.288,36.786 '131669\\n'>\n",
            "<LTFigure(Fm0) 461.287,751.701,538.287,764.701 matrix=[1.00,0.00,0.00,1.00, (461.29,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTLine 89.362,688.546,94.343,688.546>\n",
            "<LTLine 163.477,649.611,250.820,649.611>\n",
            "<LTLine 194.624,562.413,199.700,562.413>\n",
            "<LTLine 222.488,538.503,227.564,538.503>\n",
            "<LTTextBoxHorizontal(0) 252.467,749.978,538.295,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,659.916,277.384,717.700 'In our analysis, we use XLM for text classiﬁcation tasks to\\nperform sentiment analysis of texts in English. We explore\\nbi-directional context of the tokens in sentences to per-\\nform Masked Language Modeling (MLM), which is the best\\napproach for our evaluation task.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,478.638,277.384,643.964 '5) ALBERT\\nTo overcome the shortcomings of using large pre-training\\nnatural language representations such as GPU/TPU, mem-\\nory limitations, and longer training times, in 2019 Google\\nResearch and Toyota Technological Institute jointly released\\na new model that introduces BERT’s smaller and more scal-\\nable successor, called ALBERT [63]. ALBERT is based\\non two-parameter reduction methods: cross-layer parameter\\nsharing and sentence ordering objectives, in order to lower\\nmemory consumption and increase the training speed of\\nBERT. ALBERT outperforms BERT in several tasks, includ-\\ning text classiﬁcation [64]. ALBERT uses a signiﬁcantly\\nreduced number of parameters in sentiment analysis, com-\\npared to BERT and XLNet.\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,345.180,277.384,462.686 '6) RoBERTa\\nThe RoBERTa model, introduced by the Facebook research\\nteam in 2019 [4], offers an alternative optimized ver-\\nsion of BERT. Retrained on a dataset\\nten times larger,\\nwith improved training methodology and different hyper-\\nparameters, RoBERTa removes the Next Sentence Predic-\\ntion (NSP) objective and adds dynamic masking of words\\nduring the training epochs. These changes and features show\\nbetter performances compared to BERT in many NLP tasks,\\nincluding text classiﬁcation.\\n'>\n",
            "<LTTextBoxHorizontal(4) 36.170,174.958,277.384,329.229 '7) DistilBERT\\nDistilBERT, introduced in October 2019 [65], is based on a\\nmethodology that reduces the size of a BERT model by 40%,\\nwhile retaining 97% of its language understanding capa-\\nbilities and being 60% faster. The technique that produces\\na compression of the original model is known as knowl-\\nedge distillation. The compact (student) model is trained to\\nreproduce the full output distribution of the larger (teacher)\\nmodel or ensemble of models. Rather than training with a\\ncross-entropy over the hard-targets (one-hot encoding of the\\nclasses), the student obtains the knowledge based on a dis-\\ntillation loss over the soft-target probabilities of the teacher.\\nThe distillation loss Lce is calculated by using the Eq. 13.\\n'>\n",
            "<LTTextBoxHorizontal(5) 112.092,151.790,152.256,168.301 'Lce = (cid:88)\\n'>\n",
            "<LTTextBoxHorizontal(6) 153.916,151.790,194.654,163.918 'ti ∗ log(si)\\n'>\n",
            "<LTTextBoxHorizontal(7) 144.155,141.018,146.260,148.590 'i\\n'>\n",
            "<LTTextBoxHorizontal(8) 260.785,152.690,277.383,162.653 '(13)\\n'>\n",
            "<LTTextBoxHorizontal(9) 36.170,86.068,277.384,131.996 'where ti and si are the estimated probabilities of the teacher\\nand student respectively. This objective results in a richer\\ntraining signal, since soft-target probabilities enforce stricter\\nconstraints compared to a single hard-target.\\n'>\n",
            "<LTTextBoxHorizontal(10) 46.133,74.113,187.618,84.076 'We assess the performances of\\n'>\n",
            "<LTTextBoxHorizontal(11) 36.170,50.202,277.384,84.076 'three distilled ver-\\nsions (students) of the following transformers (teachers):\\nBERT-base-cased, BERT-base-uncased, and RoBERTa-base.\\n'>\n",
            "<LTTextBoxHorizontal(12) 297.079,599.144,538.293,716.650 '8) XLM-RoBERTa\\nThe XLM-RoBERTa (XLM-R) [66] model is a multilingual\\nmodel trained on one hundred different languages by using\\n2.5TB of ﬁltered CommonCrawl data and it is based on Face-\\nbook’s RoBERTa model. XLM-R achieves solid performance\\ngains for a wide range of cross-lingual transfer tasks, includ-\\ning text classiﬁcation. Additionally, XLM-RoBERTa offers\\na possibility of multilingual modeling without decreasing\\nper-language performance, which makes it more attractive for\\nevaluation compared to other transformers.\\n'>\n",
            "<LTTextBoxHorizontal(13) 297.079,526.513,538.293,597.152 'XLM-R follows the XLM approach [62], trained with a\\nMasked Language Modeling (MLM) objective with minor\\nchanges to the hyper-parameters of the original XLM model.\\nIn our analysis, we evaluate the performance of two\\ndifferent pre-trained XLM-R models: XLM − Rbase and\\nXLM − RLarge, which differ in the size of their parameters.\\n'>\n",
            "<LTTextBoxHorizontal(14) 297.079,332.145,538.293,509.427 '9) BART\\nIn October 2019, the Facebook research team published a\\nnovel transformer called BART [67] with an architecture sim-\\nilar to both BERT [3] and GPT2 (Generative Pre-Training 2)\\n[68]. BART outperforms other transformers in generation\\ntasks such as text summarizing and question answering.\\nBART leverages the advantages of the bidirectional encoder\\nfrom BERT and the GPT AR decoder. The auto-regressive\\napproach means that GPT considers left to right dependence\\nof the words in a sentence, which makes it more appropriate\\nfor text-generation compared to BERT. BART’s encoder and\\ndecoder are connected by cross-attention. Each decoder layer\\nperforms attention over the ﬁnal hidden state of the encoder\\noutput. This mechanism enables the model to generate output\\nthat is closely connected to the original input.\\n'>\n",
            "<LTTextBoxHorizontal(15) 297.079,260.414,538.293,330.153 'The ﬁne-tuned model concatenates the input sentence with\\nthe end of sequence (EOS) token and passes these compo-\\nnents as input to the BART encoder and decoder. The repre-\\nsentation of the EOS token is used to classify the sentiment\\nexpressed in the sentence. In this study, we ﬁne-tune BART\\nand adapt it to sentiment analysis in ﬁnance.\\n'>\n",
            "<LTTextBoxHorizontal(16) 297.079,161.783,538.293,242.384 'IV. DATASETS\\nWe use publicly available datasets that have been labeled\\nby ﬁnancial experts to perform a reliable evaluation of\\nthe ML models in predicting sentiments of ﬁnancial head-\\nlines. We perform binary classiﬁcations to designate each\\nof the sentences as bullish (positive) or bearish (negative),\\nas described in the following subsections.\\n'>\n",
            "<LTTextBoxHorizontal(17) 297.079,50.202,538.293,143.779 'A. FINANCIAL PHRASE BANK\\nThe Financial Phrase-Bank dataset [69] consists of 4845\\nEnglish sentences selected randomly from ﬁnancial news\\nfound on the LexisNexis database. These sentences have been\\nannotated by 16 experts with a background in ﬁnance and\\nbusiness. The annotators were asked to give labels according\\nto how they think the information in the sentence might\\ninﬂuence the mentioned company’s stock price. The dataset\\n'>\n",
            "<LTTextBoxHorizontal(18) 36.170,29.812,60.565,36.786 '131670\\n'>\n",
            "<LTTextBoxHorizontal(19) 492.714,30.058,538.290,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTFigure(Fm0) 36.170,751.701,113.170,764.701 matrix=[1.00,0.00,0.00,1.00, (36.17,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTTextBoxHorizontal(0) 36.170,749.978,321.998,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,709.848,130.165,716.836 'TABLE 1. Datasets statistics.\\n'>\n",
            "<LTTextBoxHorizontal(2) 297.079,556.053,488.131,563.040 'FIGURE 6. Distribution of number of words in training set.\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,578.573,277.384,624.402 'also includes information regarding the agreement levels on\\nsentences among annotators. All sentences are annotated with\\nthree labels: Positive, Negative, and Neutral. The distribution\\nof sentiment labels is presented in Table 1.\\n'>\n",
            "<LTTextBoxHorizontal(4) 36.170,397.581,277.384,562.889 'B. SemEval 2017 TASK 5\\nThe second dataset used in this paper is provided by the\\nSemEval-2017 task ‘‘Fine-Grained Sentiment Analysis on\\nFinancial Microblogs and News’’ [70]. The Financial News\\nStatements and Headlines dataset consists of 2510 news head-\\nlines, gathered from different publicly available sources such\\nas Yahoo Finance. Each headline (instance) is annotated by\\nthree independent ﬁnancial experts, and a sentiment score,\\nin the range between -1 and 1, is assigned to each statement.\\nA score of -1 means that the statement (message) is bearish\\nor very negative, and a score of 1 means that the statement\\nis bullish or very positive. We convert these sentiment scores\\ninto sentiment labels (bullish/bearish). The conversion pro-\\ncess is performed by using Eq. 14.\\n'>\n",
            "<LTTextBoxHorizontal(5) 92.916,362.107,109.646,373.235 'L =\\n'>\n",
            "<LTTextBoxHorizontal(6) 112.414,369.648,120.305,391.566 '\\uf8f1\\n\\uf8f4\\uf8f2\\n'>\n",
            "<LTTextBoxHorizontal(7) 112.414,348.726,120.305,361.678 '\\uf8f4\\uf8f3\\n'>\n",
            "<LTTextBoxHorizontal(8) 120.304,347.382,154.597,388.099 'Bullish,\\nBearish,\\nNeutral,\\n'>\n",
            "<LTTextBoxHorizontal(9) 164.558,347.282,212.634,388.099 'if score > 0\\nif score < 0\\nif score = 0\\n'>\n",
            "<LTTextBoxHorizontal(10) 297.079,371.503,495.391,378.490 'FIGURE 7. Distribution of number of words in validation set.\\n'>\n",
            "<LTTextBoxHorizontal(11) 260.780,362.007,277.378,371.970 '(14)\\n'>\n",
            "<LTTextBoxHorizontal(12) 46.133,326.836,277.384,336.799 'After the conversion, the number of sentences per label is\\n'>\n",
            "<LTTextBoxHorizontal(13) 36.170,314.881,118.929,324.844 'presented in Table 1.\\n'>\n",
            "<LTTextBoxHorizontal(14) 36.170,195.329,277.384,312.889 'The dataset used for evaluation is a combination of both\\ndatasets. To address the imbalance between positive and\\nnegative sentences, we perform a balancing by extract-\\ning 1093 positive and another 1093 negative sentences, which\\nwe merge into one dataset. Additionally, we shufﬂe the\\ndatasets and we set aside stratiﬁed 80% of all sentences\\nas a training and stratiﬁed 20% of the remaining sentences\\nas a validation set. At the end, our balanced training set\\nincludes 1748 samples, and a balanced validation set consist-\\ning of 438 samples.\\n'>\n",
            "<LTTextBoxHorizontal(15) 36.170,74.113,277.384,179.645 'C. DATA PRE-PROCESSING\\nFinancial headlines, similar to other real world text data,\\nare likely to be inconsistent, incomplete and contain errors.\\nHence, to prepare the data, we perform initial pre-processing\\nthat includes tokenization, stop-word removal, and stem-\\nming. Additionally, we extract the named entities (organiza-\\ntions and people) from the headlines and replace them with\\ntheir general nouns. For example, Microsoft is replaced with\\n<CMPY>, or London with <CITY>.\\n'>\n",
            "<LTTextBoxHorizontal(16) 36.170,50.202,277.384,72.120 'We impose a min-max length of sentences to 3-64 words.\\nAfter this initial ﬁltering, we obtain the distributions of the\\n'>\n",
            "<LTTextBoxHorizontal(17) 297.079,320.054,538.293,341.973 'number of words per sentence for the training set (Fig. 6) and\\nfor the validation set (Fig. 7).\\n'>\n",
            "<LTTextBoxHorizontal(18) 297.079,236.368,538.293,318.062 'When evaluating lexicon-based and word encoders,\\nwe perform left padding to sentences in order to ﬁx their\\nsize, due to their variable length. Considering the maximum\\nsize of the sentences given in Figs. 6 and 7, we pad them\\nto 64 word length. When using sentence encoders, we do not\\npad the sequences due to the ability of the sentence encoders\\nto encode sentences to ﬁxed-size vectors.\\n'>\n",
            "<LTTextBoxHorizontal(19) 297.079,174.169,538.293,218.904 'V. SENTIMENT ANALYSIS PLATFORM\\nWe evaluate the sentiment analysis methods by using the\\ngeneral platform, consisting of ﬁve phases shown in Fig. 8,\\nas follows:\\n'>\n",
            "<LTTextBoxHorizontal(20) 307.042,133.889,539.309,167.762 '• In the ﬁrst phase, we create our working dataset based on\\nthe Financial Phrase Bank and the SemEval 2017 dataset.\\n• In the second phase we apply data pre-processing func-\\n'>\n",
            "<LTTextBoxHorizontal(21) 316.805,121.933,466.892,131.896 'tions as described in subsection IV-C.\\n'>\n",
            "<LTTextBoxHorizontal(22) 307.042,50.202,538.292,119.941 '• The third phase performs text encoding by using various\\ntext representation methods in order to extract features\\nfrom the pre-processed texts. We evaluate the following\\ntext representation methods: domain lexicons, statistical\\nmodels for feature extraction, word encoders, sentence\\nencoders and NLP transformers.\\n'>\n",
            "<LTTextBoxHorizontal(23) 36.170,30.058,81.746,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTTextBoxHorizontal(24) 513.893,29.812,538.288,36.786 '131671\\n'>\n",
            "<LTFigure(Fm0) 461.287,751.701,538.287,764.701 matrix=[1.00,0.00,0.00,1.00, (461.29,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 56.902,650.484,256.647,698.653 matrix=[1.00,0.00,0.00,1.00, (56.90,650.48)]>\n",
            "<LTFigure(Fm2) 308.376,572.156,526.990,716.873 matrix=[1.00,0.00,0.00,1.00, (308.38,572.16)]>\n",
            "<LTFigure(Fm3) 308.376,387.606,526.991,532.323 matrix=[1.00,0.00,0.00,1.00, (308.38,387.61)]>\n",
            "<LTTextBoxHorizontal(0) 252.467,749.978,538.295,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 69.243,411.618,240.085,418.606 'FIGURE 8. Sentiment analysis platform architecture.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,394.794,309.878,401.781 'TABLE 2. Average performances of models grouped by text representation method.\\n'>\n",
            "<LTTextBoxHorizontal(3) 46.133,56.361,277.383,78.279 '• In the fourth phase, these embeddings are fed as input\\nto various machine-learning or deep-learning classiﬁers,\\n'>\n",
            "<LTTextBoxHorizontal(4) 316.805,56.361,538.292,78.279 'thus enabling us to evaluate many encoding-classiﬁer\\ncombinations.\\n'>\n",
            "<LTTextBoxHorizontal(5) 36.170,29.812,60.565,36.786 '131672\\n'>\n",
            "<LTTextBoxHorizontal(6) 492.714,30.058,538.290,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTFigure(Fm0) 36.170,751.701,113.170,764.701 matrix=[1.00,0.00,0.00,1.00, (36.17,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 69.243,426.227,505.217,716.876 matrix=[0.90,0.00,0.00,0.90, (69.24,426.23)]>\n",
            "<LTFigure(Fm2) 132.689,94.787,441.768,386.841 matrix=[1.00,0.00,0.00,1.00, (132.69,94.79)]>\n",
            "<LTTextBoxHorizontal(0) 36.170,749.978,321.998,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,709.848,189.005,716.836 'TABLE 3. Lexical Rule-based approach results.\\n'>\n",
            "<LTTextBoxHorizontal(2) 46.133,142.473,277.383,176.347 '• In the ﬁfth phase, we compare the real and predicted\\nlabels using several binary classiﬁcation performance\\nmetrics.\\n'>\n",
            "<LTTextBoxHorizontal(3) 297.079,142.473,538.293,176.346 'In the following subsections, we present the details of\\nmachine-learning and deep-learning classiﬁers, ﬁne-tuning\\nof NLP transformers and evaluation metrics.\\n'>\n",
            "<LTTextBoxHorizontal(4) 36.170,70.719,277.384,140.457 'The sentiment analysis platform is implemented in Python\\n3.6. The shallow models are developed using Tensorﬂow\\nKeras [71] while the pre-trained versions of NLP transform-\\ners are retrieved from the Hugging Face repository [72].\\nThe sentiment analysis modules are published at the GitHub\\nrepository.4\\n'>\n",
            "<LTTextBoxHorizontal(5) 44.140,50.635,157.741,60.563 '4https://github.com/f-data/ﬁnSENT\\n'>\n",
            "<LTTextBoxHorizontal(6) 36.170,30.058,81.746,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTTextBoxHorizontal(7) 297.079,50.202,538.293,119.870 'A. MACHINE-LEARNING CLASSIFIERS\\nIn our evaluation analysis, we use two machine-learning\\nclassiﬁers: Support Vector Classiﬁer (SVC), as a represen-\\ntative of Support Vector Machines (SVM), and an Extreme\\nGradient Boosting (XGB) [73], [74], as a representative of\\ngradient-boosted decision trees. We chose the XGB model\\n'>\n",
            "<LTTextBoxHorizontal(8) 513.893,29.812,538.288,36.786 '131673\\n'>\n",
            "<LTFigure(Fm0) 461.287,751.701,538.287,764.701 matrix=[1.00,0.00,0.00,1.00, (461.29,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 44.573,186.877,529.886,701.896 matrix=[1.00,0.00,0.00,1.00, (44.57,186.88)]>\n",
            "<LTTextBoxHorizontal(0) 252.467,749.978,538.295,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,709.848,157.629,716.836 'TABLE 4. Statistical methods results.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,477.563,277.384,523.391 'because it has achieved impressive results in many Kaggle\\ncompetitions, in the structured data category. When using\\nthe ML classiﬁers, we perform a GridSearch approach for\\nretrieving the best hyper-parameters.\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,404.362,277.384,462.074 'B. DEEP-NEURAL NETWORKS (DNN)\\nDeep-learning methods [75] are achieving outstanding results\\nin many ﬁelds, including: signal processing [76], computer\\nvision [77], speech processing [78]–[80] and text classiﬁca-\\ntion [81].\\n'>\n",
            "<LTTextBoxHorizontal(4) 36.170,308.721,277.384,402.370 'The text representations and the features extracted from\\nthe evaluation methods are fed as input into Convolutional\\nNeural Networks (CNN) [23] and Recurrent Neural Net-\\nworks (RNN) [82] in order to proceed with the classiﬁcation.\\nWhile RNN networks work well in sequence modeling and\\ncapturing long-term dependencies, CNN networks are more\\nefﬁcient in capturing spatial or temporal correlations and in\\nreducing data dimensionality.\\n'>\n",
            "<LTTextBoxHorizontal(5) 36.170,213.080,277.384,306.729 'In order to improve the architecture of previous DNN net-\\nworks, novel mechanisms have been introduced. One of them\\nis the Attention mechanism [83], which helps RNN networks\\nfocus on speciﬁc parts of the input sequence, facilitating\\nthe learning and improving the prediction. The Attention\\nmechanism is widely used in encoder-decoder architectures\\ndue to its ability to highlight important parts of the contextual\\ninformation.\\n'>\n",
            "<LTTextBoxHorizontal(6) 36.170,140.022,277.384,211.087 'Bidirectional RNN networks are often used to collect fea-\\n−→\\nh gathers token\\ntures from both directions. A forward RNN\\nfeatures from the start (x1) to the end (xn), while the backward\\n←−\\nh processes the tokens in reverse direction, from (xn)\\nRNN\\nto (x1). The resulting hidden state h uses both sets of features\\nconcatenating\\n'>\n",
            "<LTTextBoxHorizontal(7) 124.332,140.022,213.373,157.851 '←−\\nh as shown in Eq.15:\\n'>\n",
            "<LTTextBoxHorizontal(8) 93.993,140.022,121.842,157.851 '−→\\nh and\\n'>\n",
            "<LTTextBoxHorizontal(9) 125.757,117.696,143.880,129.823 'hi =\\n'>\n",
            "<LTTextBoxHorizontal(10) 146.647,117.696,167.802,136.423 '−→\\nhi ⊕\\n'>\n",
            "<LTTextBoxHorizontal(11) 170.016,117.696,180.985,136.423 '←−\\nhi\\n'>\n",
            "<LTTextBoxHorizontal(12) 260.780,118.595,277.378,128.558 '(15)\\n'>\n",
            "<LTTextBoxHorizontal(13) 36.170,98.023,214.576,109.251 'where ⊕ denotes the concatenation function.\\n'>\n",
            "<LTTextBoxHorizontal(14) 36.170,50.202,277.384,96.031 'In our analysis, we used shallow RNN and CNN networks\\nin order to evaluate the features from text representations.\\nThese shallow neural networks consist of three main layers:\\nthe input (embedding) layer, the hidden layer, and the output\\n'>\n",
            "<LTTextBoxHorizontal(15) 297.079,298.235,538.293,523.391 'layer. The input layer uses text representation methods (lexi-\\ncons/word or sentence encoders) to extract the feature vectors\\nfrom the headlines. It then gives the vector as an input to the\\nrecurrent or convolutional hidden layer to extract complex\\nfeatures from the text representation methods. The output\\nlayer uses a softmax activation function to make the ﬁnal\\nclassiﬁcation. We then add an attention layer after the hidden\\nlayer to evaluate its effectiveness. Furthermore, we build an\\nadditional group of GRU and LSTM networks, which support\\nbidirectional feature extraction, to assess their performance\\nin ﬁnance-based sentiment analysis as described in [84]. and\\nwe use binary cross-entropy loss function when training the\\nmodels. The ADAM (Adaptive Learning Rate) optimization\\nalgorithm [85] is used to ﬁnd optimal weights in the networks.\\nWe use a maximum of one hundred training epochs for all\\nDL models. We impose early stopping when the validation\\nloss does not diminish after ten epochs to prevent over-ﬁtting.\\nFinally, we use dropout layer as regularization in the CNN\\nnetwork [86].\\n'>\n",
            "<LTTextBoxHorizontal(16) 297.079,154.120,538.293,283.563 'C. MODEL FINE-TUNING\\nTo evaluate NLP transformers, we use pre-trained mod-\\nels from the Hugging Face’s repository [72]. For ﬁnBERT,\\nwe use the language model trained on TRC2 dataset, pub-\\nlished on the GitHub repository.5 We ﬁne-tune the trans-\\nformers with the training dataset by adding only one dense\\nlayer after the last hidden state. The dense layer outputs\\nthe probabilities of sentence classiﬁcation. Transformer’s\\nhyper-parameter settings during the ﬁne-tuning phase are not\\nmodel agnostic and they are directly related to the quality of\\nthe model.\\n'>\n",
            "<LTTextBoxHorizontal(17) 297.079,69.781,538.293,139.448 'D. EVALUATION METRICS\\nWe evaluate the models for sentiment analysis of ﬁnancial\\nheadlines, and present the results chronologically, based on\\nthe models’ publication date. We ﬁrst evaluate lexicon-based\\nmethods, using Harvard IV-4 and Loughran-McDonald dic-\\ntionaries. Next, we evaluate word encoders as pioneers in\\n'>\n",
            "<LTTextBoxHorizontal(18) 305.049,50.635,429.695,60.562 '5https://github.com/ProsusAI/ﬁnBERT\\n'>\n",
            "<LTTextBoxHorizontal(19) 36.170,29.812,60.565,36.786 '131674\\n'>\n",
            "<LTTextBoxHorizontal(20) 492.714,30.058,538.290,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTFigure(Fm0) 36.170,751.701,113.170,764.701 matrix=[1.00,0.00,0.00,1.00, (36.17,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 84.638,547.869,489.820,698.654 matrix=[1.00,0.00,0.00,1.00, (84.64,547.87)]>\n",
            "<LTTextBoxHorizontal(0) 36.170,749.978,321.998,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,709.848,201.795,716.836 'TABLE 5. Fixed word embedding encoders results.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,30.058,81.746,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTTextBoxHorizontal(3) 513.893,29.812,538.288,36.786 '131675\\n'>\n",
            "<LTFigure(Fm0) 461.287,751.701,538.287,764.701 matrix=[1.00,0.00,0.00,1.00, (461.29,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 43.450,72.134,531.009,696.915 matrix=[1.00,0.00,0.00,1.00, (43.45,72.13)]>\n",
            "<LTTextBoxHorizontal(0) 252.467,749.978,538.295,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,709.848,132.634,716.836 'TABLE 6. Sentence encoders.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,90.711,277.384,160.450 'modern NLP feature engineering approaches. Here, we use\\nword encoders with shallow RNN architectures, described\\nin Section V. Subsequently, we examine the performance\\nof sentence encoders with a shallow dense layer and CNN\\narchitectures. Finally, we measure the efﬁciency of the latest\\nNLP transformers, described in Section III.\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,54.846,277.384,88.719 'As a main evaluation metric, we chose Matthews Corre-\\nlation Coefﬁcient (MCC) (16), where TP and TN are True\\nPositive and True Negative samples accordingly, and FP and\\n'>\n",
            "<LTTextBoxHorizontal(4) 297.079,138.532,538.293,160.450 'FN are the False Positive and False Negative number of\\nsamples which are misclassiﬁed.\\n'>\n",
            "<LTTextBoxHorizontal(5) 322.333,115.159,355.302,126.287 'MCC =\\n'>\n",
            "<LTTextBoxHorizontal(6) 359.264,116.275,367.693,126.238 '√\\n'>\n",
            "<LTTextBoxHorizontal(7) 367.693,107.635,505.035,133.271 'tp ∗ tn−fp ∗ fn\\n(tp + fp)(fn + tn)(fp + tn)(tp + fn)\\n'>\n",
            "<LTTextBoxHorizontal(8) 521.689,115.059,538.287,125.022 '(16)\\n'>\n",
            "<LTTextBoxHorizontal(9) 297.079,55.184,538.293,101.012 'MCC is widely used in assessing binary classiﬁcation\\nperformance with a range between -1 (completely wrong\\nbinary classiﬁer) and 1 (completely accurate binary classi-\\nﬁer). It takes into consideration true and false positives and\\n'>\n",
            "<LTTextBoxHorizontal(10) 36.170,29.812,60.565,36.786 '131676\\n'>\n",
            "<LTTextBoxHorizontal(11) 492.714,30.058,538.290,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTFigure(Fm0) 36.170,751.701,113.170,764.701 matrix=[1.00,0.00,0.00,1.00, (36.17,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 36.729,184.928,537.729,698.655 matrix=[1.00,0.00,0.00,1.00, (36.73,184.93)]>\n",
            "<LTLine 359.264,119.712,505.035,119.712>\n",
            "<LTLine 367.693,117.406,505.035,117.406>\n",
            "<LTTextBoxHorizontal(0) 36.170,749.978,321.998,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,709.848,219.516,716.836 'TABLE 7. Contextual word embedding encoders results.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,329.054,277.384,350.972 'negatives, thus providing a balanced measure, which can be\\nused even if the classes have different sample sizes.\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,54.187,277.384,302.161 'VI. RESULTS AND DISCUSSION\\nIn this section, we present the model evaluation results.\\nIn Table 3, we report on the performance of\\nthe\\nlexicon-based models by using hand-crafted feature engi-\\nneering, based on the Loughran-McDonald (LM) ﬁnan-\\ncial and general Harvard IV-4 dictionaries. We perform\\nthe evaluations by using the Lydia system polarity detec-\\ntion, machine-learning classiﬁers, and deep-learning mod-\\nels, as described in previous sections. As expected, the\\nLoughran-McDonald features outperform the Harvard IV-4\\ngeneral-purpose sentiment analysis dictionary. Hence, fea-\\nture extraction with a domain-speciﬁc dictionary is a better\\napproach for sentiment analysis tasks. The best perform-\\ning model is the XGB classiﬁer using LM features, achiev-\\ning MCC=0.327. Additionally, we ﬁnd that RNN networks\\noutperform CNN and fully-connected dense networks. The\\nimproved results are due to the RNN networks’ ability to\\nremember sequential data, which is crucial for classiﬁca-\\ntion of sentences. Furthermore, the bidirectional context and\\nattention layer improve the results when used in combination\\nwith RNN networks.\\n'>\n",
            "<LTTextBoxHorizontal(4) 297.079,281.233,538.293,350.972 'In Table 4, we present the results of the experiments per-\\nformed on features extracted from statistical methods. We use\\nML classiﬁers and a deep neural network classiﬁer based\\non fully connected dense layers. These methods show good\\nresults, achieving an MCC score of 0.667, almost twice as\\ngood as the lexicon-based methods.\\n'>\n",
            "<LTTextBoxHorizontal(5) 297.079,101.905,538.293,279.241 'In Table 5, we present the evaluation results of the word\\nencoders. Generally, the best score is achieved when using\\nStanford’s GloVe with Bidirectional GRU and attention layer\\n(MCC=0.704). Here, the attention layer increases the MCC\\nscore by 0.04 compared to the BiGRU method without the\\nattention layer (MCC=0.666). Additionally, the evaluated\\nword encoders achieve better results when used with RNN\\nnetworks, which further learn the context from the attention\\nlayer. In all tests, the GRU units outperform the LSTM units.\\nThe features extracted from word encoders are signiﬁ-\\ncantly better compared to the features extracted by using\\nlexicons and dictionaries. Furthermore, the word encoders\\nperform better than statistical methods for feature extraction,\\nwhich implies that incorporating semantic meaning into the\\nword representation is useful for classiﬁcation.\\n'>\n",
            "<LTTextBoxHorizontal(6) 297.079,54.085,538.293,99.913 'The results obtained from the evaluation of sentence\\nencoders are presented in Table 6. InferSent, developed by\\nFacebook, is the best performing sentence-based encoder.\\nIts version 2 uses a simple architecture composed of\\n'>\n",
            "<LTTextBoxHorizontal(7) 36.170,30.058,81.746,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTTextBoxHorizontal(8) 513.893,29.812,538.288,36.786 '131677\\n'>\n",
            "<LTFigure(Fm0) 461.287,751.701,538.287,764.701 matrix=[1.00,0.00,0.00,1.00, (461.29,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 40.528,375.450,533.931,696.914 matrix=[1.00,0.00,0.00,1.00, (40.53,375.45)]>\n",
            "<LTTextBoxHorizontal(0) 252.467,749.978,538.295,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,709.848,137.592,716.836 'TABLE 8. Transformers results.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,61.893,277.384,167.497 'fully connected dense layers which averages FastText word\\nembeddings, thus outperforming Doc2Vec, Universal Sen-\\ntence Encoder (USE), Skip-Thought-Vectors, and LASER.\\nAdditionally, InferSent outperforms the word encoder Fast-\\nText, which implies that the InferSent’s algorithm for averag-\\ning the word embeddings has superior efﬁciency for sentence\\ncontext representation. Furthermore, we ﬁnd that the FastText\\nversion of InferSent outperforms the GloVe version of Inter-\\nSent. When using sentence vector representation, ML classi-\\n'>\n",
            "<LTTextBoxHorizontal(3) 297.079,145.579,538.293,167.497 'ﬁers are more effective than CNN and a fully connected dense\\nnetwork.\\n'>\n",
            "<LTTextBoxHorizontal(4) 297.079,61.893,538.293,143.587 'In Table 7, we present the results of the ﬁrst contextual\\nword encoder, ELMo, which we evaluate in combination with\\nML classiﬁers (SVC, XGB) and DL classiﬁer models (Dense,\\nCNN and RNN). ELMo embeddings outperform the evalu-\\nated word encoders with ﬁxed embeddings. This conﬁrms the\\nhypothesis that contextual word vectors extract better features\\nthan the ﬁxed ones. Additionally, concatenated vectors of\\n'>\n",
            "<LTTextBoxHorizontal(5) 36.170,29.812,60.565,36.786 '131678\\n'>\n",
            "<LTTextBoxHorizontal(6) 492.714,30.058,538.290,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTFigure(Fm0) 36.170,751.701,113.170,764.701 matrix=[1.00,0.00,0.00,1.00, (36.17,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 36.729,191.975,537.729,698.655 matrix=[1.00,0.00,0.00,1.00, (36.73,191.97)]>\n",
            "<LTTextBoxHorizontal(0) 36.170,749.978,321.998,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 45.449,322.268,528.042,354.349 'FIGURE 9. Sentiment analysis models’ performances grouped by text representation method. On the chart, the top of the line for each model is the\\nmaximum, the bottom is the minimum and the white rectangle is the average performance per group. The numbers in the brackets represent the\\ngroup of the text representation method, where (1) - lexicon-based methods, (2) - statistical methods, (3) - word encoders, (4) - sentence encoder,\\n(5) NLP Transformer.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,55.848,277.384,304.914 'words embeddings in combination with BiGRU network and\\nan attention layer outperform the other ML and DL networks.\\nWe also evaluate the popular NLP transformers which sup-\\nport text classiﬁcation. We ﬁne-tune them with training data\\nin order to bias the embeddings towards ﬁnancial sentiment\\nanalysis. All transformer architectures outperform word and\\nsentence encoders, as shown in Table 8. Hence, contextu-\\nalized embeddings perform semantic tasks better than their\\nnon-contextualized counterparts. Among the family of BERT\\ntransformers, BERT-Large-uncased achieves the best score\\nin classiﬁcation, with MCC=0.859. Although FinBERT was\\npre-trained on Reuters ﬁnancial texts, it does not perform as\\nwell as the other pre-trained versions of BERT, which use\\nWikipedia and BookCorpus as text corpora for pre-training.\\nRoBERTa’s dynamic masking increases the efﬁciency of\\nthe BERT algorithm by 0.023. DistilBERT retains more than\\n95% of the accuracy while having 40% fewer parameters.\\nA distilled version of RoBERTa achieves as good results as\\nBERT-large while using half the parameters of the teacher\\nRoBERTa-base model. Among the ALBERT family of trans-\\nformers, ALBERT-xxlarge pre-trained model outperforms\\n'>\n",
            "<LTTextBoxHorizontal(3) 297.079,211.265,538.303,306.179 'the other ALBERT versions, obtaining MCC=0.881. Addi-\\ntionally, ALBERT outperforms the BERT model. The\\ncross-language model (XLM) also outperforms BERT and\\nXLNet. XLM-MLM-en-2048 achieves the best result, with\\nMCC=0.863, among all XLM versions. Finally, the latest\\nNLP transformer, Facebook’s BART, outperforms all the\\nother NLP transformers when applied to ﬁnance data, achiev-\\ning the best MCC score of 0.895.\\n'>\n",
            "<LTTextBoxHorizontal(4) 307.042,199.310,449.891,209.273 'We show the performances of\\n'>\n",
            "<LTTextBoxHorizontal(5) 297.079,175.399,538.293,209.273 'representation\\napproaches in Table 2, while the performance of each method\\nchronologically is shown in Fig. 9.\\n'>\n",
            "<LTTextBoxHorizontal(6) 458.480,199.310,473.275,209.273 'text\\n'>\n",
            "<LTTextBoxHorizontal(7) 297.079,57.176,538.293,161.687 'VII. CONCLUSION\\nThis paper presents a comprehensive chronological study\\nof NLP-based methods for sentiment analysis in ﬁnance.\\nThe study begins with the lexicon-based approach, includes\\nword and sentence encoders and concludes with recent NLP\\ntransformers. The NLP transformers show superior perfor-\\nmances compared to the other evaluated approaches. The\\nmain progress in sentiment analysis accuracy is driven by\\nthe text representation methods, which feed the semantic\\n'>\n",
            "<LTTextBoxHorizontal(8) 36.170,30.058,81.746,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTTextBoxHorizontal(9) 513.893,29.812,538.288,36.786 '131679\\n'>\n",
            "<LTFigure(Fm0) 461.287,751.701,538.287,764.701 matrix=[1.00,0.00,0.00,1.00, (461.29,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 45.449,363.464,529.009,716.873 matrix=[1.00,0.00,0.00,1.00, (45.45,363.46)]>\n",
            "<LTTextBoxHorizontal(0) 252.467,749.978,538.295,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,636.006,277.384,717.700 'meaning of the words and sentences into the models. The\\nresults achieved by the best models are comparable to expert’s\\nopinion. The evaluations were performed on a relatively small\\ndataset of approximately 2000 sentences. Even though the\\ndataset is not large, we obtained good results, suggesting\\nthat this approach is appropriate for domains where large\\nannotated data is not available.\\n'>\n",
            "<LTTextBoxHorizontal(2) 91.963,624.051,125.020,634.014 'versions\\n'>\n",
            "<LTTextBoxHorizontal(3) 46.133,624.051,80.456,634.014 'Distilled\\n'>\n",
            "<LTTextBoxHorizontal(4) 36.170,552.319,277.384,634.014 '(Distilled-BERT and Distilled-\\nRoBERTa) of NLP transformers achieve text classiﬁca-\\ntion performances comparable to their large, uncompressed\\nteacher models. Hence, they can be effectively used in text\\nclassiﬁcation production environments, where the need for\\nlight-weight, responsive, energy-efﬁcient and cost-saving\\nmodels is essential.\\n'>\n",
            "<LTTextBoxHorizontal(5) 36.170,456.678,277.384,550.327 'The results of this study can be applied in areas such\\nas ﬁnance, where decision-making is based on senti-\\nment extraction from massive textual datasets. The ﬁnd-\\nings imply that selected models can be successfully used\\nfor forecasting stock market trends and corporate earnings,\\ndecision-making in securities trading and portfolio manage-\\nment, brand reputation management as well as fraud detection\\nand regulation [87]–[89].\\n'>\n",
            "<LTTextBoxHorizontal(6) 36.170,420.813,277.384,454.686 'Although this approach was constructed for sentiment\\nanalysis in the ﬁnance domain, it can be extended to other\\nareas such as healthcare, legal and business analytics.\\n'>\n",
            "<LTTextBoxHorizontal(7) 36.170,374.228,98.738,407.009 'APPENDIX A\\nRESULTS\\nSee Tables 3–8.\\n'>\n",
            "<LTTextBoxHorizontal(8) 37.166,351.459,90.756,360.425 'REFERENCES\\n'>\n",
            "<LTTextBoxHorizontal(9) 39.961,311.143,277.391,345.624 '[1] A. Yenter and A. Verma, ‘‘Deep CNN-LSTM with combined kernels from\\nmultiple branches for IMDb review sentiment analysis,’’ in Proc. IEEE 8th\\nAnnu. Ubiquitous Comput., Electron. Mobile Commun. Conf. (UEMCON),\\nOct. 2017, pp. 540–546.\\n'>\n",
            "<LTTextBoxHorizontal(10) 39.961,274.480,277.391,308.961 '[2] D. Cer, Y. Yang, S.-Y. Kong, N. Hua, N. Limtiaco, R. St. John, N. Constant,\\nM. Guajardo-Cespedes, S. Yuan, C. Tar, Y.-H. Sung, B. Strope, and\\nR. Kurzweil, ‘‘Universal sentence encoder,’’ 2018, arXiv:1803.11175.\\n[Online]. Available: http://arxiv.org/abs/1803.11175\\n'>\n",
            "<LTTextBoxHorizontal(11) 39.961,210.122,277.391,272.299 '[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\\nof deep bidirectional transformers for language understanding,’’ 2018,\\narXiv:1810.04805. [Online]. Available: http://arxiv.org/abs/1810.04805\\n[4] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V. Stoyanov, ‘‘RoBERTa: A robustly optimized\\nBERT pretraining approach,’’ 2019, arXiv:1907.11692. [Online]. Avail-\\nable: http://arxiv.org/abs/1907.11692\\n'>\n",
            "<LTTextBoxHorizontal(12) 39.961,200.358,277.378,208.016 '[5] B. G. Malkiel, ‘‘The efﬁcient market hypothesis and its critics,’’ J. Econ.\\n'>\n",
            "<LTTextBoxHorizontal(13) 53.782,191.392,195.012,199.050 'Perspect., vol. 17, no. 1, pp. 59–82, Feb. 2003.\\n'>\n",
            "<LTTextBoxHorizontal(14) 39.961,163.696,277.391,189.211 '[6] M.-Y. Day and C.-C. Lee, ‘‘Deep learning for ﬁnancial sentiment analysis\\non ﬁnance news providers,’’ in Proc. IEEE/ACM Int. Conf. Adv. Social\\nNetw. Anal. Mining (ASONAM), Aug. 2016, pp. 1127–1134.\\n'>\n",
            "<LTTextBoxHorizontal(15) 39.961,118.067,277.391,161.514 '[7] L. Dodevska, V. Petreski, K. Mishev, A. Gjorgjevikj, I. Vodenska,\\nL. Chitkushev, and D. Trajanov, ‘‘Predicting companies stock price direc-\\ntion by using sentiment analysis of news articles,’’ in Proc. 15th Annu.\\nInt. Conf. Comput. Sci. Educ. Comput. Sci., Fulda, Germany, Jul. 2019,\\npp. 37–42.\\n'>\n",
            "<LTTextBoxHorizontal(16) 39.961,90.371,277.391,115.886 '[8] W. Souma, I. Vodenska, and H. Aoyama, ‘‘Enhanced news sentiment\\nanalysis using deep learning methods,’’ J. Comput. Social Sci., vol. 2, no. 1,\\npp. 33–46, Jan. 2019.\\n'>\n",
            "<LTTextBoxHorizontal(17) 39.961,53.708,277.391,88.189 '[9] S. F. Crone and C. Koeppel, ‘‘Predicting exchange rates with sentiment\\nindicators: An empirical evaluation using text mining and multilayer\\nperceptrons,’’ in Proc. IEEE Conf. Comput. Intell. Financial Eng. Econ.\\n(CIFEr), Mar. 2014, pp. 114–121.\\n'>\n",
            "<LTTextBoxHorizontal(18) 297.079,653.659,538.300,715.836 '[10] C. Curme, H. E. Stanley, and I. Vodenska, ‘‘Coupled network approach\\nto predictability of ﬁnancial market returns and news sentiments,’’ Int. J.\\nTheor. Appl. Finance, vol. 18, no. 7, Nov. 2015, Art. no. 1550043.\\n[11] K. Mishev, A. Gjorgjevikj, I. Vodenska, L. Chitkushev, W. Souma, and\\nD. Trajanov, ‘‘Forecasting corporate revenue by using deep-learning\\nmethodologies,’’ in Proc. Int. Conf. Control, Artif. Intell., Robot. Optim.\\n(ICCAIRO), May 2019, pp. 115–120.\\n'>\n",
            "<LTTextBoxHorizontal(19) 297.079,625.962,538.300,651.477 '[12] T. Loughran and B. Mcdonald, ‘‘When is a liability not a liability? Textual\\nanalysis, dictionaries, and 10-ks,’’ J. Finance, vol. 66, no. 1, pp. 35–65,\\nFeb. 2011.\\n'>\n",
            "<LTTextBoxHorizontal(20) 297.079,533.908,538.300,623.781 '[13] M. Ghiassi, J. Skinner, and D. Zimbra, ‘‘Twitter brand sentiment analysis:\\nA hybrid system using n-gram analysis and dynamic artiﬁcial neural\\nnetwork,’’ Expert Syst. Appl., vol. 40, no. 16, pp. 6266–6282, Nov. 2013.\\n[14] N. Li, X. Liang, X. Li, C. Wang, and D. D. Wu, ‘‘Network environment\\nand ﬁnancial risk using machine learning and sentiment analysis,’’ Hum.\\nEcological Risk Assessment, Int. J., vol. 15, no. 2, pp. 227–252, Apr. 2009.\\n[15] G. Wang, T. Wang, B. Wang, D. Sambasivan, Z. Zhang, H. Zheng, and\\nB. Y. Zhao, ‘‘Crowds on wall street: Extracting value from collaborative\\ninvesting platforms,’’ in Proc. 18th ACM Conf. Comput. Supported Coop-\\nerat. Work Social Comput. CSCW, 2015, pp. 17–30.\\n'>\n",
            "<LTTextBoxHorizontal(21) 297.079,506.212,538.300,531.726 '[16] M. Atzeni, A. Dridi, and D. R. Recupero, ‘‘Fine-grained sentiment analysis\\non ﬁnancial microblogs and news headlines,’’ in Semantic Web Challenges.\\nCham, Switzerland: Springer, 2017, pp. 124–128.\\n'>\n",
            "<LTTextBoxHorizontal(22) 297.079,478.515,538.300,504.030 '[17] S. Agaian and P. Kolm, ‘‘Financial sentiment analysis using machine\\nlearning techniques,’’ Int. J. Investment Manage. Financial Innov., vol. 3,\\npp. 1–9, 2017.\\n'>\n",
            "<LTTextBoxHorizontal(23) 297.079,450.819,538.300,476.334 '[18] S. Sohangir, N. Petty, and D. Wang, ‘‘Financial sentiment lexicon analy-\\nsis,’’ in Proc. IEEE 12th Int. Conf. Semantic Comput. (ICSC), Jan. 2018,\\npp. 286–289.\\n'>\n",
            "<LTTextBoxHorizontal(24) 297.079,423.123,538.300,448.638 '[19] S. Sohangir, D. Wang, A. Pomeranets, and T. M. Khoshgoftaar, ‘‘Big data:\\nDeep learning for ﬁnancial sentiment analysis,’’ J. Big Data, vol. 5, no. 1,\\np. 3, Dec. 2018.\\n'>\n",
            "<LTTextBoxHorizontal(25) 297.079,395.427,538.301,420.942 '[20] L. Zhang, S. Wang, and B. Liu, ‘‘Deep learning for sentiment analysis: A\\nsurvey,’’ Wiley Interdiscipl. Rev., Data Mining Knowl. Discovery, vol. 8,\\nno. 4, 2018, Art. no. e1253.\\n'>\n",
            "<LTTextBoxHorizontal(26) 297.079,367.731,538.300,393.246 '[21] D. Tang, B. Qin, and T. Liu, ‘‘Document modeling with gated recurrent\\nneural network for sentiment classiﬁcation,’’ in Proc. Conf. Empirical\\nMethods Natural Lang. Process., 2015, pp. 1422–1432.\\n'>\n",
            "<LTTextBoxHorizontal(27) 297.079,340.035,538.300,365.549 '[22] K. Sheng Tai, R. Socher, and C. D. Manning, ‘‘Improved semantic repre-\\nsentations from tree-structured long short-term memory networks,’’ 2015,\\narXiv:1503.00075. [Online]. Available: http://arxiv.org/abs/1503.00075\\n'>\n",
            "<LTTextBoxHorizontal(28) 297.079,312.339,538.300,337.853 '[23] Y. Kim, ‘‘Convolutional neural networks for sentence classiﬁcation,’’\\n2014, arXiv:1408.5882. [Online]. Available: http://arxiv.org/abs/1408.\\n5882\\n'>\n",
            "<LTTextBoxHorizontal(29) 297.079,284.642,538.300,310.157 '[24] X. Zhang, J. Zhao, and Y. LeCun, ‘‘Character-level convolutional networks\\nfor text classiﬁcation,’’ in Proc. Adv. Neural Inf. Process. Syst., 2015,\\npp. 649–657.\\n'>\n",
            "<LTTextBoxHorizontal(30) 297.079,256.946,538.300,282.461 '[25] R. Johnson and T. Zhang, ‘‘Deep pyramid convolutional neural networks\\nfor text categorization,’’ in Proc. 55th Annu. Meeting Assoc. Comput.\\nLinguistics (Long Papers), vol. 1, 2017, pp. 562–570.\\n'>\n",
            "<LTTextBoxHorizontal(31) 297.079,220.284,538.300,254.765 '[26] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, ‘‘Hierarchical\\nattention networks for document classiﬁcation,’’ in Proc. Conf. North\\nAmer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol., 2016,\\npp. 1480–1489.\\n'>\n",
            "<LTTextBoxHorizontal(32) 297.079,192.588,538.300,218.102 '[27] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, ‘‘Distributed\\nrepresentations of words and phrases and their compositionality,’’ in Proc.\\nAdv. Neural Inf. Process. Syst., 2013, pp. 3111–3119.\\n'>\n",
            "<LTTextBoxHorizontal(33) 297.079,164.892,538.300,190.406 '[28] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\\nProcess. (EMNLP), 2014, pp. 1532–1543.\\n'>\n",
            "<LTTextBoxHorizontal(34) 297.079,137.195,538.300,162.710 '[29] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, ‘‘Enriching word\\nvectors with subword information,’’ Trans. Assoc. Comput. Linguistics,\\nvol. 5, pp. 135–146, Dec. 2017.\\n'>\n",
            "<LTTextBoxHorizontal(35) 297.079,90.770,538.300,135.014 '[30] Q. Le and T. Mikolov, ‘‘Distributed representations of sentences and\\ndocuments,’’ in Proc. Int. Conf. Mach. Learn., 2014, pp. 1188–1196.\\n[31] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba,\\nand S. Fidler, ‘‘Skip-thought vectors,’’ in Proc. Adv. Neural Inf. Process.\\nSyst., 2015, pp. 3294–3302.\\n'>\n",
            "<LTTextBoxHorizontal(36) 297.079,54.107,538.300,88.588 '[32] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes, ‘‘Super-\\nvised learning of universal sentence representations from natural lan-\\nguage inference data,’’ 2017, arXiv:1705.02364. [Online]. Available:\\nhttp://arxiv.org/abs/1705.02364\\n'>\n",
            "<LTTextBoxHorizontal(37) 36.170,29.812,60.565,36.786 '131680\\n'>\n",
            "<LTTextBoxHorizontal(38) 492.714,30.058,538.290,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTFigure(Fm0) 36.170,751.701,113.170,764.701 matrix=[1.00,0.00,0.00,1.00, (36.17,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTTextBoxHorizontal(0) 36.170,749.978,321.998,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,690.321,277.391,715.836 '[33] M. Artetxe and H. Schwenk, ‘‘Massively multilingual sentence embed-\\ndings for zero-shot cross-lingual transfer and beyond,’’ Trans. Assoc. Com-\\nput. Linguistics, vol. 7, pp. 597–610, Mar. 2019.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,662.625,277.391,688.140 '[34] X. Man, T. Luo, and J. Lin, ‘‘Financial sentiment analysis(FSA): A sur-\\nvey,’’ in Proc. IEEE Int. Conf. Ind. Cyber Phys. Syst. (ICPS), May 2019,\\npp. 617–622.\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,634.929,277.391,660.443 '[35] S. Yang, J. Rosenfeld, and J. Makutonin, ‘‘Financial aspect-based sen-\\ntiment analysis using deep representations,’’ 2018, arXiv:1808.07931.\\n[Online]. Available: http://arxiv.org/abs/1808.07931\\n'>\n",
            "<LTTextBoxHorizontal(4) 36.170,625.165,193.398,632.747 '[36] C.-H. Du, M.-F. Tsai, and C.-J. Wang,\\n'>\n",
            "<LTTextBoxHorizontal(5) 53.782,598.266,277.391,632.747 'to\\nsentence-level sentiment analysis for ﬁnancial reports,’’ in Proc. IEEE\\n(ICASSP), May 2019,\\nInt. Conf. Acoust., Speech Signal Process.\\npp. 1562–1566.\\n'>\n",
            "<LTTextBoxHorizontal(6) 199.289,625.165,265.601,632.747 '‘‘Beyond word-level\\n'>\n",
            "<LTTextBoxHorizontal(7) 36.170,570.570,277.391,596.085 '[37] L. Zhao, L. Li, and X. Zheng, ‘‘A BERT based sentiment analysis\\nand key entity detection approach for online ﬁnancial texts,’’ 2020,\\narXiv:2001.05326. [Online]. Available: http://arxiv.org/abs/2001.05326\\n'>\n",
            "<LTTextBoxHorizontal(8) 36.170,542.874,277.391,568.389 '[38] J. Howard and S. Ruder, ‘‘Universal language model ﬁne-tuning for\\ntext classiﬁcation,’’ 2018, arXiv:1801.06146. [Online]. Available: http://\\narxiv.org/abs/1801.06146\\n'>\n",
            "<LTTextBoxHorizontal(9) 36.170,496.448,277.391,540.769 '[39] P. J. Stone, D. C. Dunphy, and M. S. Smith, The General Inquirer: A\\nComputer Approach to Content Analysis. Oxford, U.K.: MIT Press, 1966.\\n[40] J. L. Rogers, A. Van Buskirk, and S. L. C. Zechman, ‘‘Disclosure tone and\\nshareholder litigation,’’ Accounting Rev., vol. 86, no. 6, pp. 2155–2183,\\nNov. 2011.\\n'>\n",
            "<LTTextBoxHorizontal(10) 36.170,468.752,277.391,494.267 '[41] A. K. Davis, W. Ge, D. Matsumoto, and J. L. Zhang, ‘‘The effect of\\nmanager-speciﬁc optimism on the tone of earnings conference calls,’’ Rev.\\nAccounting Stud., vol. 20, no. 2, pp. 639–673, Jun. 2015.\\n'>\n",
            "<LTTextBoxHorizontal(11) 36.170,441.056,277.391,466.571 '[42] W. Zhang and S. Skiena, ‘‘Trading strategies to exploit blog and news\\nsentiment,’’ in Proc. 4th Int. AAAI Conf. Weblogs Social Media, May 2010,\\npp. 1–4.\\n'>\n",
            "<LTTextBoxHorizontal(12) 36.170,413.360,277.391,438.874 '[43] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‘‘Efﬁcient estimation of\\nword representations in vector space,’’ 2013, arXiv:1301.3781. [Online].\\nAvailable: http://arxiv.org/abs/1301.3781\\n'>\n",
            "<LTTextBoxHorizontal(13) 36.170,403.596,97.690,411.178 '[44] Z. S. Harris,\\n'>\n",
            "<LTTextBoxHorizontal(14) 102.899,403.596,277.385,411.254 '‘‘Distributional structure,’’ Word, vol. 10, nos. 2–3,\\n'>\n",
            "<LTTextBoxHorizontal(15) 53.782,394.630,129.390,402.212 'pp. 146–162, Aug. 1954.\\n'>\n",
            "<LTTextBoxHorizontal(16) 36.170,366.934,277.391,392.449 '[45] T. K. Landauer and S. T. Dumais, ‘‘A solution to Plato’s problem: The latent\\nsemantic analysis theory of acquisition, induction, and representation of\\nknowledge.,’’ Psychol. Rev., vol. 104, no. 2, p. 211, 1997.\\n'>\n",
            "<LTTextBoxHorizontal(17) 36.170,330.271,277.391,364.752 '[46] M. Sahlgren, ‘‘The word-space model: Using distributional analysis\\nto represent syntagmatic and paradigmatic relations between words in\\nhigh-dimensional vector spaces,’’ Ph.D. dissertation, Dept. Comput. Lin-\\nguistics, Stockholm Univ., Stockholm, Sweden, 2006.\\n'>\n",
            "<LTTextBoxHorizontal(18) 36.170,283.845,277.391,328.090 '[47] P. D. Turney and P. Pantel, ‘‘From frequency to meaning: Vector space\\nmodels of semantics,’’ J. Artif. Intell. Res., vol. 37, pp. 141–188, Feb. 2010.\\n[48] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, ‘‘Bag of tricks for\\nefﬁcient text classiﬁcation,’’ in Proc. 15th Conf. Eur. Chapter Assoc.\\nComput. Linguistics, Short Papers, vol. 2, 2017, pp. 427–431.\\n'>\n",
            "<LTTextBoxHorizontal(19) 36.170,256.149,277.391,281.664 '[49] Y. Sharma, G. Agrawal, P. Jain, and T. Kumar, ‘‘Vector representation\\nof words for sentiment analysis using GloVe,’’ in Proc. Int. Conf. Intell.\\nCommun. Comput. Techn. (ICCT), Dec. 2017, pp. 279–284.\\n'>\n",
            "<LTTextBoxHorizontal(20) 36.170,219.487,277.391,253.968 '[50] P. Lauren, G. Qu, J. Yang, P. Watta, G.-B. Huang, and A. Lendasse,\\n‘‘Generating word embeddings from an extreme learning machine for\\nsentiment analysis and sequence labeling tasks,’’ Cognit. Comput., vol. 10,\\nno. 4, pp. 625–638, Aug. 2018.\\n'>\n",
            "<LTTextBoxHorizontal(21) 36.170,191.791,277.391,217.305 '[51] S. M. Rezaeinia, R. Rahmani, A. Ghodsi, and H. Veisi, ‘‘Sentiment analysis\\nbased on improved pre-trained word embeddings,’’ Expert Syst. Appl.,\\nvol. 117, pp. 139–147, Mar. 2019.\\n'>\n",
            "<LTTextBoxHorizontal(22) 36.170,164.095,277.391,189.609 '[52] T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin, ‘‘Advances\\nin pre-training distributed word representations,’’ 2017, arXiv:1712.09405.\\n[Online]. Available: http://arxiv.org/abs/1712.09405\\n'>\n",
            "<LTTextBoxHorizontal(23) 36.170,136.398,277.391,161.913 '[53] R. Velioglu, T. Yildiz, and S. Yildirim, ‘‘Sentiment analysis using learning\\napproaches over emojis for turkish tweets,’’ in Proc. 3rd Int. Conf. Comput.\\nSci. Eng. (UBMK), Sep. 2018, pp. 303–307.\\n'>\n",
            "<LTTextBoxHorizontal(24) 36.170,108.702,277.391,134.217 '[54] A. A. Altowayan and A. Elnagar, ‘‘Improving arabic sentiment analysis\\nwith sentiment-speciﬁc embeddings,’’ in Proc. IEEE Int. Conf. Big Data\\n(Big Data), Dec. 2017, pp. 4314–4320.\\n'>\n",
            "<LTTextBoxHorizontal(25) 36.170,81.006,277.391,106.521 '[55] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer, ‘‘Deep contextualized word representations,’’ 2018,\\narXiv:1802.05365. [Online]. Available: http://arxiv.org/abs/1802.05365\\n'>\n",
            "<LTTextBoxHorizontal(26) 36.170,53.310,277.391,78.825 '[56] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, ‘‘Empirical evalua-\\ntion of gated recurrent neural networks on sequence modeling,’’ 2014,\\narXiv:1412.3555. [Online]. Available: http://arxiv.org/abs/1412.3555\\n'>\n",
            "<LTTextBoxHorizontal(27) 297.079,690.321,538.300,715.836 '[57] L. Logeswaran and H. Lee, ‘‘An efﬁcient framework for learning sen-\\ntence representations,’’ 2018, arXiv:1803.02893. [Online]. Available:\\nhttp://arxiv.org/abs/1803.02893\\n'>\n",
            "<LTTextBoxHorizontal(28) 297.079,664.239,538.300,689.754 '[58] A. Akbik, D. Blythe, and R. Vollgraf, ‘‘Contextual string embeddings for\\nsequence labeling,’’ in Proc. 27th Int. Conf. Comput. Linguistics, 2018,\\npp. 1638–1649.\\n'>\n",
            "<LTTextBoxHorizontal(29) 297.079,638.157,538.300,663.672 '[59] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\\n'>\n",
            "<LTTextBoxHorizontal(30) 297.079,603.108,538.300,637.590 '[60] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,\\nand S. Fidler, ‘‘Aligning books and movies: Towards story-like visual\\nexplanations by watching movies and reading books,’’ in Proc. IEEE Int.\\nConf. Comput. Vis. (ICCV), Dec. 2015, pp. 19–27.\\n'>\n",
            "<LTTextBoxHorizontal(31) 297.079,577.026,538.300,602.541 '[61] D. Araci, ‘‘FinBERT: Financial sentiment analysis with pre-trained lan-\\nguage models,’’ 2019, arXiv:1908.10063. [Online]. Available: http://arxiv.\\norg/abs/1908.10063\\n'>\n",
            "<LTTextBoxHorizontal(32) 297.079,550.944,538.300,576.459 '[62] G. Lample and A. Conneau, ‘‘Cross-lingual language model pretrain-\\ning,’’ 2019, arXiv:1901.07291. [Online]. Available: http://arxiv.org/abs/\\n1901.07291\\n'>\n",
            "<LTTextBoxHorizontal(33) 297.079,515.896,538.300,550.377 '[63] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\\n‘‘ALBERT: A lite BERT for self-supervised learning of language rep-\\nresentations,’’ 2019, arXiv:1909.11942. [Online]. Available: http://arxiv.\\norg/abs/1909.11942\\n'>\n",
            "<LTTextBoxHorizontal(34) 343.086,489.889,351.934,497.471 'for\\n'>\n",
            "<LTTextBoxHorizontal(35) 297.079,453.948,538.300,515.328 '[64] M. A. Al-Garadi, Y.-C. Yang, H. Cai, Y. Ruan, K. O’Connor,\\nG. Gonzalez-Hernandez, J. Perrone, and A. Sarker, Text Classiﬁcation\\nModels\\nthe Automatic Detection of Nonmedical Prescription\\nMedication Use from Social Media. Cold Spring Harbor Laboratory\\nPress, 2020. [Online]. Available: https://www.medrxiv.org/content/early/\\n2020/04/17/2020.04.13.20064089.full.pdf,\\n10.1101/2020.04.13.\\n20064089.\\n'>\n",
            "<LTTextBoxHorizontal(36) 453.874,462.914,465.671,470.496 'doi:\\n'>\n",
            "<LTTextBoxHorizontal(37) 297.079,427.866,538.300,453.380 '[65] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, ‘‘DistilBERT, a dis-\\ntilled version of BERT: Smaller, faster, cheaper and lighter,’’ 2019,\\narXiv:1910.01108. [Online]. Available: http://arxiv.org/abs/1910.01108\\n'>\n",
            "<LTTextBoxHorizontal(38) 297.079,392.817,538.300,427.298 '[66] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek,\\nF. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov,\\n‘‘Unsupervised cross-lingual representation learning at scale,’’ 2019,\\narXiv:1911.02116. [Online]. Available: http://arxiv.org/abs/1911.02116\\n'>\n",
            "<LTTextBoxHorizontal(39) 297.079,348.802,538.300,392.250 '[67] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV. Stoyanov, and L. Zettlemoyer,\\n‘‘BART: Denoising sequence-to-\\nsequence pre-training for natural language generation, translation, and\\ncomprehension,’’ 2019, arXiv:1910.13461. [Online]. Available: http://\\narxiv.org/abs/1910.13461\\n'>\n",
            "<LTTextBoxHorizontal(40) 297.079,322.720,538.300,348.235 '[68] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\\n‘‘Language models are unsupervised multitask learners,’’ OpenAI Blog,\\nvol. 1, no. 8, p. 9, 2019.\\n'>\n",
            "<LTTextBoxHorizontal(41) 297.079,296.638,538.300,322.153 '[69] P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala, ‘‘Good debt\\nor bad debt: Detecting semantic orientations in economic texts,’’ J. Assoc.\\nInf. Sci. Technol., vol. 65, no. 4, pp. 782–796, Apr. 2014.\\n'>\n",
            "<LTTextBoxHorizontal(42) 297.079,261.589,538.300,296.070 '[70] K. Cortis, A. Freitas, T. Daudert, M. Huerlimann, M. Zarrouk, S. Hand-\\nschuh, and B. Davis, ‘‘SemEval-2017 task 5: Fine-grained sentiment\\nanalysis on ﬁnancial microblogs and news,’’ in Proc. 11th Int. Workshop\\nSemantic Eval. (SemEval-), 2017, pp. 519–535.\\n'>\n",
            "<LTTextBoxHorizontal(43) 297.079,227.358,538.300,261.098 '[71] F. Chollet. (2015). Keras. [Online]. Available: https://keras.io\\n[72] T. Wolf et al., ‘‘HuggingFace’s transformers: State-of-the-art natural lan-\\nguage processing,’’ 2019, arXiv:1910.03771. [Online]. Available: http://\\narxiv.org/abs/1910.03771\\n'>\n",
            "<LTTextBoxHorizontal(44) 297.079,219.208,538.300,226.790 '[73] J. H. Friedman, ‘‘Greedy function approximation: A gradient boosting\\n'>\n",
            "<LTTextBoxHorizontal(45) 314.691,210.242,498.780,217.900 'machine,’’ Ann. Statist., vol. 29, no. 5, pp. 1189–1232, 2001.\\n'>\n",
            "<LTTextBoxHorizontal(46) 297.079,166.227,538.300,209.675 '[74] T. Chen and C. Guestrin, ‘‘XGBoost: A scalable tree boosting sys-\\nin Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery\\ntem,’’\\nData Mining (KDD), San Francisco, CA, USA. New York, NY, USA:\\nACM, 2016, pp. 785–794. [Online]. Available: http://doi.acm.org/10.1145/\\n2939672.2939785, doi: 10.1145/2939672.2939785.\\n'>\n",
            "<LTTextBoxHorizontal(47) 297.079,140.145,538.300,165.660 '[75] L. Deng and D. Yu, ‘‘Deep learning: Methods and applications,’’\\nFound. Trends Signal Process., vol. 7, nos. 3–4, pp. 197–387,\\nJun. 2014.\\n'>\n",
            "<LTTextBoxHorizontal(48) 297.079,114.063,538.300,139.577 '[76] D. Yu and L. Deng, ‘‘Deep learning and its applications to signal and\\ninformation processing [exploratory DSP],’’ IEEE Signal Process. Mag.,\\nvol. 28, no. 1, pp. 145–154, Jan. 2011.\\n'>\n",
            "<LTTextBoxHorizontal(49) 297.079,87.980,538.300,113.495 '[77] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, ‘‘Deep\\nlearning for computer vision: A brief review,’’ Comput. Intell. Neurosci.,\\nvol. 2018, pp. 1–13, Feb. 2018.\\n'>\n",
            "<LTTextBoxHorizontal(50) 297.079,52.932,538.300,87.413 '[78] L. Deng, G. Hinton, and B. Kingsbury, ‘‘New types of deep neural network\\nlearning for speech recognition and related applications: An overview,’’\\nin Proc. IEEE Int. Conf. Acoust., Speech Signal Process., May 2013,\\npp. 8599–8603.\\n'>\n",
            "<LTTextBoxHorizontal(51) 36.170,30.059,81.746,36.037 'VOLUME 8, 2020\\n'>\n",
            "<LTTextBoxHorizontal(52) 513.892,29.813,538.287,36.787 '131681\\n'>\n",
            "<LTFigure(Fm0) 461.287,751.701,538.287,764.701 matrix=[1.00,0.00,0.00,1.00, (461.29,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTTextBoxHorizontal(0) 252.467,749.978,538.295,756.973 'K. Mishev et al.: Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers\\n'>\n",
            "<LTTextBoxHorizontal(1) 36.170,672.388,277.391,715.836 '[79] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen,\\nY. Zhang, Y. Wang, R. Skerrv-Ryan, R. A. Saurous, Y. Agiomvrgiannakis,\\nand Y. Wu, ‘‘Natural TTS synthesis by conditioning wavenet on MEL\\nspectrogram predictions,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal\\nProcess. (ICASSP), Apr. 2018, pp. 4779–4783.\\n'>\n",
            "<LTTextBoxHorizontal(2) 36.170,637.340,277.391,671.821 '[80] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang,\\nJ. Raiman, and J. Miller, ‘‘Deep voice 3: Scaling text-to-speech with convo-\\nlutional sequence learning,’’ 2017, arXiv:1710.07654. [Online]. Available:\\nhttp://arxiv.org/abs/1710.07654\\n'>\n",
            "<LTTextBoxHorizontal(3) 36.170,611.258,277.391,636.772 '[81] G. Liu and J. Guo, ‘‘Bidirectional LSTM with attention mechanism and\\nconvolutional layer for text classiﬁcation,’’ Neurocomputing, vol. 337,\\npp. 325–338, Apr. 2019.\\n'>\n",
            "<LTTextBoxHorizontal(4) 36.170,585.175,277.391,610.690 '[82] P. Liu, X. Qiu, and X. Huang, ‘‘Recurrent neural network for text clas-\\nsiﬁcation with multi-task learning,’’ 2016, arXiv:1605.05101. [Online].\\nAvailable: http://arxiv.org/abs/1605.05101\\n'>\n",
            "<LTTextBoxHorizontal(5) 36.170,559.093,277.391,584.608 '[83] D. Bahdanau, K. Cho, and Y. Bengio, ‘‘Neural machine translation by\\njointly learning to align and translate,’’ 2014, arXiv:1409.0473. [Online].\\nAvailable: http://arxiv.org/abs/1409.0473\\n'>\n",
            "<LTTextBoxHorizontal(6) 36.170,515.078,277.390,558.602 '[84] K. Mishev et al., ‘‘Performance evaluation of word and sentence embed-\\ndings for ﬁnance headlines sentiment analysis,’’ in ICT Innovations 2019.\\nBig Data Processing and Mining (Communications in Computer and\\nInformation Science), vol. 1110, S. Gievska and G. Madjarov, Eds. Cham,\\nSwitzerland: Springer, 2019, pp. 161–172.\\n'>\n",
            "<LTTextBoxHorizontal(7) 36.170,488.996,277.391,514.511 '[85] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimiza-\\ntion,’’ 2014, arXiv:1412.6980. [Online]. Available: http://arxiv.org/abs/\\n1412.6980\\n'>\n",
            "<LTTextBoxHorizontal(8) 36.170,480.847,207.250,488.429 '[86] N. Srivastava, G. Hinton, A. Krizhevsky,\\n'>\n",
            "<LTTextBoxHorizontal(9) 53.782,453.948,277.391,488.429 'and\\nR. Salakhutdinov, ‘‘Dropout: A simple way to prevent neural networks\\nfrom overﬁtting,’’ J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958,\\n2014.\\n'>\n",
            "<LTTextBoxHorizontal(10) 215.136,480.847,258.558,488.429 'I. Sutskever,\\n'>\n",
            "<LTTextBoxHorizontal(11) 36.170,445.798,277.391,453.380 '[87] T. Rao and S. Srivastava, ‘‘Analyzing stock market movements using\\n'>\n",
            "<LTTextBoxHorizontal(12) 53.782,436.832,192.411,444.414 'twitter sentiment analysis,’’ Tech. Rep., 2012.\\n'>\n",
            "<LTTextBoxHorizontal(13) 36.170,401.783,277.391,436.264 '[88] J. Smailović, M. Grčar, N. Lavrač, and M. Žnidaršič, ‘‘Predictive sentiment\\nanalysis of tweets: A stock market application,’’ in Proc. Int. Workshop\\nHum.-Comput. Interact. Knowl. Discovery Complex, Unstructured, Big\\nData. Berlin, Germany: Springer, 2013, pp. 77–88.\\n'>\n",
            "<LTTextBoxHorizontal(14) 36.170,375.701,277.391,401.216 '[89] X. Li, H. Xie, L. Chen, J. Wang, and X. Deng, ‘‘News impact on stock price\\nreturn via sentiment analysis,’’ Knowl.-Based Syst., vol. 69, pp. 14–23,\\nOct. 2014.\\n'>\n",
            "<LTTextBoxHorizontal(15) 36.170,212.130,277.379,325.305 'KOSTADIN MISHEV received the bachelor’s\\ndegree in informatics and computer engineering\\nand the master’s degree in computer networks\\nand e-technologies degree from Saints Cyril and\\nMethodius University, Skopje, in 2013 and 2016,\\nrespectively, where he is currently pursuing the\\nPh.D. degree. He is also a Teaching and a Research\\nAssistant with the Faculty of Computer Science\\nand Engineering, Saints Cyril and Methodius Uni-\\nversity. His research interests include data science,\\nnatural language processing, semantic Web, enterprise application architec-\\ntures, Web technologies, and computer networks.\\n'>\n",
            "<LTTextBoxHorizontal(16) 36.170,50.635,277.379,163.811 'received the bachelor’s\\nANA GJORGJEVIKJ\\ndegree in computer science and engineering and\\nthe master’s degree in computer networks and\\ne-technologies from Saints Cyril and Methodius\\nUniversity, Skopje, in 2010 and 2014, respectively,\\nwhere she is currently pursuing the Ph.D. degree in\\nthe domain of data science, with particular focus\\non deep learning and natural language processing.\\nShe has been working as a Software Engineer,\\nsince 2010. Her research interests include data\\nscience, machine learning, natural language processing, and knowledge\\nrepresentation.\\n'>\n",
            "<LTTextBoxHorizontal(17) 297.079,469.066,538.290,716.140 'IRENA VODENSKA received the B.S. degree in\\ncomputer information systems from the University\\nof Belgrade, the Ph.D. degree in econophysics (sta-\\ntistical ﬁnance) from Boston University, and the\\nM.B.A. degree from the Owen Graduate School\\nof Management, Vanderbilt University. She is cur-\\nrently an Associate Professor of ﬁnance and the\\nDirector of ﬁnance programs with the Boston\\nUniversity’s Metropolitan College. Her research\\nfocuses on network theory and complexity science\\nin macroeconomics. She conducts a theoretical and applied interdisciplinary\\nresearch using quantitative approaches for modeling interdependences of\\nﬁnancial networks, banking system dynamics, and global ﬁnancial crises.\\nMore speciﬁcally, her research focuses on modeling of early warning indi-\\ncators and systemic risk propagation throughout interconnected ﬁnancial\\nand economic networks. She also studies the effects of news announcement\\non ﬁnancial markets, corporations, ﬁnancial institutions, and related global\\neconomic systems. She uses neural networks and deep learning methodolo-\\ngies for natural language processing to text mine important factors affecting\\ncorporate performance and global economic trends. She teaches Invest-\\nment Analysis and Portfolio Management, International Finance and Trade,\\nFinancial Regulation and Ethics, and Derivatives Securities and Markets at\\nBoston University. She is also a Chartered Financial Analyst (CFA) charter\\nholder. As a Principal Investigator (PI) for Boston University, she has won\\ninterdisciplinary research grants awarded by the European Commission, EU,\\nU.S. Army Research Ofﬁce, and the National Science Foundation, USA.\\n'>\n",
            "<LTTextBoxHorizontal(18) 297.079,220.399,538.288,448.344 'LUBOMIR T. CHITKUSHEV received the\\nDipl.Ing. degree in electrical engineering from\\nthe University of Belgrade, the M.Sc. degree in\\nbiomedical engineering from the Medical Col-\\nlege of Virginia, VCU, and the Ph.D. degree in\\nbiomedical engineering from Boston University.\\nHe is currently an Associate Professor of computer\\nscience with the Boston University’s Metropolitan\\nCollege, where he serves as the Director of health\\ninformatics and health sciences and the Associate\\nDean of academic affairs. His research activity is focused on modeling of\\ncomplex systems, computer network security and architecture, and biomed-\\nical and health informatics. He is also the founder of the Health Informatics\\nProgram, Boston University, and a founding member of Boston University’s\\nRINA Laboratory, where Recursive Inter-Network Architecture (RINA) was\\nintroduced as efﬁcient, scalable, and secure approach to Internet architecture.\\nHe is also a Co-Founder and the Associate Director of the Center for Reliable\\nInformation Systems and Cyber Security (RISCS), Boston University, which\\ncoordinates research on reliable and secure computational systems and\\ninfrastructure and information assurance education. He has served as the\\nPrincipal Investigator for Boston University on research grants awarded by\\nthe European Commission, EU, the National Security Agency, USA, and the\\nU.S. Department of Justice. He has also served as a Reviewer with the U.S.\\nNational Science Foundation.\\n'>\n",
            "<LTTextBoxHorizontal(19) 297.079,60.200,538.288,211.632 'DIMITAR TRAJANOV (Member, IEEE) received\\nthe Ph.D. degree. He is currently a Professor\\nand the Head of the Department of Informa-\\ntion Systems and Network Technologies, Faculty\\nof Computer Science and Engineering, Ss. Cyril\\nand Methodius University, Skopje. He is also the\\nLeader of Regional Social Innovation Hub estab-\\nlished, in 2013, as a co-operation between UNDP\\nand the Faculty of Computer Science and Engi-\\nneering. He is the author of more than 150 journal\\nand conference papers and seven books. He has been involved in more\\nthan 60 research and industrial projects, mostly as the Project Leader. His\\nresearch interests include data science, machine learning, NLP, FinTech,\\nsemantic Web, open data, sharing economy, social innovation, e-commerce,\\nentrepreneurship, technology for development, mobile development, and\\nclimate change.\\n'>\n",
            "<LTTextBoxHorizontal(20) 36.170,29.812,60.565,36.786 '131682\\n'>\n",
            "<LTTextBoxHorizontal(21) 492.714,30.058,538.290,36.036 'VOLUME 8, 2020\\n'>\n",
            "<LTFigure(Fm0) 36.170,751.701,113.170,764.701 matrix=[1.00,0.00,0.00,1.00, (36.17,751.70)]>\n",
            "<LTLine 36.170,746.190,538.287,746.190>\n",
            "<LTFigure(Fm1) 36.170,234.458,108.171,324.460 matrix=[1.00,0.00,0.00,1.00, (36.17,234.46)]>\n",
            "<LTFigure(Fm2) 36.170,72.964,108.171,162.966 matrix=[1.00,0.00,0.00,1.00, (36.17,72.96)]>\n",
            "<LTFigure(Fm3) 297.079,625.292,369.080,715.294 matrix=[1.00,0.00,0.00,1.00, (297.08,625.29)]>\n",
            "<LTFigure(Fm4) 297.079,357.497,369.080,447.499 matrix=[1.00,0.00,0.00,1.00, (297.08,357.50)]>\n",
            "<LTFigure(Fm5) 297.079,120.784,369.080,210.786 matrix=[1.00,0.00,0.00,1.00, (297.08,120.78)]>\n",
            "<LTFigure(Fm6) 525.287,52.364,538.287,55.364 matrix=[0.00,1.00,-1.00,0.00, (538.29,52.36)]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Font types and sizes. "
      ],
      "metadata": {
        "id": "c3EiC40xANKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_pages\n",
        "from pdfminer.layout import LTTextContainer, LTChar\n",
        "\n",
        "for page_layout in extract_pages(path):\n",
        "    for element in page_layout:\n",
        "        if isinstance(element, LTTextContainer):\n",
        "            for text_line in element:\n",
        "                for character in text_line:\n",
        "                    if isinstance(character, LTChar):\n",
        "                        print(character.fontname)\n",
        "                        print(character.size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5Is-c5U-eEb",
        "outputId": "851a7f64-2f6e-44d9-a931-594e90da1804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "ULUFXX+FormataOTFMd\n",
            "7.970000000000027\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.96999999999997\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "ULUFXX+FormataOTFMd\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999999\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "HQOMRN+TimesLTStd-Roman\n",
            "7.969999999999992\n",
            "YEAKOT+FormataOTF-Reg\n",
            "6.974\n",
            "YEAKOT+FormataOTF-Reg\n",
            "6.974\n",
            "YEAKOT+FormataOTF-Reg\n",
            "6.974\n",
            "YEAKOT+FormataOTF-Reg\n",
            "6.974\n",
            "YEAKOT+FormataOTF-Reg\n",
            "6.974\n",
            "YEAKOT+FormataOTF-Reg\n",
            "6.974\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n",
            "YEAKOT+FormataOTF-Reg\n",
            "5.977999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_pages\n",
        "from pdfminer.layout import LTTextContainer, LTChar\n",
        "character_list= set()\n",
        "size_list = set()\n",
        "for page_layout in extract_pages(path):\n",
        "    for element in page_layout:\n",
        "        if isinstance(element, LTTextContainer):\n",
        "            for text_line in element:\n",
        "                for character in text_line:\n",
        "                    if isinstance(character, LTChar):\n",
        "                        character_list.add(character.fontname)\n",
        "                        size_list.add(character.size)\n",
        "print(\"Total different fonts:\", character_list)\n",
        "print(\"Total different fonts_size:\", size_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lxuz8gN_uJF",
        "outputId": "5de8cac1-8156-4627-b0df-aab66f8e14e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total different fonts: {'JOJIDE+GiovanniStd-BookItalic', 'TZFEUC+FormataOTFCond-Md', 'HQOMRN+TimesLTStd-Roman', 'TTKHMF+CMSY10', 'YEAKOT+FormataOTF-Reg', 'PEMFVY+MTSYN', 'AZWITB+FormataOTF-Bold', 'AIMLZW+FormataOTF-Italic', 'SVXLZL+TimesLTStd-Bold', 'DOOUMH+RMTMI', 'PWBZCV+CMSY8', 'VWALRS+MTEX', 'ULUFXX+FormataOTFMd', 'KYIKJZ+FormataOTFMdIt', 'TZPOLL+TimesLTStd-Italic'}\n",
            "Total different fonts_size: {4.996999999999957, 5.977999999999952, 6.974000000000046, 7.495999999999981, 6.5750000000000455, 9.862999999999943, 7.671000000000049, 9.963000000000022, 9.962999999999965, 8.966000000000008, 9.962999999999994, 7.572000000000003, 9.96299999999998, 9.963000000000008, 9.963000000000001, 21.918000000000006, 5.977999999999998, 5.9780000000000015, 5.977999999999994, 5.9780000000000655, 6.974, 6.7749999999999915, 6.973999999999933, 6.973999999999997, 6.774999999999999, 7.969999999999999, 7.571999999999889, 6.9739999999999895, 7.970000000000027, 7.5719999999999885, 8.965999999999894, 8.966000000000037, 8.965999999999994, 8.96599999999998, 7.581999999999994, 7.581999999999937, 7.96999999999997, 7.969999999999914, 7.582000000000001, 7.582000000000008, 7.969999999999992, 7.582000000000022}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZVVq-Fiyt2_g7KYp-zbkMbNzuES1RENG",
      "authorship_tag": "ABX9TyNgp4/EOQsPyh6VIvOcpvQn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}